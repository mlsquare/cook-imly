{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_nodes = clf.tree_.children_left[clf.tree_.children_left>0]\n",
    "right_nodes = clf.tree_.children_right[clf.tree_.children_right>0]\n",
    "node_indicator = clf.decision_path(X)\n",
    "path_list = []\n",
    "for i, j in enumerate(X):\n",
    "    path_list.append(node_indicator.indices[node_indicator.indptr[i]:node_indicator.indptr[i+1]])\n",
    "\n",
    "## Convert path to strings\n",
    "path_column = np.array([])\n",
    "for i, j in enumerate(X):\n",
    "    path_as_string = []\n",
    "    for node in path_list[i]:\n",
    "        if node == 0:\n",
    "            path_as_string.append('S')\n",
    "        elif node in left_nodes:\n",
    "            path_as_string.append('L')\n",
    "        elif node in right_nodes:\n",
    "            path_as_string.append('R')\n",
    "            \n",
    "    path_as_string.append('E')\n",
    "    path_as_string = ' '.join(path_as_string)\n",
    "    path_column = np.append(path_column, path_as_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = ['S', 'L', 'R', 'E']\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "Xnew = np.hstack((X, path_column.reshape(-1,1)))\n",
    "path_sequence = Xnew[:,4]\n",
    "data = pd.DataFrame(Xnew)\n",
    "data[5]=y\n",
    "df = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# prepare dataset for training\n",
    "get_path_lengths = lambda t: len(t.split())\n",
    "paths_lengths = np.array([get_path_lengths(xi) for xi in path_sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 4\n",
    "label_size = 3\n",
    "feature_size = 4\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = np.max(paths_lengths)\n",
    "sentences = []\n",
    "next_chars = []\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "for i in range(0, len(df)):\n",
    "    # get the feature\n",
    "    curr_feat = np.array([df.iloc[i,0:4]])\n",
    "    curr_path = df.iloc[i,4].split()\n",
    "    curr_path_len = len(curr_path)\n",
    "    curr_label = y[i]\n",
    "    for j in range(1,curr_path_len):\n",
    "        features.append(curr_feat)\n",
    "        labels.append(curr_label)\n",
    "        sentences.append(curr_path[0:j])\n",
    "        next_chars.append(curr_path[j])\n",
    "print('Vectorization...')\n",
    "\n",
    "x_sent = np.zeros((len(sentences), maxlen, vocab_size), dtype=np.bool)\n",
    "x_feat = np.zeros((len(sentences), feature_size), dtype=np.float)\n",
    "y_chars = np.zeros((len(sentences), vocab_size), dtype=np.bool)\n",
    "y_feat = np.zeros((len(sentences), label_size), dtype=np.float)\n",
    "#from keras.utils import to_categorical\n",
    "#y_feat_tmp = to_categorical(df[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x_sent[i, t, char_indices[char]] = 1\n",
    "    y_chars[i, char_indices[next_chars[i]]] = 1\n",
    "    x_feat[i,:] = features[i]\n",
    "    y_feat[i,labels[i]]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False  True] [1. 0. 0.] [[ True False False False]\n",
      " [False False  True False]\n",
      " [False False  True False]\n",
      " [False  True False False]\n",
      " [False  True False False]\n",
      " [False False False False]\n",
      " [False False False False]] [6.  3.  4.8 1.8]\n",
      "(560, 4) (560, 3) (560, 7, 4) (560, 4)\n"
     ]
    }
   ],
   "source": [
    "index = 10\n",
    "print(y_chars[index],y_feat[index],x_sent[index],x_feat[index])\n",
    "print(y_chars.shape,y_feat.shape,x_sent.shape,x_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "ip_x (InputLayer)               (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ip_sent (InputLayer)            (None, 7, 4)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hidden_x (Dense)                (None, 5)            25          ip_x[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "lstm_sent (LSTM)                (None, 5)            200         ip_sent[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "op_x (Dense)                    (None, 3)            18          hidden_x[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "op_sent (Dense)                 (None, 4)            24          lstm_sent[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 267\n",
      "Trainable params: 267\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Concatenate, concatenate, Flatten\n",
    "\n",
    "h1_size = 5\n",
    "latent_dim = 5\n",
    "\n",
    "input_x_features = Input(shape=(feature_size,),name='ip_x')\n",
    "hidden_state_x = Dense(h1_size, activation='relu',name='hidden_x')(input_x_features)\n",
    "output_labels = Dense(3, activation='softmax',name='op_x')(hidden_state_x)\n",
    "\n",
    "input_sent_features = Input(shape=(maxlen,vocab_size),name='ip_sent')\n",
    "decoder = LSTM(latent_dim,return_state=False,return_sequences=False,name='lstm_sent')\n",
    "decoder_outputs = decoder(input_sent_features)\n",
    "\n",
    "merge_layer = concatenate([hidden_state_x,decoder_outputs],name='cat')\n",
    "output_chars = Dense(vocab_size, activation='softmax',name='op_sent')(merge_layer)\n",
    "model = Model([input_x_features,input_sent_features], [output_labels,output_chars])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "ip_x (InputLayer)               (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hidden_x (Dense)                (None, 5)            25          ip_x[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "ip_sent (InputLayer)            (None, 7, 4)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_sent (GRU)                 (None, 5)            150         ip_sent[0][0]                    \n",
      "                                                                 hidden_x[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "op_x (Dense)                    (None, 3)            18          hidden_x[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "op_sent (Dense)                 (None, 4)            24          lstm_sent[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 217\n",
      "Trainable params: 217\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Concatenate, concatenate, Flatten, GRU\n",
    "\n",
    "h1_size = 5\n",
    "latent_dim = 5\n",
    "\n",
    "input_x_features = Input(shape=(feature_size,),name='ip_x')\n",
    "hidden_state_x = Dense(h1_size, activation='relu',name='hidden_x')(input_x_features)\n",
    "output_labels = Dense(3, activation='softmax',name='op_x')(hidden_state_x)\n",
    "\n",
    "input_sent_features = Input(shape=(maxlen,vocab_size),name='ip_sent')\n",
    "decoder = GRU(latent_dim,return_state=False,return_sequences=False,name='lstm_sent')\n",
    "decoder_outputs = decoder(input_sent_features,initial_state=hidden_state_x)\n",
    "\n",
    "#merge_layer = concatenate([hidden_state_x,decoder_outputs],name='cat')\n",
    "output_chars = Dense(vocab_size, activation='softmax',name='op_sent')(decoder_outputs)\n",
    "model = Model([input_x_features,input_sent_features], [output_labels,output_chars])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "560/560 [==============================] - 1s 3ms/step - loss: 1.7422 - op_x_loss: 1.1171 - op_sent_loss: 0.6251 - op_x_acc: 0.3071 - op_sent_acc: 0.7411\n",
      "Epoch 2/200\n",
      "560/560 [==============================] - 0s 403us/step - loss: 1.7316 - op_x_loss: 1.1110 - op_sent_loss: 0.6207 - op_x_acc: 0.3161 - op_sent_acc: 0.7339\n",
      "Epoch 3/200\n",
      "560/560 [==============================] - 0s 425us/step - loss: 1.7298 - op_x_loss: 1.1095 - op_sent_loss: 0.6203 - op_x_acc: 0.3393 - op_sent_acc: 0.7446\n",
      "Epoch 4/200\n",
      "560/560 [==============================] - 0s 431us/step - loss: 1.7230 - op_x_loss: 1.1088 - op_sent_loss: 0.6142 - op_x_acc: 0.3036 - op_sent_acc: 0.7446\n",
      "Epoch 5/200\n",
      "560/560 [==============================] - 0s 424us/step - loss: 1.7175 - op_x_loss: 1.1070 - op_sent_loss: 0.6106 - op_x_acc: 0.3375 - op_sent_acc: 0.7518\n",
      "Epoch 6/200\n",
      "560/560 [==============================] - 0s 410us/step - loss: 1.7148 - op_x_loss: 1.1059 - op_sent_loss: 0.6089 - op_x_acc: 0.3339 - op_sent_acc: 0.7411\n",
      "Epoch 7/200\n",
      "560/560 [==============================] - 0s 396us/step - loss: 1.7098 - op_x_loss: 1.1047 - op_sent_loss: 0.6050 - op_x_acc: 0.3232 - op_sent_acc: 0.7429\n",
      "Epoch 8/200\n",
      "560/560 [==============================] - 0s 431us/step - loss: 1.7059 - op_x_loss: 1.1010 - op_sent_loss: 0.6049 - op_x_acc: 0.3304 - op_sent_acc: 0.7464\n",
      "Epoch 9/200\n",
      "560/560 [==============================] - 0s 441us/step - loss: 1.7019 - op_x_loss: 1.1005 - op_sent_loss: 0.6014 - op_x_acc: 0.3607 - op_sent_acc: 0.7393\n",
      "Epoch 10/200\n",
      "560/560 [==============================] - 0s 447us/step - loss: 1.6983 - op_x_loss: 1.0995 - op_sent_loss: 0.5988 - op_x_acc: 0.3411 - op_sent_acc: 0.7411\n",
      "Epoch 11/200\n",
      "560/560 [==============================] - 0s 461us/step - loss: 1.6890 - op_x_loss: 1.0985 - op_sent_loss: 0.5904 - op_x_acc: 0.3554 - op_sent_acc: 0.7393\n",
      "Epoch 12/200\n",
      "560/560 [==============================] - 0s 497us/step - loss: 1.6854 - op_x_loss: 1.0979 - op_sent_loss: 0.5876 - op_x_acc: 0.3500 - op_sent_acc: 0.7482\n",
      "Epoch 13/200\n",
      "560/560 [==============================] - 0s 427us/step - loss: 1.6814 - op_x_loss: 1.0980 - op_sent_loss: 0.5834 - op_x_acc: 0.3411 - op_sent_acc: 0.7482\n",
      "Epoch 14/200\n",
      "560/560 [==============================] - 0s 437us/step - loss: 1.6759 - op_x_loss: 1.0962 - op_sent_loss: 0.5797 - op_x_acc: 0.3714 - op_sent_acc: 0.7554\n",
      "Epoch 15/200\n",
      "560/560 [==============================] - 0s 464us/step - loss: 1.6786 - op_x_loss: 1.0997 - op_sent_loss: 0.5789 - op_x_acc: 0.3250 - op_sent_acc: 0.7518\n",
      "Epoch 16/200\n",
      "560/560 [==============================] - 0s 457us/step - loss: 1.6686 - op_x_loss: 1.0956 - op_sent_loss: 0.5730 - op_x_acc: 0.3571 - op_sent_acc: 0.7607\n",
      "Epoch 17/200\n",
      "560/560 [==============================] - 0s 466us/step - loss: 1.6640 - op_x_loss: 1.0951 - op_sent_loss: 0.5689 - op_x_acc: 0.3536 - op_sent_acc: 0.7589\n",
      "Epoch 18/200\n",
      "560/560 [==============================] - 0s 468us/step - loss: 1.6650 - op_x_loss: 1.0939 - op_sent_loss: 0.5712 - op_x_acc: 0.3714 - op_sent_acc: 0.7571\n",
      "Epoch 19/200\n",
      "560/560 [==============================] - 0s 565us/step - loss: 1.6556 - op_x_loss: 1.0921 - op_sent_loss: 0.5635 - op_x_acc: 0.3464 - op_sent_acc: 0.7589\n",
      "Epoch 20/200\n",
      "560/560 [==============================] - 0s 531us/step - loss: 1.6512 - op_x_loss: 1.0929 - op_sent_loss: 0.5583 - op_x_acc: 0.3643 - op_sent_acc: 0.7571\n",
      "Epoch 21/200\n",
      "560/560 [==============================] - 0s 510us/step - loss: 1.6451 - op_x_loss: 1.0933 - op_sent_loss: 0.5518 - op_x_acc: 0.3696 - op_sent_acc: 0.7696\n",
      "Epoch 22/200\n",
      "560/560 [==============================] - 0s 464us/step - loss: 1.6388 - op_x_loss: 1.0934 - op_sent_loss: 0.5455 - op_x_acc: 0.3464 - op_sent_acc: 0.7679\n",
      "Epoch 23/200\n",
      "560/560 [==============================] - 0s 494us/step - loss: 1.6335 - op_x_loss: 1.0925 - op_sent_loss: 0.5410 - op_x_acc: 0.3786 - op_sent_acc: 0.7696\n",
      "Epoch 24/200\n",
      "560/560 [==============================] - 0s 504us/step - loss: 1.6281 - op_x_loss: 1.0920 - op_sent_loss: 0.5361 - op_x_acc: 0.3839 - op_sent_acc: 0.7661\n",
      "Epoch 25/200\n",
      "560/560 [==============================] - 0s 486us/step - loss: 1.6219 - op_x_loss: 1.0908 - op_sent_loss: 0.5310 - op_x_acc: 0.3357 - op_sent_acc: 0.7714\n",
      "Epoch 26/200\n",
      "560/560 [==============================] - 0s 409us/step - loss: 1.6194 - op_x_loss: 1.0899 - op_sent_loss: 0.5295 - op_x_acc: 0.3446 - op_sent_acc: 0.7696\n",
      "Epoch 27/200\n",
      "560/560 [==============================] - 0s 464us/step - loss: 1.6110 - op_x_loss: 1.0906 - op_sent_loss: 0.5204 - op_x_acc: 0.3821 - op_sent_acc: 0.7714\n",
      "Epoch 28/200\n",
      "560/560 [==============================] - 0s 462us/step - loss: 1.6083 - op_x_loss: 1.0901 - op_sent_loss: 0.5182 - op_x_acc: 0.3411 - op_sent_acc: 0.7696\n",
      "Epoch 29/200\n",
      "560/560 [==============================] - 1s 1ms/step - loss: 1.5993 - op_x_loss: 1.0891 - op_sent_loss: 0.5102 - op_x_acc: 0.3696 - op_sent_acc: 0.7804\n",
      "Epoch 30/200\n",
      "560/560 [==============================] - 0s 627us/step - loss: 1.5983 - op_x_loss: 1.0926 - op_sent_loss: 0.5057 - op_x_acc: 0.4000 - op_sent_acc: 0.7750\n",
      "Epoch 31/200\n",
      "560/560 [==============================] - 0s 499us/step - loss: 1.5919 - op_x_loss: 1.0921 - op_sent_loss: 0.4997 - op_x_acc: 0.3589 - op_sent_acc: 0.7732\n",
      "Epoch 32/200\n",
      "560/560 [==============================] - 1s 1ms/step - loss: 1.5874 - op_x_loss: 1.0927 - op_sent_loss: 0.4947 - op_x_acc: 0.3607 - op_sent_acc: 0.7839\n",
      "Epoch 33/200\n",
      "560/560 [==============================] - 0s 834us/step - loss: 1.5824 - op_x_loss: 1.0927 - op_sent_loss: 0.4897 - op_x_acc: 0.3643 - op_sent_acc: 0.7839\n",
      "Epoch 34/200\n",
      "560/560 [==============================] - 1s 916us/step - loss: 1.5777 - op_x_loss: 1.0889 - op_sent_loss: 0.4889 - op_x_acc: 0.3804 - op_sent_acc: 0.7821\n",
      "Epoch 35/200\n",
      "560/560 [==============================] - 0s 815us/step - loss: 1.5752 - op_x_loss: 1.0912 - op_sent_loss: 0.4840 - op_x_acc: 0.4000 - op_sent_acc: 0.7750\n",
      "Epoch 36/200\n",
      "560/560 [==============================] - 0s 477us/step - loss: 1.5625 - op_x_loss: 1.0893 - op_sent_loss: 0.4733 - op_x_acc: 0.3714 - op_sent_acc: 0.7893\n",
      "Epoch 37/200\n",
      "560/560 [==============================] - 0s 399us/step - loss: 1.5582 - op_x_loss: 1.0881 - op_sent_loss: 0.4702 - op_x_acc: 0.3768 - op_sent_acc: 0.7929\n",
      "Epoch 38/200\n",
      "560/560 [==============================] - 0s 457us/step - loss: 1.5532 - op_x_loss: 1.0899 - op_sent_loss: 0.4634 - op_x_acc: 0.3821 - op_sent_acc: 0.7893\n",
      "Epoch 39/200\n",
      "560/560 [==============================] - 0s 419us/step - loss: 1.5516 - op_x_loss: 1.0889 - op_sent_loss: 0.4627 - op_x_acc: 0.3714 - op_sent_acc: 0.7929\n",
      "Epoch 40/200\n",
      "560/560 [==============================] - 0s 415us/step - loss: 1.5411 - op_x_loss: 1.0877 - op_sent_loss: 0.4534 - op_x_acc: 0.3964 - op_sent_acc: 0.8018\n",
      "Epoch 41/200\n",
      "560/560 [==============================] - 0s 669us/step - loss: 1.5375 - op_x_loss: 1.0894 - op_sent_loss: 0.4481 - op_x_acc: 0.3839 - op_sent_acc: 0.8036\n",
      "Epoch 42/200\n",
      "560/560 [==============================] - 1s 1ms/step - loss: 1.5337 - op_x_loss: 1.0885 - op_sent_loss: 0.4452 - op_x_acc: 0.3821 - op_sent_acc: 0.8071\n",
      "Epoch 43/200\n",
      "560/560 [==============================] - 0s 478us/step - loss: 1.5259 - op_x_loss: 1.0906 - op_sent_loss: 0.4353 - op_x_acc: 0.3750 - op_sent_acc: 0.8179\n",
      "Epoch 44/200\n",
      "560/560 [==============================] - 0s 751us/step - loss: 1.5190 - op_x_loss: 1.0884 - op_sent_loss: 0.4305 - op_x_acc: 0.4018 - op_sent_acc: 0.8196\n",
      "Epoch 45/200\n",
      "560/560 [==============================] - 0s 594us/step - loss: 1.5149 - op_x_loss: 1.0891 - op_sent_loss: 0.4258 - op_x_acc: 0.3964 - op_sent_acc: 0.8214\n",
      "Epoch 46/200\n",
      "560/560 [==============================] - 0s 539us/step - loss: 1.5165 - op_x_loss: 1.0895 - op_sent_loss: 0.4270 - op_x_acc: 0.3911 - op_sent_acc: 0.8268\n",
      "Epoch 47/200\n",
      "560/560 [==============================] - 0s 851us/step - loss: 1.5067 - op_x_loss: 1.0875 - op_sent_loss: 0.4192 - op_x_acc: 0.3875 - op_sent_acc: 0.8250\n",
      "Epoch 48/200\n",
      "560/560 [==============================] - 0s 483us/step - loss: 1.5021 - op_x_loss: 1.0890 - op_sent_loss: 0.4130 - op_x_acc: 0.3804 - op_sent_acc: 0.8375\n",
      "Epoch 49/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560/560 [==============================] - 0s 674us/step - loss: 1.4951 - op_x_loss: 1.0882 - op_sent_loss: 0.4070 - op_x_acc: 0.3839 - op_sent_acc: 0.8357\n",
      "Epoch 50/200\n",
      "560/560 [==============================] - 0s 458us/step - loss: 1.4887 - op_x_loss: 1.0865 - op_sent_loss: 0.4022 - op_x_acc: 0.3964 - op_sent_acc: 0.8446\n",
      "Epoch 51/200\n",
      "560/560 [==============================] - 0s 583us/step - loss: 1.4815 - op_x_loss: 1.0875 - op_sent_loss: 0.3940 - op_x_acc: 0.3750 - op_sent_acc: 0.8464\n",
      "Epoch 52/200\n",
      "560/560 [==============================] - 0s 729us/step - loss: 1.4795 - op_x_loss: 1.0866 - op_sent_loss: 0.3929 - op_x_acc: 0.3946 - op_sent_acc: 0.8500\n",
      "Epoch 53/200\n",
      "560/560 [==============================] - 0s 821us/step - loss: 1.4739 - op_x_loss: 1.0880 - op_sent_loss: 0.3859 - op_x_acc: 0.3875 - op_sent_acc: 0.8554\n",
      "Epoch 54/200\n",
      "560/560 [==============================] - 1s 981us/step - loss: 1.4663 - op_x_loss: 1.0864 - op_sent_loss: 0.3800 - op_x_acc: 0.4143 - op_sent_acc: 0.8607\n",
      "Epoch 55/200\n",
      "560/560 [==============================] - 0s 860us/step - loss: 1.4615 - op_x_loss: 1.0877 - op_sent_loss: 0.3739 - op_x_acc: 0.3804 - op_sent_acc: 0.8821\n",
      "Epoch 56/200\n",
      "560/560 [==============================] - 0s 528us/step - loss: 1.4581 - op_x_loss: 1.0897 - op_sent_loss: 0.3684 - op_x_acc: 0.4018 - op_sent_acc: 0.8679\n",
      "Epoch 57/200\n",
      "560/560 [==============================] - 0s 866us/step - loss: 1.4532 - op_x_loss: 1.0889 - op_sent_loss: 0.3643 - op_x_acc: 0.3982 - op_sent_acc: 0.8804\n",
      "Epoch 58/200\n",
      "560/560 [==============================] - 1s 986us/step - loss: 1.4498 - op_x_loss: 1.0884 - op_sent_loss: 0.3614 - op_x_acc: 0.3839 - op_sent_acc: 0.8768\n",
      "Epoch 59/200\n",
      "560/560 [==============================] - 0s 843us/step - loss: 1.4428 - op_x_loss: 1.0864 - op_sent_loss: 0.3563 - op_x_acc: 0.3857 - op_sent_acc: 0.8786\n",
      "Epoch 60/200\n",
      "560/560 [==============================] - 0s 547us/step - loss: 1.4402 - op_x_loss: 1.0857 - op_sent_loss: 0.3545 - op_x_acc: 0.4125 - op_sent_acc: 0.8839\n",
      "Epoch 61/200\n",
      "560/560 [==============================] - 1s 1ms/step - loss: 1.4328 - op_x_loss: 1.0873 - op_sent_loss: 0.3455 - op_x_acc: 0.3750 - op_sent_acc: 0.8893\n",
      "Epoch 62/200\n",
      "560/560 [==============================] - 0s 828us/step - loss: 1.4273 - op_x_loss: 1.0896 - op_sent_loss: 0.3377 - op_x_acc: 0.3875 - op_sent_acc: 0.8839\n",
      "Epoch 63/200\n",
      "560/560 [==============================] - 1s 979us/step - loss: 1.4309 - op_x_loss: 1.0892 - op_sent_loss: 0.3417 - op_x_acc: 0.3893 - op_sent_acc: 0.8875\n",
      "Epoch 64/200\n",
      "560/560 [==============================] - 0s 759us/step - loss: 1.4272 - op_x_loss: 1.0890 - op_sent_loss: 0.3382 - op_x_acc: 0.3768 - op_sent_acc: 0.8964\n",
      "Epoch 65/200\n",
      "560/560 [==============================] - 1s 937us/step - loss: 1.4115 - op_x_loss: 1.0856 - op_sent_loss: 0.3259 - op_x_acc: 0.3929 - op_sent_acc: 0.9018\n",
      "Epoch 66/200\n",
      "560/560 [==============================] - 1s 951us/step - loss: 1.4091 - op_x_loss: 1.0855 - op_sent_loss: 0.3235 - op_x_acc: 0.4054 - op_sent_acc: 0.8911\n",
      "Epoch 67/200\n",
      "560/560 [==============================] - 0s 875us/step - loss: 1.4036 - op_x_loss: 1.0862 - op_sent_loss: 0.3174 - op_x_acc: 0.4071 - op_sent_acc: 0.9000\n",
      "Epoch 68/200\n",
      "560/560 [==============================] - 0s 857us/step - loss: 1.4021 - op_x_loss: 1.0857 - op_sent_loss: 0.3165 - op_x_acc: 0.4000 - op_sent_acc: 0.9036\n",
      "Epoch 69/200\n",
      "560/560 [==============================] - 1s 932us/step - loss: 1.4028 - op_x_loss: 1.0899 - op_sent_loss: 0.3130 - op_x_acc: 0.3768 - op_sent_acc: 0.9071\n",
      "Epoch 70/200\n",
      "560/560 [==============================] - 0s 701us/step - loss: 1.3924 - op_x_loss: 1.0847 - op_sent_loss: 0.3077 - op_x_acc: 0.3839 - op_sent_acc: 0.9089\n",
      "Epoch 71/200\n",
      "560/560 [==============================] - 0s 739us/step - loss: 1.3898 - op_x_loss: 1.0850 - op_sent_loss: 0.3047 - op_x_acc: 0.4000 - op_sent_acc: 0.9071\n",
      "Epoch 72/200\n",
      "560/560 [==============================] - 0s 573us/step - loss: 1.3813 - op_x_loss: 1.0848 - op_sent_loss: 0.2965 - op_x_acc: 0.3875 - op_sent_acc: 0.9054\n",
      "Epoch 73/200\n",
      "560/560 [==============================] - 1s 949us/step - loss: 1.3788 - op_x_loss: 1.0864 - op_sent_loss: 0.2924 - op_x_acc: 0.3893 - op_sent_acc: 0.9071\n",
      "Epoch 74/200\n",
      "560/560 [==============================] - 0s 678us/step - loss: 1.3735 - op_x_loss: 1.0862 - op_sent_loss: 0.2873 - op_x_acc: 0.3786 - op_sent_acc: 0.9125\n",
      "Epoch 75/200\n",
      "560/560 [==============================] - 0s 679us/step - loss: 1.3711 - op_x_loss: 1.0855 - op_sent_loss: 0.2856 - op_x_acc: 0.4071 - op_sent_acc: 0.9071\n",
      "Epoch 76/200\n",
      "560/560 [==============================] - 0s 593us/step - loss: 1.3682 - op_x_loss: 1.0841 - op_sent_loss: 0.2841 - op_x_acc: 0.3964 - op_sent_acc: 0.9089 0s - loss: 1.3421 - op_x_loss: 1.0809 - op_sent_loss: 0.2612 - op_x_acc: 0.4125 - op_sent_acc: 0.9\n",
      "Epoch 77/200\n",
      "560/560 [==============================] - 0s 701us/step - loss: 1.3640 - op_x_loss: 1.0848 - op_sent_loss: 0.2793 - op_x_acc: 0.4107 - op_sent_acc: 0.9143\n",
      "Epoch 78/200\n",
      "560/560 [==============================] - 0s 848us/step - loss: 1.3647 - op_x_loss: 1.0853 - op_sent_loss: 0.2794 - op_x_acc: 0.4071 - op_sent_acc: 0.9196\n",
      "Epoch 79/200\n",
      "560/560 [==============================] - 1s 914us/step - loss: 1.3593 - op_x_loss: 1.0860 - op_sent_loss: 0.2733 - op_x_acc: 0.4071 - op_sent_acc: 0.9143\n",
      "Epoch 80/200\n",
      "560/560 [==============================] - 0s 598us/step - loss: 1.3566 - op_x_loss: 1.0845 - op_sent_loss: 0.2722 - op_x_acc: 0.4089 - op_sent_acc: 0.9089\n",
      "Epoch 81/200\n",
      "560/560 [==============================] - 1s 909us/step - loss: 1.3475 - op_x_loss: 1.0846 - op_sent_loss: 0.2629 - op_x_acc: 0.3982 - op_sent_acc: 0.9268\n",
      "Epoch 82/200\n",
      "560/560 [==============================] - 0s 664us/step - loss: 1.3452 - op_x_loss: 1.0850 - op_sent_loss: 0.2601 - op_x_acc: 0.4107 - op_sent_acc: 0.9232 0s - loss: 1.3484 - op_x_loss: 1.0787 - op_sent_loss: 0.2697 - op_x_acc: 0.3800 - op_sent_acc: \n",
      "Epoch 83/200\n",
      "560/560 [==============================] - 0s 809us/step - loss: 1.3587 - op_x_loss: 1.0847 - op_sent_loss: 0.2740 - op_x_acc: 0.4000 - op_sent_acc: 0.9036\n",
      "Epoch 84/200\n",
      "560/560 [==============================] - 0s 560us/step - loss: 1.3368 - op_x_loss: 1.0843 - op_sent_loss: 0.2525 - op_x_acc: 0.4000 - op_sent_acc: 0.9232\n",
      "Epoch 85/200\n",
      "560/560 [==============================] - 0s 759us/step - loss: 1.3369 - op_x_loss: 1.0866 - op_sent_loss: 0.2502 - op_x_acc: 0.4018 - op_sent_acc: 0.9232\n",
      "Epoch 86/200\n",
      "560/560 [==============================] - 0s 476us/step - loss: 1.3293 - op_x_loss: 1.0842 - op_sent_loss: 0.2450 - op_x_acc: 0.3982 - op_sent_acc: 0.9250\n",
      "Epoch 87/200\n",
      "560/560 [==============================] - 0s 515us/step - loss: 1.3279 - op_x_loss: 1.0857 - op_sent_loss: 0.2422 - op_x_acc: 0.3929 - op_sent_acc: 0.9214\n",
      "Epoch 88/200\n",
      "560/560 [==============================] - 1s 958us/step - loss: 1.3233 - op_x_loss: 1.0846 - op_sent_loss: 0.2387 - op_x_acc: 0.4196 - op_sent_acc: 0.9214\n",
      "Epoch 89/200\n",
      "560/560 [==============================] - 0s 744us/step - loss: 1.3228 - op_x_loss: 1.0868 - op_sent_loss: 0.2360 - op_x_acc: 0.3839 - op_sent_acc: 0.9214\n",
      "Epoch 90/200\n",
      "560/560 [==============================] - 0s 649us/step - loss: 1.3193 - op_x_loss: 1.0843 - op_sent_loss: 0.2350 - op_x_acc: 0.3893 - op_sent_acc: 0.9304\n",
      "Epoch 91/200\n",
      "560/560 [==============================] - 0s 559us/step - loss: 1.3200 - op_x_loss: 1.0881 - op_sent_loss: 0.2319 - op_x_acc: 0.3982 - op_sent_acc: 0.9232\n",
      "Epoch 92/200\n",
      "560/560 [==============================] - 0s 637us/step - loss: 1.3096 - op_x_loss: 1.0843 - op_sent_loss: 0.2253 - op_x_acc: 0.3857 - op_sent_acc: 0.9214\n",
      "Epoch 93/200\n",
      "560/560 [==============================] - 0s 589us/step - loss: 1.3044 - op_x_loss: 1.0838 - op_sent_loss: 0.2206 - op_x_acc: 0.4107 - op_sent_acc: 0.9268\n",
      "Epoch 94/200\n",
      "560/560 [==============================] - 0s 474us/step - loss: 1.3011 - op_x_loss: 1.0849 - op_sent_loss: 0.2162 - op_x_acc: 0.4071 - op_sent_acc: 0.9321\n",
      "Epoch 95/200\n",
      "560/560 [==============================] - 0s 494us/step - loss: 1.3020 - op_x_loss: 1.0821 - op_sent_loss: 0.2198 - op_x_acc: 0.4196 - op_sent_acc: 0.9286\n",
      "Epoch 96/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560/560 [==============================] - 0s 495us/step - loss: 1.2961 - op_x_loss: 1.0845 - op_sent_loss: 0.2116 - op_x_acc: 0.3839 - op_sent_acc: 0.9304\n",
      "Epoch 97/200\n",
      "560/560 [==============================] - 0s 462us/step - loss: 1.2935 - op_x_loss: 1.0835 - op_sent_loss: 0.2100 - op_x_acc: 0.4018 - op_sent_acc: 0.9321\n",
      "Epoch 98/200\n",
      "560/560 [==============================] - 0s 487us/step - loss: 1.2957 - op_x_loss: 1.0857 - op_sent_loss: 0.2100 - op_x_acc: 0.4000 - op_sent_acc: 0.9339\n",
      "Epoch 99/200\n",
      "560/560 [==============================] - 0s 649us/step - loss: 1.2829 - op_x_loss: 1.0831 - op_sent_loss: 0.1998 - op_x_acc: 0.4018 - op_sent_acc: 0.9286\n",
      "Epoch 100/200\n",
      "560/560 [==============================] - 0s 456us/step - loss: 1.2903 - op_x_loss: 1.0862 - op_sent_loss: 0.2040 - op_x_acc: 0.4036 - op_sent_acc: 0.9446\n",
      "Epoch 101/200\n",
      "560/560 [==============================] - 0s 590us/step - loss: 1.2863 - op_x_loss: 1.0840 - op_sent_loss: 0.2023 - op_x_acc: 0.4089 - op_sent_acc: 0.9339\n",
      "Epoch 102/200\n",
      "560/560 [==============================] - 0s 478us/step - loss: 1.2851 - op_x_loss: 1.0837 - op_sent_loss: 0.2015 - op_x_acc: 0.4036 - op_sent_acc: 0.9375\n",
      "Epoch 103/200\n",
      "560/560 [==============================] - 0s 710us/step - loss: 1.2886 - op_x_loss: 1.0858 - op_sent_loss: 0.2028 - op_x_acc: 0.3875 - op_sent_acc: 0.9411\n",
      "Epoch 104/200\n",
      "560/560 [==============================] - 1s 897us/step - loss: 1.2706 - op_x_loss: 1.0839 - op_sent_loss: 0.1867 - op_x_acc: 0.3964 - op_sent_acc: 0.9375\n",
      "Epoch 105/200\n",
      "560/560 [==============================] - 1s 922us/step - loss: 1.2773 - op_x_loss: 1.0831 - op_sent_loss: 0.1942 - op_x_acc: 0.4214 - op_sent_acc: 0.9411\n",
      "Epoch 106/200\n",
      "560/560 [==============================] - 0s 589us/step - loss: 1.2686 - op_x_loss: 1.0829 - op_sent_loss: 0.1857 - op_x_acc: 0.4125 - op_sent_acc: 0.9411\n",
      "Epoch 107/200\n",
      "560/560 [==============================] - 0s 828us/step - loss: 1.2824 - op_x_loss: 1.0843 - op_sent_loss: 0.1981 - op_x_acc: 0.4054 - op_sent_acc: 0.9375\n",
      "Epoch 108/200\n",
      "560/560 [==============================] - 0s 738us/step - loss: 1.2680 - op_x_loss: 1.0832 - op_sent_loss: 0.1848 - op_x_acc: 0.4125 - op_sent_acc: 0.9393\n",
      "Epoch 109/200\n",
      "560/560 [==============================] - 0s 677us/step - loss: 1.2588 - op_x_loss: 1.0827 - op_sent_loss: 0.1761 - op_x_acc: 0.4161 - op_sent_acc: 0.9429\n",
      "Epoch 110/200\n",
      "560/560 [==============================] - 0s 505us/step - loss: 1.2575 - op_x_loss: 1.0842 - op_sent_loss: 0.1732 - op_x_acc: 0.4161 - op_sent_acc: 0.9482\n",
      "Epoch 111/200\n",
      "560/560 [==============================] - 0s 695us/step - loss: 1.2640 - op_x_loss: 1.0829 - op_sent_loss: 0.1811 - op_x_acc: 0.4054 - op_sent_acc: 0.9464\n",
      "Epoch 112/200\n",
      "560/560 [==============================] - 0s 536us/step - loss: 1.2584 - op_x_loss: 1.0867 - op_sent_loss: 0.1716 - op_x_acc: 0.3893 - op_sent_acc: 0.9518\n",
      "Epoch 113/200\n",
      "560/560 [==============================] - 0s 812us/step - loss: 1.2458 - op_x_loss: 1.0829 - op_sent_loss: 0.1629 - op_x_acc: 0.3893 - op_sent_acc: 0.9482\n",
      "Epoch 114/200\n",
      "560/560 [==============================] - 0s 746us/step - loss: 1.2564 - op_x_loss: 1.0850 - op_sent_loss: 0.1714 - op_x_acc: 0.4000 - op_sent_acc: 0.9500\n",
      "Epoch 115/200\n",
      "560/560 [==============================] - 0s 783us/step - loss: 1.2431 - op_x_loss: 1.0832 - op_sent_loss: 0.1598 - op_x_acc: 0.4071 - op_sent_acc: 0.9518\n",
      "Epoch 116/200\n",
      "560/560 [==============================] - 0s 491us/step - loss: 1.2406 - op_x_loss: 1.0826 - op_sent_loss: 0.1580 - op_x_acc: 0.4107 - op_sent_acc: 0.9518\n",
      "Epoch 117/200\n",
      "560/560 [==============================] - 0s 672us/step - loss: 1.2418 - op_x_loss: 1.0819 - op_sent_loss: 0.1599 - op_x_acc: 0.4196 - op_sent_acc: 0.9571\n",
      "Epoch 118/200\n",
      "560/560 [==============================] - 0s 518us/step - loss: 1.2442 - op_x_loss: 1.0832 - op_sent_loss: 0.1610 - op_x_acc: 0.4161 - op_sent_acc: 0.9500\n",
      "Epoch 119/200\n",
      "560/560 [==============================] - 0s 852us/step - loss: 1.2355 - op_x_loss: 1.0831 - op_sent_loss: 0.1524 - op_x_acc: 0.4143 - op_sent_acc: 0.9589\n",
      "Epoch 120/200\n",
      "560/560 [==============================] - 0s 767us/step - loss: 1.2325 - op_x_loss: 1.0822 - op_sent_loss: 0.1503 - op_x_acc: 0.4125 - op_sent_acc: 0.9482\n",
      "Epoch 121/200\n",
      "560/560 [==============================] - 0s 539us/step - loss: 1.2402 - op_x_loss: 1.0833 - op_sent_loss: 0.1569 - op_x_acc: 0.4071 - op_sent_acc: 0.9518\n",
      "Epoch 122/200\n",
      "560/560 [==============================] - 0s 648us/step - loss: 1.2531 - op_x_loss: 1.0851 - op_sent_loss: 0.1680 - op_x_acc: 0.4089 - op_sent_acc: 0.9500\n",
      "Epoch 123/200\n",
      "560/560 [==============================] - 0s 665us/step - loss: 1.2392 - op_x_loss: 1.0891 - op_sent_loss: 0.1501 - op_x_acc: 0.3821 - op_sent_acc: 0.9607\n",
      "Epoch 124/200\n",
      "560/560 [==============================] - 0s 702us/step - loss: 1.2287 - op_x_loss: 1.0834 - op_sent_loss: 0.1453 - op_x_acc: 0.4000 - op_sent_acc: 0.9589\n",
      "Epoch 125/200\n",
      "560/560 [==============================] - 0s 498us/step - loss: 1.2253 - op_x_loss: 1.0828 - op_sent_loss: 0.1425 - op_x_acc: 0.4107 - op_sent_acc: 0.9607\n",
      "Epoch 126/200\n",
      "560/560 [==============================] - 0s 593us/step - loss: 1.2234 - op_x_loss: 1.0816 - op_sent_loss: 0.1419 - op_x_acc: 0.4214 - op_sent_acc: 0.9607\n",
      "Epoch 127/200\n",
      "560/560 [==============================] - 0s 704us/step - loss: 1.2197 - op_x_loss: 1.0821 - op_sent_loss: 0.1376 - op_x_acc: 0.4071 - op_sent_acc: 0.9661\n",
      "Epoch 128/200\n",
      "560/560 [==============================] - 0s 680us/step - loss: 1.2269 - op_x_loss: 1.0847 - op_sent_loss: 0.1422 - op_x_acc: 0.3804 - op_sent_acc: 0.9554\n",
      "Epoch 129/200\n",
      "560/560 [==============================] - 0s 740us/step - loss: 1.2190 - op_x_loss: 1.0848 - op_sent_loss: 0.1342 - op_x_acc: 0.3929 - op_sent_acc: 0.9607\n",
      "Epoch 130/200\n",
      "560/560 [==============================] - 0s 514us/step - loss: 1.2210 - op_x_loss: 1.0825 - op_sent_loss: 0.1384 - op_x_acc: 0.4054 - op_sent_acc: 0.9554\n",
      "Epoch 131/200\n",
      "560/560 [==============================] - 0s 658us/step - loss: 1.2159 - op_x_loss: 1.0822 - op_sent_loss: 0.1337 - op_x_acc: 0.4000 - op_sent_acc: 0.9661\n",
      "Epoch 132/200\n",
      "560/560 [==============================] - 0s 687us/step - loss: 1.2221 - op_x_loss: 1.0826 - op_sent_loss: 0.1395 - op_x_acc: 0.4089 - op_sent_acc: 0.9589\n",
      "Epoch 133/200\n",
      "560/560 [==============================] - 0s 629us/step - loss: 1.2309 - op_x_loss: 1.0856 - op_sent_loss: 0.1452 - op_x_acc: 0.3946 - op_sent_acc: 0.9482\n",
      "Epoch 134/200\n",
      "560/560 [==============================] - 0s 697us/step - loss: 1.2129 - op_x_loss: 1.0833 - op_sent_loss: 0.1297 - op_x_acc: 0.4125 - op_sent_acc: 0.9679\n",
      "Epoch 135/200\n",
      "560/560 [==============================] - 0s 642us/step - loss: 1.2316 - op_x_loss: 1.0837 - op_sent_loss: 0.1480 - op_x_acc: 0.4268 - op_sent_acc: 0.9589\n",
      "Epoch 136/200\n",
      "560/560 [==============================] - 0s 687us/step - loss: 1.2105 - op_x_loss: 1.0828 - op_sent_loss: 0.1278 - op_x_acc: 0.4196 - op_sent_acc: 0.9589\n",
      "Epoch 137/200\n",
      "560/560 [==============================] - 0s 489us/step - loss: 1.2119 - op_x_loss: 1.0824 - op_sent_loss: 0.1294 - op_x_acc: 0.4036 - op_sent_acc: 0.9643\n",
      "Epoch 138/200\n",
      "560/560 [==============================] - 0s 766us/step - loss: 1.2083 - op_x_loss: 1.0815 - op_sent_loss: 0.1268 - op_x_acc: 0.3964 - op_sent_acc: 0.9607\n",
      "Epoch 139/200\n",
      "560/560 [==============================] - 1s 927us/step - loss: 1.2077 - op_x_loss: 1.0827 - op_sent_loss: 0.1250 - op_x_acc: 0.4036 - op_sent_acc: 0.9661\n",
      "Epoch 140/200\n",
      "560/560 [==============================] - 0s 619us/step - loss: 1.2093 - op_x_loss: 1.0834 - op_sent_loss: 0.1259 - op_x_acc: 0.4054 - op_sent_acc: 0.9625\n",
      "Epoch 141/200\n",
      "560/560 [==============================] - 0s 733us/step - loss: 1.2055 - op_x_loss: 1.0828 - op_sent_loss: 0.1227 - op_x_acc: 0.4196 - op_sent_acc: 0.9643\n",
      "Epoch 142/200\n",
      "560/560 [==============================] - 0s 630us/step - loss: 1.2094 - op_x_loss: 1.0845 - op_sent_loss: 0.1249 - op_x_acc: 0.3875 - op_sent_acc: 0.9625\n",
      "Epoch 143/200\n",
      "560/560 [==============================] - 1s 913us/step - loss: 1.2017 - op_x_loss: 1.0841 - op_sent_loss: 0.1176 - op_x_acc: 0.3982 - op_sent_acc: 0.9679\n",
      "Epoch 144/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560/560 [==============================] - 0s 782us/step - loss: 1.2082 - op_x_loss: 1.0867 - op_sent_loss: 0.1215 - op_x_acc: 0.3875 - op_sent_acc: 0.9696\n",
      "Epoch 145/200\n",
      "560/560 [==============================] - 0s 644us/step - loss: 1.2006 - op_x_loss: 1.0805 - op_sent_loss: 0.1201 - op_x_acc: 0.4107 - op_sent_acc: 0.9643\n",
      "Epoch 146/200\n",
      "560/560 [==============================] - 0s 659us/step - loss: 1.2014 - op_x_loss: 1.0837 - op_sent_loss: 0.1177 - op_x_acc: 0.4125 - op_sent_acc: 0.9679\n",
      "Epoch 147/200\n",
      "560/560 [==============================] - 0s 551us/step - loss: 1.1917 - op_x_loss: 1.0811 - op_sent_loss: 0.1106 - op_x_acc: 0.4125 - op_sent_acc: 0.9714\n",
      "Epoch 148/200\n",
      "560/560 [==============================] - 0s 812us/step - loss: 1.2003 - op_x_loss: 1.0818 - op_sent_loss: 0.1185 - op_x_acc: 0.4125 - op_sent_acc: 0.9661\n",
      "Epoch 149/200\n",
      "560/560 [==============================] - 0s 468us/step - loss: 1.1931 - op_x_loss: 1.0809 - op_sent_loss: 0.1122 - op_x_acc: 0.4161 - op_sent_acc: 0.9696\n",
      "Epoch 150/200\n",
      "560/560 [==============================] - 0s 660us/step - loss: 1.2087 - op_x_loss: 1.0817 - op_sent_loss: 0.1270 - op_x_acc: 0.4054 - op_sent_acc: 0.9643\n",
      "Epoch 151/200\n",
      "560/560 [==============================] - 0s 477us/step - loss: 1.2079 - op_x_loss: 1.0803 - op_sent_loss: 0.1277 - op_x_acc: 0.4107 - op_sent_acc: 0.9589\n",
      "Epoch 152/200\n",
      "560/560 [==============================] - 0s 490us/step - loss: 1.2043 - op_x_loss: 1.0799 - op_sent_loss: 0.1244 - op_x_acc: 0.4143 - op_sent_acc: 0.9661\n",
      "Epoch 153/200\n",
      "560/560 [==============================] - 1s 901us/step - loss: 1.1933 - op_x_loss: 1.0823 - op_sent_loss: 0.1110 - op_x_acc: 0.4071 - op_sent_acc: 0.9661\n",
      "Epoch 154/200\n",
      "560/560 [==============================] - 0s 620us/step - loss: 1.2053 - op_x_loss: 1.0818 - op_sent_loss: 0.1234 - op_x_acc: 0.4125 - op_sent_acc: 0.9643\n",
      "Epoch 155/200\n",
      "560/560 [==============================] - 0s 616us/step - loss: 1.1977 - op_x_loss: 1.0815 - op_sent_loss: 0.1162 - op_x_acc: 0.4089 - op_sent_acc: 0.9607\n",
      "Epoch 156/200\n",
      "560/560 [==============================] - 0s 498us/step - loss: 1.1883 - op_x_loss: 1.0822 - op_sent_loss: 0.1061 - op_x_acc: 0.4018 - op_sent_acc: 0.9696\n",
      "Epoch 157/200\n",
      "560/560 [==============================] - 0s 623us/step - loss: 1.1918 - op_x_loss: 1.0804 - op_sent_loss: 0.1114 - op_x_acc: 0.4107 - op_sent_acc: 0.9679\n",
      "Epoch 158/200\n",
      "560/560 [==============================] - 0s 668us/step - loss: 1.1879 - op_x_loss: 1.0810 - op_sent_loss: 0.1069 - op_x_acc: 0.4018 - op_sent_acc: 0.9696\n",
      "Epoch 159/200\n",
      "560/560 [==============================] - 0s 604us/step - loss: 1.1848 - op_x_loss: 1.0802 - op_sent_loss: 0.1046 - op_x_acc: 0.4107 - op_sent_acc: 0.9732\n",
      "Epoch 160/200\n",
      "560/560 [==============================] - 0s 807us/step - loss: 1.1903 - op_x_loss: 1.0841 - op_sent_loss: 0.1062 - op_x_acc: 0.3929 - op_sent_acc: 0.9732\n",
      "Epoch 161/200\n",
      "560/560 [==============================] - 0s 755us/step - loss: 1.1855 - op_x_loss: 1.0801 - op_sent_loss: 0.1055 - op_x_acc: 0.3964 - op_sent_acc: 0.9679\n",
      "Epoch 162/200\n",
      "560/560 [==============================] - 0s 727us/step - loss: 1.2009 - op_x_loss: 1.0821 - op_sent_loss: 0.1187 - op_x_acc: 0.3929 - op_sent_acc: 0.9625\n",
      "Epoch 163/200\n",
      "560/560 [==============================] - 0s 453us/step - loss: 1.2020 - op_x_loss: 1.0824 - op_sent_loss: 0.1196 - op_x_acc: 0.4179 - op_sent_acc: 0.9643\n",
      "Epoch 164/200\n",
      "560/560 [==============================] - 0s 655us/step - loss: 1.1848 - op_x_loss: 1.0804 - op_sent_loss: 0.1044 - op_x_acc: 0.4036 - op_sent_acc: 0.9696\n",
      "Epoch 165/200\n",
      "560/560 [==============================] - 0s 481us/step - loss: 1.1898 - op_x_loss: 1.0839 - op_sent_loss: 0.1059 - op_x_acc: 0.3929 - op_sent_acc: 0.9661\n",
      "Epoch 166/200\n",
      "560/560 [==============================] - 0s 478us/step - loss: 1.1796 - op_x_loss: 1.0811 - op_sent_loss: 0.0985 - op_x_acc: 0.3982 - op_sent_acc: 0.9750\n",
      "Epoch 167/200\n",
      "560/560 [==============================] - 0s 625us/step - loss: 1.1795 - op_x_loss: 1.0819 - op_sent_loss: 0.0977 - op_x_acc: 0.4196 - op_sent_acc: 0.9768\n",
      "Epoch 168/200\n",
      "560/560 [==============================] - 0s 433us/step - loss: 1.1777 - op_x_loss: 1.0810 - op_sent_loss: 0.0967 - op_x_acc: 0.3964 - op_sent_acc: 0.9786\n",
      "Epoch 169/200\n",
      "560/560 [==============================] - 0s 479us/step - loss: 1.1774 - op_x_loss: 1.0797 - op_sent_loss: 0.0977 - op_x_acc: 0.3929 - op_sent_acc: 0.9768\n",
      "Epoch 170/200\n",
      "560/560 [==============================] - 0s 669us/step - loss: 1.1721 - op_x_loss: 1.0804 - op_sent_loss: 0.0916 - op_x_acc: 0.4071 - op_sent_acc: 0.9786\n",
      "Epoch 171/200\n",
      "560/560 [==============================] - 0s 478us/step - loss: 1.1902 - op_x_loss: 1.0831 - op_sent_loss: 0.1071 - op_x_acc: 0.4125 - op_sent_acc: 0.9696\n",
      "Epoch 172/200\n",
      "560/560 [==============================] - 0s 473us/step - loss: 1.1779 - op_x_loss: 1.0797 - op_sent_loss: 0.0983 - op_x_acc: 0.3982 - op_sent_acc: 0.9750\n",
      "Epoch 173/200\n",
      "560/560 [==============================] - 0s 462us/step - loss: 1.1757 - op_x_loss: 1.0812 - op_sent_loss: 0.0945 - op_x_acc: 0.4000 - op_sent_acc: 0.9786\n",
      "Epoch 174/200\n",
      "560/560 [==============================] - 0s 692us/step - loss: 1.1729 - op_x_loss: 1.0819 - op_sent_loss: 0.0910 - op_x_acc: 0.4107 - op_sent_acc: 0.9732\n",
      "Epoch 175/200\n",
      "560/560 [==============================] - 0s 465us/step - loss: 1.1769 - op_x_loss: 1.0780 - op_sent_loss: 0.0990 - op_x_acc: 0.4143 - op_sent_acc: 0.9750\n",
      "Epoch 176/200\n",
      "560/560 [==============================] - 0s 501us/step - loss: 1.1772 - op_x_loss: 1.0799 - op_sent_loss: 0.0973 - op_x_acc: 0.4089 - op_sent_acc: 0.9750\n",
      "Epoch 177/200\n",
      "560/560 [==============================] - 0s 479us/step - loss: 1.1705 - op_x_loss: 1.0803 - op_sent_loss: 0.0902 - op_x_acc: 0.4089 - op_sent_acc: 0.9732\n",
      "Epoch 178/200\n",
      "560/560 [==============================] - 0s 482us/step - loss: 1.1685 - op_x_loss: 1.0806 - op_sent_loss: 0.0879 - op_x_acc: 0.4107 - op_sent_acc: 0.9786\n",
      "Epoch 179/200\n",
      "560/560 [==============================] - 0s 623us/step - loss: 1.1757 - op_x_loss: 1.0811 - op_sent_loss: 0.0946 - op_x_acc: 0.4054 - op_sent_acc: 0.9768\n",
      "Epoch 180/200\n",
      "560/560 [==============================] - 0s 457us/step - loss: 1.1646 - op_x_loss: 1.0787 - op_sent_loss: 0.0858 - op_x_acc: 0.4107 - op_sent_acc: 0.9804\n",
      "Epoch 181/200\n",
      "560/560 [==============================] - 1s 959us/step - loss: 1.1717 - op_x_loss: 1.0782 - op_sent_loss: 0.0936 - op_x_acc: 0.4107 - op_sent_acc: 0.9714\n",
      "Epoch 182/200\n",
      "560/560 [==============================] - 0s 798us/step - loss: 1.1644 - op_x_loss: 1.0804 - op_sent_loss: 0.0840 - op_x_acc: 0.4107 - op_sent_acc: 0.9804\n",
      "Epoch 183/200\n",
      "560/560 [==============================] - 0s 500us/step - loss: 1.1676 - op_x_loss: 1.0811 - op_sent_loss: 0.0865 - op_x_acc: 0.4161 - op_sent_acc: 0.9750\n",
      "Epoch 184/200\n",
      "560/560 [==============================] - 1s 943us/step - loss: 1.1626 - op_x_loss: 1.0790 - op_sent_loss: 0.0835 - op_x_acc: 0.3982 - op_sent_acc: 0.9768\n",
      "Epoch 185/200\n",
      "560/560 [==============================] - 0s 867us/step - loss: 1.1624 - op_x_loss: 1.0804 - op_sent_loss: 0.0820 - op_x_acc: 0.4179 - op_sent_acc: 0.9804\n",
      "Epoch 186/200\n",
      "560/560 [==============================] - 0s 832us/step - loss: 1.1614 - op_x_loss: 1.0786 - op_sent_loss: 0.0827 - op_x_acc: 0.4161 - op_sent_acc: 0.9786\n",
      "Epoch 187/200\n",
      "560/560 [==============================] - 0s 763us/step - loss: 1.1612 - op_x_loss: 1.0792 - op_sent_loss: 0.0820 - op_x_acc: 0.4018 - op_sent_acc: 0.9786\n",
      "Epoch 188/200\n",
      "560/560 [==============================] - 0s 776us/step - loss: 1.1605 - op_x_loss: 1.0786 - op_sent_loss: 0.0818 - op_x_acc: 0.4089 - op_sent_acc: 0.9786\n",
      "Epoch 189/200\n",
      "560/560 [==============================] - 0s 661us/step - loss: 1.1599 - op_x_loss: 1.0791 - op_sent_loss: 0.0807 - op_x_acc: 0.3911 - op_sent_acc: 0.9786\n",
      "Epoch 190/200\n",
      "560/560 [==============================] - 0s 829us/step - loss: 1.1603 - op_x_loss: 1.0788 - op_sent_loss: 0.0815 - op_x_acc: 0.3911 - op_sent_acc: 0.9768\n",
      "Epoch 191/200\n",
      "560/560 [==============================] - 0s 490us/step - loss: 1.1582 - op_x_loss: 1.0783 - op_sent_loss: 0.0798 - op_x_acc: 0.3982 - op_sent_acc: 0.9768\n",
      "Epoch 192/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560/560 [==============================] - 0s 689us/step - loss: 1.1651 - op_x_loss: 1.0803 - op_sent_loss: 0.0848 - op_x_acc: 0.4071 - op_sent_acc: 0.9732\n",
      "Epoch 193/200\n",
      "560/560 [==============================] - 0s 641us/step - loss: 1.1669 - op_x_loss: 1.0805 - op_sent_loss: 0.0864 - op_x_acc: 0.3982 - op_sent_acc: 0.9786\n",
      "Epoch 194/200\n",
      "560/560 [==============================] - 0s 882us/step - loss: 1.1651 - op_x_loss: 1.0791 - op_sent_loss: 0.0860 - op_x_acc: 0.4089 - op_sent_acc: 0.9679\n",
      "Epoch 195/200\n",
      "560/560 [==============================] - 0s 675us/step - loss: 1.1549 - op_x_loss: 1.0781 - op_sent_loss: 0.0768 - op_x_acc: 0.4036 - op_sent_acc: 0.9768\n",
      "Epoch 196/200\n",
      "560/560 [==============================] - 0s 746us/step - loss: 1.1588 - op_x_loss: 1.0778 - op_sent_loss: 0.0809 - op_x_acc: 0.4000 - op_sent_acc: 0.9821\n",
      "Epoch 197/200\n",
      "560/560 [==============================] - 0s 512us/step - loss: 1.1627 - op_x_loss: 1.0781 - op_sent_loss: 0.0846 - op_x_acc: 0.4018 - op_sent_acc: 0.9714\n",
      "Epoch 198/200\n",
      "560/560 [==============================] - 0s 682us/step - loss: 1.1762 - op_x_loss: 1.0777 - op_sent_loss: 0.0985 - op_x_acc: 0.4054 - op_sent_acc: 0.9696\n",
      "Epoch 199/200\n",
      "560/560 [==============================] - 0s 692us/step - loss: 1.1651 - op_x_loss: 1.0788 - op_sent_loss: 0.0863 - op_x_acc: 0.4089 - op_sent_acc: 0.9786\n",
      "Epoch 200/200\n",
      "560/560 [==============================] - 0s 819us/step - loss: 1.1578 - op_x_loss: 1.0786 - op_sent_loss: 0.0793 - op_x_acc: 0.4089 - op_sent_acc: 0.9786\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12eb81828>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit([x_feat,x_sent],[y_feat,y_chars],batch_size =20, epochs = 200,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(x):\n",
    "    n = x.shape[0]\n",
    "    x_f = x.reshape(1,feature_size)\n",
    "    token = 'S'\n",
    "    cont = True\n",
    "    text = [token]\n",
    "    x_sent = np.zeros((1,maxlen,vocab_size),dtype=np.bool)\n",
    "    x_sent[0,0,char_indices[token]] = 1\n",
    "    label = []\n",
    "    index = 1\n",
    "    while cont & (index <maxlen):\n",
    "        pred = model.predict([x_f.reshape(1,feature_size),x_sent])\n",
    "        char_index = np.argmax(pred[1])\n",
    "        label.append(np.argmax(pred[0])) \n",
    "        x_sent[0,index,char_index] = 1\n",
    "        next_char = indices_char[char_index]\n",
    "        text.append(next_char)\n",
    "        index += 1    \n",
    "        if next_char == 'E':\n",
    "            cont = False\n",
    "    return [text,label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual vs predicted:  S R R R E  vs  S R R R E labels:  2 0\n",
      "actual vs predicted:  S L E  vs  S L E labels:  0 1\n",
      "actual vs predicted:  S R R L L E  vs  S R R L L E labels:  2 0\n",
      "actual vs predicted:  S R L L R E  vs  S R L L L E labels:  2 0\n",
      "actual vs predicted:  S R R L R E  vs  S R R L L E labels:  1 0\n",
      "actual vs predicted:  S R L L L E  vs  S R L L L E labels:  1 1\n",
      "actual vs predicted:  S L E  vs  S L E labels:  0 1\n",
      "actual vs predicted:  S R R R E  vs  S R R R E labels:  2 0\n",
      "actual vs predicted:  S L E  vs  S L E labels:  0 1\n",
      "actual vs predicted:  S R R R E  vs  S R R R E labels:  2 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = []\n",
    "for i in range(10):\n",
    "    curr_feat = np.array([df.iloc[i,0:4]])\n",
    "    path,label= sample(curr_feat)\n",
    "    print('actual vs predicted: ', df.iloc[i,4] ,' vs ', ' '.join(path), 'labels: ', df.iloc[i,5],label[0])\n",
    "    count.append(df.iloc[i,5]==label[0])\n",
    "np.mean(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
