{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contributing SVD to mlsquare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fork mlsquare repository to your account and clone.**\n",
    "\n",
    "**Or just Clone https://github.com/mlsquare/mlsquare.git**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Navigate to `src/mlsquare/architectures` folder, Where the code for mapping Logistic regression  to DNN resides.\n",
    "* The code for mapping primal model(SVD) to corresponding dnn equivalent is saved as `surprise_svd.py` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kev/Desktop/mlsquare/src'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/kev/Desktop/pyvirtual2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kev/Desktop/pyvirtual2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kev/Desktop/pyvirtual2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kev/Desktop/pyvirtual2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kev/Desktop/pyvirtual2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kev/Desktop/pyvirtual2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "2019-11-14 13:53:12,633\tINFO node.py:423 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-11-14_13-53-12_4521/logs.\n",
      "2019-11-14 13:53:12,796\tINFO services.py:363 -- Waiting for redis server at 127.0.0.1:49414 to respond...\n",
      "2019-11-14 13:53:12,926\tINFO services.py:363 -- Waiting for redis server at 127.0.0.1:22540 to respond...\n",
      "2019-11-14 13:53:12,933\tINFO services.py:760 -- Starting Redis shard with 20.0 GB max memory.\n",
      "2019-11-14 13:53:12,961\tINFO services.py:1384 -- Starting the Plasma object store with 1.0 GB memory using /dev/shm.\n"
     ]
    }
   ],
   "source": [
    "from mlsquare.base import BaseModel\n",
    "import tensorflow as tf\n",
    "\n",
    "#from ..adapters.AdaptDeepctr import DeepCtr\n",
    "from mlsquare.adapters.AdaptDeepctr import DeepCtr\n",
    "#from tensorflow.keras.layers import Dense\n",
    "\n",
    "from deepctr.inputs import build_input_features, input_from_feature_columns\n",
    "from deepctr.inputs import SparseFeat\n",
    "from deepctr.layers.interaction import FM\n",
    "from deepctr.layers.utils import concat_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "??BaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Requires a separate adapter for surpriselib's SVD or can work with sklearn methods??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ..base import registry, BaseModel\n",
    "\n",
    "from mlsquare.base import registry, BaseModel\n",
    "\n",
    "#from mlsquare.adapters.sklearn import #SurpriselibModels\n",
    "\n",
    "@registry.register\n",
    "class SVD(BaseModel):\n",
    "    def __init__(self):\n",
    "        self.adapter = DeepCtr\n",
    "        self.module_name = 'deepctr'\n",
    "        self.name = 'SVD'\n",
    "        self.version = 'default'\n",
    "        #feature_cols= feature_columns\n",
    "    def create_model(self,feature_columns, **kwargs):\n",
    "        \"\"\"Instantiates the Neural Factorization Machine architecture.\n",
    "\n",
    "        :param feature_columns: An iterable containing all the sparse features used by model.\n",
    "        :param num_factors: number of units in latent representation layer.\n",
    "        :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector\n",
    "        :param l2_reg_linear: float. L2 regularizer strength applied to linear part.\n",
    "        :param l2_reg_dnn: float . L2 regularizer strength applied to DNN\n",
    "        :param init_std: float,to use as the initialize std of embedding vector\n",
    "        :param seed: integer ,to use as random seed.\n",
    "        :param biout_dropout: When not ``None``, the probability we will drop out the output of BiInteractionPooling Layer.\n",
    "        :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.\n",
    "        :param act_func: Activation function to use at prediction layer.\n",
    "        :param task: str, ``\"binary\"`` for  'binary_crossentropy' loss or  ``\"multiclass\"`` for 'categorical_crossentropy' loss\n",
    "        :return: A Keras model instance.\n",
    "        \"\"\"\n",
    "    #ensure that the `feature columns` is a list of `DenseFeat` Instances otherwise the model resulting here will have an Input shape (None,1)\n",
    "        kwargs.setdefault('embedding_size', 100)\n",
    "        kwargs.setdefault('l2_reg_embedding',1e-5)\n",
    "        kwargs.setdefault('l2_reg_linear', 1e-5)\n",
    "        kwargs.setdefault('l2_reg_dnn', 0)\n",
    "        kwargs.setdefault('init_std',0.0001)\n",
    "        kwargs.setdefault('seed', 1024)\n",
    "        kwargs.setdefault('bi_dropout', 0)\n",
    "        kwargs.setdefault('dnn_dropout', 0)\n",
    "        \n",
    "    \n",
    "        features = build_input_features(feature_columns)\n",
    "\n",
    "        input_layers = list(features.values())\n",
    "\n",
    "        sparse_embedding_list, _ = input_from_feature_columns(features,feature_columns, kwargs['embedding_size'], kwargs['l2_reg_embedding'], kwargs['init_std'], kwargs['seed'])\n",
    "    \n",
    "        fm_input = concat_fun(sparse_embedding_list, axis=1)\n",
    "        fm_logit = FM()(fm_input)\n",
    "\n",
    "    #if task=='binary':\n",
    "    #    act_func = 'sigmoid'\n",
    "    #    n_last = 1\n",
    "    #elif task=='multiclass':\n",
    "    #    act_func= 'softmax'\n",
    "    #    n_last = 5\n",
    "\n",
    "    #predictions = Dense(n_last, activation=act_func)(merge_layer)\n",
    "    \n",
    "        model = tf.keras.models.Model(inputs=input_layers, outputs=fm_logit)\n",
    "    \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def set_params(self, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def get_params(self):\n",
    "        pass#return self._model_params\n",
    "\n",
    "    def update_params(self, params):\n",
    "        pass#self._model_params.update(params)\n",
    "\n",
    "    def adapter(self):\n",
    "        return self._adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Defining interaction between user and deepctr-interface-SVD (provided from inside the mlsquare lib); **Once the new model is registered in mlsquare.**\n",
    "    1. a) User instantiates a primal model svd (DeepFM imported from deepctr) explicitly as a module `mlsquare.models.svd`.\n",
    "    \n",
    "    b) User loads the model object & adapter from `mlsquare.base.regsitry` and then instantiate with required arguments.\n",
    "    \n",
    "    2. Thereafter, `Dope` equips above primal model with standard methods--fit, predict, score, save, explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Following are data preparation steps required to instantiate a svd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparseFeat:movie_id, SparseFeat:user_id]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from deepctr.inputs import SparseFeat\n",
    "\n",
    "data_path = os.path.expanduser('u.data')\n",
    "\n",
    "df= pd.read_csv(data_path, sep='\\t',names= 'user_id\tmovie_id\trating\ttimestamp'.split('\t'))#, header=None)#used for DeepCTR\n",
    "\n",
    "sparse_features = [\"movie_id\", \"user_id\"]\n",
    "y= ['rating']\n",
    "\n",
    "#This counts unique values & encodes existing value to new lable in progression\n",
    "for feat in sparse_features:\n",
    "        lbe = LabelEncoder()\n",
    "        df[feat] = lbe.fit_transform(df[feat])\n",
    "    ##unique features for each sparse field\n",
    "    \n",
    "feature_columns = [SparseFeat(feat, df[feat].nunique()) for feat in sparse_features]\n",
    "print(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainset, testset= train_test_split(df, test_size=0.2)\n",
    "\n",
    "train_model_input = [trainset[name].values for name in sparse_features]#includes values from only data[user_id], data[movie_id]\n",
    "train_y= trainset[y].values\n",
    "\n",
    "test_model_input = [testset[name].values for name in sparse_features]#includes values from only data[user_id], data[movie_id]\n",
    "test_y= testset[y].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.a User instantiates a primal model svd (DeepFM imported from deepctr) explicitly as a module `mlsquare.models.svd`. ---?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from mlsquare.models import svd\n",
    "\n",
    "#svd_mod = svd.SVD(feature_columns, task='multiclass')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.b User loads the model object & adapter from `mlsquare.base.regsitry` and then instantiate with required arguments.** -- For example `registry[('sklearn', 'LogisticRegression')]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mlsquare.base import registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('sklearn',\n",
       "  'LogisticRegression'): {'default': [<mlsquare.architectures.sklearn.LogisticRegression at 0x7f750b06efd0>,\n",
       "   mlsquare.adapters.sklearn.SklearnKerasClassifier]},\n",
       " ('sklearn',\n",
       "  'LinearRegression'): {'default': [<mlsquare.architectures.sklearn.LinearRegression at 0x7f7553aa0f28>,\n",
       "   mlsquare.adapters.sklearn.SklearnKerasRegressor]},\n",
       " ('sklearn',\n",
       "  'Ridge'): {'default': [<mlsquare.architectures.sklearn.Ridge at 0x7f7553aab128>,\n",
       "   mlsquare.adapters.sklearn.SklearnKerasRegressor]},\n",
       " ('sklearn',\n",
       "  'Lasso'): {'default': [<mlsquare.architectures.sklearn.Lasso at 0x7f7553aab2e8>,\n",
       "   mlsquare.adapters.sklearn.SklearnKerasRegressor]},\n",
       " ('sklearn',\n",
       "  'ElasticNet'): {'default': [<mlsquare.architectures.sklearn.ElasticNet at 0x7f7553aab4a8>,\n",
       "   mlsquare.adapters.sklearn.SklearnKerasRegressor]},\n",
       " ('sklearn',\n",
       "  'LinearSVC'): {'default': [<mlsquare.architectures.sklearn.LinearSVC at 0x7f7553aab668>,\n",
       "   mlsquare.adapters.sklearn.SklearnKerasClassifier]},\n",
       " ('sklearn',\n",
       "  'SVC'): {'default': [<mlsquare.architectures.sklearn.SVC at 0x7f7553aab9b0>,\n",
       "   mlsquare.adapters.sklearn.SklearnKerasClassifier]},\n",
       " ('sklearn',\n",
       "  'DecisionTreeClassifier'): {'default': [<mlsquare.architectures.sklearn.DecisionTreeClassifier at 0x7f7553aabcf8>,\n",
       "   mlsquare.adapters.sklearn.SklearnKerasClassifier]},\n",
       " ('deepctr',\n",
       "  'SVD'): {'default': [<mlsquare.architectures.svd.SVD at 0x7f7553ab55f8>,\n",
       "   mlsquare.adapters.AdaptDeepctr.DeepCtr]}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registry.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlsquare.architectures.sklearn.LogisticRegression at 0x7f750b06efd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proxy_model, adapter =registry[('sklearn', 'LogisticRegression')]['default']\n",
    "proxy_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method GeneralizedLinearModel.create_model of <mlsquare.architectures.sklearn.LogisticRegression object at 0x7f750b06efd0>>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proxy_model.create_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Codeblocks from adapters & tune.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "\n",
    "# Split the data in to test and train batches\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kev/Desktop/pyvirtual2/lib/python3.6/site-packages/scikit_learn-0.21.3-py3.6-linux-x86_64.egg/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/kev/Desktop/pyvirtual2/lib/python3.6/site-packages/scikit_learn-0.21.3-py3.6-linux-x86_64.egg/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train)\n",
    "y_pred= model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {'layer_1': {'units': 1, ## Make key name private - '_layer'\n",
    "                                    'l1': 0,\n",
    "                                    'l2': 0,\n",
    "                                    'activation': 'sigmoid'},\n",
    "                        'optimizer': 'adam',\n",
    "                        'loss': 'binary_crossentropy'\n",
    "                        }\n",
    "proxy_model.set_params(params=model_params, set_by='model_init')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y, y_pred= proxy_model.transform_data(x_train, y_train, y_pred)\n",
    "proxy_model.X = X ##  abstract -> model_skeleton\n",
    "proxy_model.y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlsquare.architectures.sklearn.LogisticRegression at 0x7f229b2626a0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proxy_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kev/Desktop/pyvirtual2/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7f22f0314470>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proxy_model.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegression' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-95a72cc7e52d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mproxy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#fit doesnt work as its a method of `sklearnKerasClassifier adapter`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#proxy model has to be passed into an adapter object -- adapter(proxy_model, primal_model) # line 66 dope function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "proxy_model.fit\n",
    "#fit doesnt work as its a method of `sklearnKerasClassifier adapter`\n",
    "#proxy model has to be passed into an adapter object -- adapter(proxy_model, primal_model) # line 66 dope function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Possible interactions:\n",
    "    * User invokes the registery.register to load SVD model object and corresponding adapter.\n",
    "    * User then invokes create_model() on obtained svd object to create a primal model by providing feature columns, and thus model is initiated as a primal model.\n",
    "    * The primal model is then passed into adapter as proxy model directly(Or into dope to enable access to methods--fit, save & explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1. User invokes the registery.register to load SVD model object and corresponding adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_model, svd_adapter =registry[('deepctr', 'SVD')]['default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mlsquare.architectures.svd.SVD object at 0x7f7553ab55f8> \n",
      " <class 'mlsquare.adapters.AdaptDeepctr.DeepCtr'>\n"
     ]
    }
   ],
   "source": [
    "print(svd_model, '\\n', svd_adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. User then invokes create_model() on obtained svd object to create a primal model by providing feature columns, and thus model is initiated as a primal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kev/Desktop/pyvirtual2/lib/python3.6/site-packages/deepctr/layers/utils.py:156: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "model = svd_model.create_model(feature_columns)#primal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "movie_id (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_id (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sparse_emb_movie_id (Embedding) (None, 1, 100)       168200      movie_id[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sparse_emb_user_id (Embedding)  (None, 1, 100)       94300       user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "no_mask (NoMask)                (None, 1, 100)       0           sparse_emb_movie_id[0][0]        \n",
      "                                                                 sparse_emb_user_id[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 2, 100)       0           no_mask[0][0]                    \n",
      "                                                                 no_mask[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "fm (FM)                         (None, 1)            0           concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 262,500\n",
      "Trainable params: 262,500\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. The primal model is then passed into adapter as proxy model directly(Or into dope to enable access to methods--fit, save & explain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svd_adapter(proxy_model=proxy_model,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Faulty implementation as of now, the `model.create_model()` is been originally called inside `tune.py` (line#29), despite being used explicitly to define model structure in step 2 above.\n",
    "* Either the model definition has to be done with existing arrangement,  with means of creating `SparseFeat` objects(Cell 6 &7) and passing `feature columns`(cell 22) within the `AdaptDeepctr` itself; Or optimizer code needs to modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Or primal model object can be passed into adapter in usual manner, and step leading to create_model() i.e.,(Cells 6&7, Cell 22) can be moved inside `.fit()` method of adapter to save the boilerplat data preparations.\n",
    "* `.fit()` method will there take new arguments -- `trainset dataframe= df, sparse_features= [\"movie_id\", \"user_id\"], target = ['rating']`.\n",
    "* The adapater fit method needs to accodmodate creation of feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from deepctr.inputs import SparseFeat\n",
    "\n",
    "data_path = os.path.expanduser('u.data')\n",
    "\n",
    "df= pd.read_csv(data_path, sep='\\t',names= 'user_id\tmovie_id\trating\ttimestamp'.split('\t'))#, header=None)#used for DeepCTR\n",
    "\n",
    "sparse_features = [\"movie_id\", \"user_id\"]\n",
    "y= ['rating']\n",
    "\n",
    "#This counts unique values & encodes existing value to new lable in progression\n",
    "for feat in sparse_features:\n",
    "        lbe = LabelEncoder()\n",
    "        df[feat] = lbe.fit_transform(df[feat])\n",
    "    ##unique features for each sparse field\n",
    "    \n",
    "feature_columns = [SparseFeat(feat, df[feat].nunique()) for feat in sparse_features]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainset, testset= train_test_split(df, test_size=0.2)\n",
    "\n",
    "train_model_input = [trainset[name].values for name in sparse_features]#includes values from only data[user_id], data[movie_id]\n",
    "train_y= trainset[y].values\n",
    "\n",
    "test_model_input = [testset[name].values for name in sparse_features]#includes values from only data[user_id], data[movie_id]\n",
    "test_y= testset[y].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "2019-11-14 15:24:11,151\tINFO node.py:423 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-11-14_15-24-11_6541/logs.\n",
      "2019-11-14 15:24:11,299\tINFO services.py:363 -- Waiting for redis server at 127.0.0.1:10802 to respond...\n",
      "2019-11-14 15:24:11,430\tINFO services.py:363 -- Waiting for redis server at 127.0.0.1:36963 to respond...\n",
      "2019-11-14 15:24:11,438\tINFO services.py:760 -- Starting Redis shard with 20.0 GB max memory.\n",
      "2019-11-14 15:24:11,480\tINFO services.py:1384 -- Starting the Plasma object store with 1.0 GB memory using /dev/shm.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mlsquare.architectures.svd.SVD object at 0x7fcc60ff6be0> \n",
      " <class 'mlsquare.adapters.AdaptDeepctr.DeepCtr'>\n"
     ]
    }
   ],
   "source": [
    "#Step 1 from above\n",
    "from mlsquare.base import registry\n",
    "svd_model, svd_adapter =registry[('deepctr', 'SVD')]['default']\n",
    "print(svd_model, '\\n', svd_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "??svd_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "??svd_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = svd_adapter(proxy_model=svd_model)\n",
    "model = svd_adapter(proxy_model=svd_model, feature_columns= feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(train_model_input, train_y)\n",
    "#throws error with `tune.get_best_model()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kev/Desktop/pyvirtual2/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/kev/Desktop/pyvirtual2/lib/python3.6/site-packages/deepctr/layers/utils.py:156: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/kev/Desktop/pyvirtual2/lib/python3.6/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/kev/Desktop/pyvirtual2/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/32\n",
      " - 5s - loss: 5.2309 - mean_squared_error: 5.1988\n",
      "Epoch 2/32\n",
      " - 4s - loss: 1.1013 - mean_squared_error: 1.0347\n",
      "Epoch 3/32\n",
      " - 4s - loss: 1.0113 - mean_squared_error: 0.9367\n",
      "Epoch 4/32\n",
      " - 4s - loss: 0.9883 - mean_squared_error: 0.9104\n",
      "Epoch 5/32\n",
      " - 4s - loss: 0.9709 - mean_squared_error: 0.8906\n",
      "Epoch 6/32\n",
      " - 4s - loss: 0.9448 - mean_squared_error: 0.8626\n",
      "Epoch 7/32\n",
      " - 4s - loss: 0.9074 - mean_squared_error: 0.8233\n",
      "Epoch 8/32\n",
      " - 4s - loss: 0.8677 - mean_squared_error: 0.7811\n",
      "Epoch 9/32\n",
      " - 4s - loss: 0.8239 - mean_squared_error: 0.7345\n",
      "Epoch 10/32\n",
      " - 4s - loss: 0.7769 - mean_squared_error: 0.6843\n",
      "Epoch 11/32\n",
      " - 4s - loss: 0.7221 - mean_squared_error: 0.6256\n",
      "Epoch 12/32\n",
      " - 4s - loss: 0.6605 - mean_squared_error: 0.5593\n",
      "Epoch 13/32\n",
      " - 4s - loss: 0.5942 - mean_squared_error: 0.4876\n",
      "Epoch 14/32\n",
      " - 4s - loss: 0.5284 - mean_squared_error: 0.4157\n",
      "Epoch 15/32\n",
      " - 4s - loss: 0.4664 - mean_squared_error: 0.3472\n",
      "Epoch 16/32\n",
      " - 4s - loss: 0.4121 - mean_squared_error: 0.2864\n",
      "Epoch 17/32\n",
      " - 4s - loss: 0.3663 - mean_squared_error: 0.2345\n",
      "Epoch 18/32\n",
      " - 4s - loss: 0.3287 - mean_squared_error: 0.1913\n",
      "Epoch 19/32\n",
      " - 4s - loss: 0.2984 - mean_squared_error: 0.1562\n",
      "Epoch 20/32\n",
      " - 4s - loss: 0.2745 - mean_squared_error: 0.1281\n",
      "Epoch 21/32\n",
      " - 4s - loss: 0.2559 - mean_squared_error: 0.1060\n",
      "Epoch 22/32\n",
      " - 4s - loss: 0.2416 - mean_squared_error: 0.0889\n",
      "Epoch 23/32\n",
      " - 4s - loss: 0.2297 - mean_squared_error: 0.0747\n",
      "Epoch 24/32\n",
      " - 4s - loss: 0.2211 - mean_squared_error: 0.0645\n",
      "Epoch 25/32\n",
      " - 4s - loss: 0.2143 - mean_squared_error: 0.0563\n",
      "Epoch 26/32\n",
      " - 4s - loss: 0.2085 - mean_squared_error: 0.0495\n",
      "Epoch 27/32\n",
      " - 4s - loss: 0.2043 - mean_squared_error: 0.0443\n",
      "Epoch 28/32\n",
      " - 4s - loss: 0.2006 - mean_squared_error: 0.0400\n",
      "Epoch 29/32\n",
      " - 4s - loss: 0.1978 - mean_squared_error: 0.0368\n",
      "Epoch 30/32\n",
      " - 4s - loss: 0.1954 - mean_squared_error: 0.0341\n",
      "Epoch 31/32\n",
      " - 4s - loss: 0.1936 - mean_squared_error: 0.0321\n",
      "Epoch 32/32\n",
      " - 4s - loss: 0.1920 - mean_squared_error: 0.0304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcc60b1bd68>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_model_input, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = svd_adapter(proxy_model=svd_model)\n",
    "model = svd_adapter(proxy_model=svd_model, feature_columns= feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So in above, the model object along with feature columns are  passed into adapter.\n",
    "* It withholds `feature columns` vals as `params` till `X` & `y` are passed into `.fit()` method, and instantiates the model soon similar to Cell 22, but merely with `model.create_model()`, as `params` are passed implictly into `svd.SVD.create_model()`\n",
    "\n",
    "**Note: Optimum Model search is not functional, as the `tune.get_best_model()` method is bypassed for now.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Or Pass it onto dope along with adapter by making corresponding changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepctr\n",
    "from mlsquare import dope\n",
    "\n",
    "#m= dope(proxy_model= model, adapter=)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
