{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris['data']\n",
    "y = iris['target']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_nodes = clf.tree_.children_left[clf.tree_.children_left>0]\n",
    "right_nodes = clf.tree_.children_right[clf.tree_.children_right>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  3,  4,  5,  8, 10, 13, 14])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, 12,  7,  6,  9, 11, 16, 15])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.tree_.node_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract path\n",
    "node_indicator = clf.decision_path(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list = []\n",
    "for i, j in enumerate(X):\n",
    "    path_list.append(node_indicator.indices[node_indicator.indptr[i]:node_indicator.indptr[i+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int32)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert path to strings\n",
    "path_column = np.array([])\n",
    "for i, j in enumerate(X):\n",
    "    path_as_string = []\n",
    "    for node in path_list[i]:\n",
    "        if node == 0:\n",
    "            path_as_string.append('START')\n",
    "        elif node in left_nodes:\n",
    "            path_as_string.append('LEFT')\n",
    "        elif node in right_nodes:\n",
    "            path_as_string.append('RIGHT')\n",
    "            \n",
    "    path_as_string.append('END')\n",
    "    path_as_string = ' '.join(path_as_string)\n",
    "    path_column = np.append(path_column, path_as_string)\n",
    "\n",
    "# X[:,:-1] = path_column.reshape(-1,1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((X, path_column.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_sequence = X[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>START LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>START LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>START LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>START LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>START LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>START LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>START LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>START LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>START LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>START LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>START LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>START LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>START LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>START LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>START LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>START LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>START LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>START LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>START LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>START LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>START LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>START LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>START LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>START LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>START LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>START LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>START LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>START LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>START LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>START LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>START RIGHT RIGHT RIGHT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>START RIGHT RIGHT RIGHT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>START RIGHT RIGHT RIGHT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>START RIGHT RIGHT RIGHT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>START RIGHT RIGHT RIGHT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>START RIGHT RIGHT RIGHT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>START RIGHT RIGHT LEFT LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>START RIGHT RIGHT RIGHT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>START RIGHT RIGHT RIGHT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>START RIGHT LEFT RIGHT RIGHT RIGHT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>START RIGHT RIGHT RIGHT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>START RIGHT RIGHT RIGHT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>START RIGHT RIGHT RIGHT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>START RIGHT LEFT RIGHT LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>START RIGHT LEFT RIGHT LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>START RIGHT RIGHT RIGHT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>START RIGHT RIGHT RIGHT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>START RIGHT RIGHT RIGHT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>START RIGHT RIGHT LEFT LEFT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>START RIGHT RIGHT RIGHT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>START RIGHT RIGHT RIGHT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>START RIGHT RIGHT RIGHT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>START RIGHT RIGHT RIGHT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>START RIGHT RIGHT RIGHT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>START RIGHT RIGHT RIGHT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>START RIGHT RIGHT RIGHT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>START RIGHT RIGHT RIGHT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>START RIGHT RIGHT RIGHT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>START RIGHT RIGHT RIGHT END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>START RIGHT RIGHT RIGHT END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3                                       4\n",
       "0    5.1  3.5  1.4  0.2                          START LEFT END\n",
       "1    4.9  3.0  1.4  0.2                          START LEFT END\n",
       "2    4.7  3.2  1.3  0.2                          START LEFT END\n",
       "3    4.6  3.1  1.5  0.2                          START LEFT END\n",
       "4    5.0  3.6  1.4  0.2                          START LEFT END\n",
       "5    5.4  3.9  1.7  0.4                          START LEFT END\n",
       "6    4.6  3.4  1.4  0.3                          START LEFT END\n",
       "7    5.0  3.4  1.5  0.2                          START LEFT END\n",
       "8    4.4  2.9  1.4  0.2                          START LEFT END\n",
       "9    4.9  3.1  1.5  0.1                          START LEFT END\n",
       "10   5.4  3.7  1.5  0.2                          START LEFT END\n",
       "11   4.8  3.4  1.6  0.2                          START LEFT END\n",
       "12   4.8  3.0  1.4  0.1                          START LEFT END\n",
       "13   4.3  3.0  1.1  0.1                          START LEFT END\n",
       "14   5.8  4.0  1.2  0.2                          START LEFT END\n",
       "15   5.7  4.4  1.5  0.4                          START LEFT END\n",
       "16   5.4  3.9  1.3  0.4                          START LEFT END\n",
       "17   5.1  3.5  1.4  0.3                          START LEFT END\n",
       "18   5.7  3.8  1.7  0.3                          START LEFT END\n",
       "19   5.1  3.8  1.5  0.3                          START LEFT END\n",
       "20   5.4  3.4  1.7  0.2                          START LEFT END\n",
       "21   5.1  3.7  1.5  0.4                          START LEFT END\n",
       "22   4.6  3.6  1.0  0.2                          START LEFT END\n",
       "23   5.1  3.3  1.7  0.5                          START LEFT END\n",
       "24   4.8  3.4  1.9  0.2                          START LEFT END\n",
       "25   5.0  3.0  1.6  0.2                          START LEFT END\n",
       "26   5.0  3.4  1.6  0.4                          START LEFT END\n",
       "27   5.2  3.5  1.5  0.2                          START LEFT END\n",
       "28   5.2  3.4  1.4  0.2                          START LEFT END\n",
       "29   4.7  3.2  1.6  0.2                          START LEFT END\n",
       "..   ...  ...  ...  ...                                     ...\n",
       "120  6.9  3.2  5.7  2.3             START RIGHT RIGHT RIGHT END\n",
       "121  5.6  2.8  4.9  2.0             START RIGHT RIGHT RIGHT END\n",
       "122  7.7  2.8  6.7  2.0             START RIGHT RIGHT RIGHT END\n",
       "123  6.3  2.7  4.9  1.8             START RIGHT RIGHT RIGHT END\n",
       "124  6.7  3.3  5.7  2.1             START RIGHT RIGHT RIGHT END\n",
       "125  7.2  3.2  6.0  1.8             START RIGHT RIGHT RIGHT END\n",
       "126  6.2  2.8  4.8  1.8         START RIGHT RIGHT LEFT LEFT END\n",
       "127  6.1  3.0  4.9  1.8             START RIGHT RIGHT RIGHT END\n",
       "128  6.4  2.8  5.6  2.1             START RIGHT RIGHT RIGHT END\n",
       "129  7.2  3.0  5.8  1.6  START RIGHT LEFT RIGHT RIGHT RIGHT END\n",
       "130  7.4  2.8  6.1  1.9             START RIGHT RIGHT RIGHT END\n",
       "131  7.9  3.8  6.4  2.0             START RIGHT RIGHT RIGHT END\n",
       "132  6.4  2.8  5.6  2.2             START RIGHT RIGHT RIGHT END\n",
       "133  6.3  2.8  5.1  1.5         START RIGHT LEFT RIGHT LEFT END\n",
       "134  6.1  2.6  5.6  1.4         START RIGHT LEFT RIGHT LEFT END\n",
       "135  7.7  3.0  6.1  2.3             START RIGHT RIGHT RIGHT END\n",
       "136  6.3  3.4  5.6  2.4             START RIGHT RIGHT RIGHT END\n",
       "137  6.4  3.1  5.5  1.8             START RIGHT RIGHT RIGHT END\n",
       "138  6.0  3.0  4.8  1.8         START RIGHT RIGHT LEFT LEFT END\n",
       "139  6.9  3.1  5.4  2.1             START RIGHT RIGHT RIGHT END\n",
       "140  6.7  3.1  5.6  2.4             START RIGHT RIGHT RIGHT END\n",
       "141  6.9  3.1  5.1  2.3             START RIGHT RIGHT RIGHT END\n",
       "142  5.8  2.7  5.1  1.9             START RIGHT RIGHT RIGHT END\n",
       "143  6.8  3.2  5.9  2.3             START RIGHT RIGHT RIGHT END\n",
       "144  6.7  3.3  5.7  2.5             START RIGHT RIGHT RIGHT END\n",
       "145  6.7  3.0  5.2  2.3             START RIGHT RIGHT RIGHT END\n",
       "146  6.3  2.5  5.0  1.9             START RIGHT RIGHT RIGHT END\n",
       "147  6.5  3.0  5.2  2.0             START RIGHT RIGHT RIGHT END\n",
       "148  6.2  3.4  5.4  2.3             START RIGHT RIGHT RIGHT END\n",
       "149  5.9  3.0  5.1  1.8             START RIGHT RIGHT RIGHT END\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tok = Tokenizer()\n",
    "\n",
    "tok.fit_on_texts(data[4])\n",
    "\n",
    "## Fixed input variable output.\n",
    "## Search for the right arch. Not the cells/states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idx = tok.word_index\n",
    "idx_word = tok.index_word\n",
    "num_words = len(word_idx) + 1\n",
    "word_counts = tok.word_counts\n",
    "\n",
    "# Convert text to sequences of integers\n",
    "sequences = tok.texts_to_sequences(data[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 2, 1, 2, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 2, 2, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 2, 2, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 1, 1, 1, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 1, 1, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 1, 2, 1, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 1, 1, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 1, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 1, 2, 1, 4],\n",
       " [3, 2, 1, 2, 1, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 1, 1, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4],\n",
       " [3, 2, 2, 2, 4]]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "np.asarray(sequences)\n",
    "\n",
    "s_array = np.array(np.array(x) for x in sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_df = pd.DataFrame(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1  2    3    4    5    6\n",
       "0    3  1  4  NaN  NaN  NaN  NaN\n",
       "1    3  1  4  NaN  NaN  NaN  NaN\n",
       "2    3  1  4  NaN  NaN  NaN  NaN\n",
       "3    3  1  4  NaN  NaN  NaN  NaN\n",
       "4    3  1  4  NaN  NaN  NaN  NaN\n",
       "5    3  1  4  NaN  NaN  NaN  NaN\n",
       "6    3  1  4  NaN  NaN  NaN  NaN\n",
       "7    3  1  4  NaN  NaN  NaN  NaN\n",
       "8    3  1  4  NaN  NaN  NaN  NaN\n",
       "9    3  1  4  NaN  NaN  NaN  NaN\n",
       "10   3  1  4  NaN  NaN  NaN  NaN\n",
       "11   3  1  4  NaN  NaN  NaN  NaN\n",
       "12   3  1  4  NaN  NaN  NaN  NaN\n",
       "13   3  1  4  NaN  NaN  NaN  NaN\n",
       "14   3  1  4  NaN  NaN  NaN  NaN\n",
       "15   3  1  4  NaN  NaN  NaN  NaN\n",
       "16   3  1  4  NaN  NaN  NaN  NaN\n",
       "17   3  1  4  NaN  NaN  NaN  NaN\n",
       "18   3  1  4  NaN  NaN  NaN  NaN\n",
       "19   3  1  4  NaN  NaN  NaN  NaN\n",
       "20   3  1  4  NaN  NaN  NaN  NaN\n",
       "21   3  1  4  NaN  NaN  NaN  NaN\n",
       "22   3  1  4  NaN  NaN  NaN  NaN\n",
       "23   3  1  4  NaN  NaN  NaN  NaN\n",
       "24   3  1  4  NaN  NaN  NaN  NaN\n",
       "25   3  1  4  NaN  NaN  NaN  NaN\n",
       "26   3  1  4  NaN  NaN  NaN  NaN\n",
       "27   3  1  4  NaN  NaN  NaN  NaN\n",
       "28   3  1  4  NaN  NaN  NaN  NaN\n",
       "29   3  1  4  NaN  NaN  NaN  NaN\n",
       "..  .. .. ..  ...  ...  ...  ...\n",
       "120  3  2  2  2.0  4.0  NaN  NaN\n",
       "121  3  2  2  2.0  4.0  NaN  NaN\n",
       "122  3  2  2  2.0  4.0  NaN  NaN\n",
       "123  3  2  2  2.0  4.0  NaN  NaN\n",
       "124  3  2  2  2.0  4.0  NaN  NaN\n",
       "125  3  2  2  2.0  4.0  NaN  NaN\n",
       "126  3  2  2  1.0  1.0  4.0  NaN\n",
       "127  3  2  2  2.0  4.0  NaN  NaN\n",
       "128  3  2  2  2.0  4.0  NaN  NaN\n",
       "129  3  2  1  2.0  2.0  2.0  4.0\n",
       "130  3  2  2  2.0  4.0  NaN  NaN\n",
       "131  3  2  2  2.0  4.0  NaN  NaN\n",
       "132  3  2  2  2.0  4.0  NaN  NaN\n",
       "133  3  2  1  2.0  1.0  4.0  NaN\n",
       "134  3  2  1  2.0  1.0  4.0  NaN\n",
       "135  3  2  2  2.0  4.0  NaN  NaN\n",
       "136  3  2  2  2.0  4.0  NaN  NaN\n",
       "137  3  2  2  2.0  4.0  NaN  NaN\n",
       "138  3  2  2  1.0  1.0  4.0  NaN\n",
       "139  3  2  2  2.0  4.0  NaN  NaN\n",
       "140  3  2  2  2.0  4.0  NaN  NaN\n",
       "141  3  2  2  2.0  4.0  NaN  NaN\n",
       "142  3  2  2  2.0  4.0  NaN  NaN\n",
       "143  3  2  2  2.0  4.0  NaN  NaN\n",
       "144  3  2  2  2.0  4.0  NaN  NaN\n",
       "145  3  2  2  2.0  4.0  NaN  NaN\n",
       "146  3  2  2  2.0  4.0  NaN  NaN\n",
       "147  3  2  2  2.0  4.0  NaN  NaN\n",
       "148  3  2  2  2.0  4.0  NaN  NaN\n",
       "149  3  2  2  2.0  4.0  NaN  NaN\n",
       "\n",
       "[150 rows x 7 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      4\n",
       "1      4\n",
       "2      4\n",
       "3      4\n",
       "4      4\n",
       "5      4\n",
       "6      4\n",
       "7      4\n",
       "8      4\n",
       "9      4\n",
       "10     4\n",
       "11     4\n",
       "12     4\n",
       "13     4\n",
       "14     4\n",
       "15     4\n",
       "16     4\n",
       "17     4\n",
       "18     4\n",
       "19     4\n",
       "20     4\n",
       "21     4\n",
       "22     4\n",
       "23     4\n",
       "24     4\n",
       "25     4\n",
       "26     4\n",
       "27     4\n",
       "28     4\n",
       "29     4\n",
       "      ..\n",
       "120    2\n",
       "121    2\n",
       "122    2\n",
       "123    2\n",
       "124    2\n",
       "125    2\n",
       "126    2\n",
       "127    2\n",
       "128    2\n",
       "129    1\n",
       "130    2\n",
       "131    2\n",
       "132    2\n",
       "133    1\n",
       "134    1\n",
       "135    2\n",
       "136    2\n",
       "137    2\n",
       "138    2\n",
       "139    2\n",
       "140    2\n",
       "141    2\n",
       "142    2\n",
       "143    2\n",
       "144    2\n",
       "145    2\n",
       "146    2\n",
       "147    2\n",
       "148    2\n",
       "149    2\n",
       "Name: 2, Length: 150, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Trial 1\n",
    "## Add first 2 steps as feature and predict third step\n",
    "## Merge X and first 2 columns from steps\n",
    "## Extract 3rd step as target\n",
    "## OHE this target\n",
    "## Train on SimpleRNN and predict\n",
    "\n",
    "feature = data.iloc[:,:4]\n",
    "\n",
    "feature[5] = s_df.iloc[:,0]\n",
    "feature[6] = s_df.iloc[:,1]\n",
    "\n",
    "target = s_df.iloc[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3  5  6\n",
       "0    5.1  3.5  1.4  0.2  3  1\n",
       "1    4.9  3.0  1.4  0.2  3  1\n",
       "2    4.7  3.2  1.3  0.2  3  1\n",
       "3    4.6  3.1  1.5  0.2  3  1\n",
       "4    5.0  3.6  1.4  0.2  3  1\n",
       "5    5.4  3.9  1.7  0.4  3  1\n",
       "6    4.6  3.4  1.4  0.3  3  1\n",
       "7    5.0  3.4  1.5  0.2  3  1\n",
       "8    4.4  2.9  1.4  0.2  3  1\n",
       "9    4.9  3.1  1.5  0.1  3  1\n",
       "10   5.4  3.7  1.5  0.2  3  1\n",
       "11   4.8  3.4  1.6  0.2  3  1\n",
       "12   4.8  3.0  1.4  0.1  3  1\n",
       "13   4.3  3.0  1.1  0.1  3  1\n",
       "14   5.8  4.0  1.2  0.2  3  1\n",
       "15   5.7  4.4  1.5  0.4  3  1\n",
       "16   5.4  3.9  1.3  0.4  3  1\n",
       "17   5.1  3.5  1.4  0.3  3  1\n",
       "18   5.7  3.8  1.7  0.3  3  1\n",
       "19   5.1  3.8  1.5  0.3  3  1\n",
       "20   5.4  3.4  1.7  0.2  3  1\n",
       "21   5.1  3.7  1.5  0.4  3  1\n",
       "22   4.6  3.6  1.0  0.2  3  1\n",
       "23   5.1  3.3  1.7  0.5  3  1\n",
       "24   4.8  3.4  1.9  0.2  3  1\n",
       "25   5.0  3.0  1.6  0.2  3  1\n",
       "26   5.0  3.4  1.6  0.4  3  1\n",
       "27   5.2  3.5  1.5  0.2  3  1\n",
       "28   5.2  3.4  1.4  0.2  3  1\n",
       "29   4.7  3.2  1.6  0.2  3  1\n",
       "..   ...  ...  ...  ... .. ..\n",
       "120  6.9  3.2  5.7  2.3  3  2\n",
       "121  5.6  2.8  4.9  2.0  3  2\n",
       "122  7.7  2.8  6.7  2.0  3  2\n",
       "123  6.3  2.7  4.9  1.8  3  2\n",
       "124  6.7  3.3  5.7  2.1  3  2\n",
       "125  7.2  3.2  6.0  1.8  3  2\n",
       "126  6.2  2.8  4.8  1.8  3  2\n",
       "127  6.1  3.0  4.9  1.8  3  2\n",
       "128  6.4  2.8  5.6  2.1  3  2\n",
       "129  7.2  3.0  5.8  1.6  3  2\n",
       "130  7.4  2.8  6.1  1.9  3  2\n",
       "131  7.9  3.8  6.4  2.0  3  2\n",
       "132  6.4  2.8  5.6  2.2  3  2\n",
       "133  6.3  2.8  5.1  1.5  3  2\n",
       "134  6.1  2.6  5.6  1.4  3  2\n",
       "135  7.7  3.0  6.1  2.3  3  2\n",
       "136  6.3  3.4  5.6  2.4  3  2\n",
       "137  6.4  3.1  5.5  1.8  3  2\n",
       "138  6.0  3.0  4.8  1.8  3  2\n",
       "139  6.9  3.1  5.4  2.1  3  2\n",
       "140  6.7  3.1  5.6  2.4  3  2\n",
       "141  6.9  3.1  5.1  2.3  3  2\n",
       "142  5.8  2.7  5.1  1.9  3  2\n",
       "143  6.8  3.2  5.9  2.3  3  2\n",
       "144  6.7  3.3  5.7  2.5  3  2\n",
       "145  6.7  3.0  5.2  2.3  3  2\n",
       "146  6.3  2.5  5.0  1.9  3  2\n",
       "147  6.5  3.0  5.2  2.0  3  2\n",
       "148  6.2  3.4  5.4  2.3  3  2\n",
       "149  5.9  3.0  5.1  1.8  3  2\n",
       "\n",
       "[150 rows x 6 columns]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target = to_categorical(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = feature[:100]\n",
    "y_train = target[:100]\n",
    "x_test = feature[100:150]\n",
    "y_test = target[100:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes\n",
    "\n",
    "1) Split into 2 models\n",
    "2) First model - basic feed forward. Second model - LSTM sequence to seq model\n",
    "3) Concat 2 models\n",
    "4) Merge at Dense to Dense(Hidden layers of both models)\n",
    "5) Fix the seq generation part first. Then move on to attaching the prior model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional, SimpleRNN\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model using SimpleRNN - Phase I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_21 (SimpleRNN)    (None, 32)                1248      \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 5)                 45        \n",
      "=================================================================\n",
      "Total params: 1,557\n",
      "Trainable params: 1,557\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense_1)\n",
    "model.add(SimpleRNN(units=32, input_shape=(1,6), activation=\"sigmoid\"))\n",
    "model.add(Dense_2(8, activation=\"sigmoid\"))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(5))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 2.4749 - acc: 0.0100\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 0s 63us/step - loss: 2.0716 - acc: 0.0100\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 0s 115us/step - loss: 1.8402 - acc: 0.0100\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 0s 83us/step - loss: 1.6888 - acc: 0.0100\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 0s 75us/step - loss: 1.5753 - acc: 0.0100\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 0s 98us/step - loss: 1.4861 - acc: 0.0100\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 0s 70us/step - loss: 1.4127 - acc: 0.0100\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 0s 106us/step - loss: 1.3496 - acc: 0.0100\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 0s 84us/step - loss: 1.2927 - acc: 0.0100\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 0s 61us/step - loss: 1.2407 - acc: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1cb9893d68>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.reshape(x_train.values,(100,1,6)), y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4500318574905395, 0.0]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(np.reshape(x_test.values,(50,1,6)), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(np.reshape(x_test.values,(50,1,6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-a822db545a5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/test_imly/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test_imly/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test_imly/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test_imly/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test_imly/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_single_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 'Got tensor with shape: %s' % str(shape))\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "model.predict(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple RNN Phase II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ml_master approach\n",
    "# \n",
    "\n",
    "for i in range(5):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LSTM network and generate text\n",
    "\n",
    "# import sys\n",
    "# import numpy\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import Dropout\n",
    "# from keras.layers import LSTM\n",
    "# from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.utils import np_utils\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers, and a reverse mapping\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print \"Total Characters: \", n_chars\n",
    "print \"Total Vocab: \", n_vocab\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print \"Total Patterns: \", n_patterns\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "# load the network weights\n",
    "filename = \"weights-improvement-19-1.9435.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print \"Seed:\"\n",
    "print \"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\"\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print \"\\nDone.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different types of RNN implementaions\n",
    "\n",
    "1) one-to-many\n",
    "2) many-to-many\n",
    "3) many-to-one(low priority)\n",
    "4) Simple RNN layer\n",
    "5) LSTM and GRU specific layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### seq2seq encoder-decoder arch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = '../data/fra.txt'\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " ...]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(txt) for txt in input_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 70\n",
      "Number of unique output tokens: 93\n",
      "Max sequence length for inputs: 16\n",
      "Max sequence length for outputs: 59\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t\n",
      "1 Ç\n",
      "2 a\n",
      "3  \n",
      "4 n\n",
      "5 '\n",
      "6 e\n",
      "7 n\n",
      "8  \n",
      "9 e\n",
      "10 s\n",
      "11 t\n",
      "12  \n",
      "13 p\n",
      "14 a\n",
      "15 s\n",
      "16  \n",
      "17 l\n",
      "18 a\n",
      "19  \n",
      "20 r\n",
      "21 a\n",
      "22 i\n",
      "23 s\n",
      "24 o\n",
      "25 n\n",
      "26 .\n",
      "27 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t, char in enumerate(target_texts[8888]):\n",
    "    print(t, char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[0][3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_index[' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input -- 1)G 2)o 3).\n",
    "target -- 1)o 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shakkeel/anaconda3/envs/test_imly/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens)) ## Layer\n",
    "\n",
    "encoder = LSTM(latent_dim, return_state=True) ## Layer\n",
    "\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "## Tensor\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens)) ## Layer\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the \n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) ## Layer\n",
    "\n",
    "## Tensor\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states) \n",
    "\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax') ## Layer\n",
    "\n",
    "## Tensor\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 25s 3ms/step - loss: 0.4026 - acc: 0.8789 - val_loss: 0.4950 - val_acc: 0.8527\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 24s 3ms/step - loss: 0.3830 - acc: 0.8844 - val_loss: 0.4797 - val_acc: 0.8581\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 24s 3ms/step - loss: 0.3682 - acc: 0.8889 - val_loss: 0.4761 - val_acc: 0.8586\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 23s 3ms/step - loss: 0.3539 - acc: 0.8933 - val_loss: 0.4674 - val_acc: 0.8619\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 24s 3ms/step - loss: 0.3402 - acc: 0.8974 - val_loss: 0.4565 - val_acc: 0.8660\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 24s 3ms/step - loss: 0.3279 - acc: 0.9011 - val_loss: 0.4583 - val_acc: 0.8652\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 23s 3ms/step - loss: 0.3162 - acc: 0.9046 - val_loss: 0.4521 - val_acc: 0.8673\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 23s 3ms/step - loss: 0.3049 - acc: 0.9082 - val_loss: 0.4459 - val_acc: 0.8693\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 24s 3ms/step - loss: 0.2941 - acc: 0.9116 - val_loss: 0.4436 - val_acc: 0.8703\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 24s 3ms/step - loss: 0.2846 - acc: 0.9140 - val_loss: 0.4443 - val_acc: 0.8708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f33ab5ef1d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 70)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 93)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 256), (None, 334848      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 256),  358400      input_2[0][0]                    \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 93)     23901       lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 717,149\n",
      "Trainable params: 717,149\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "Inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, None, 93)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 256),  358400      input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 93)     23901       lstm_2[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 382,301\n",
      "Trainable params: 382,301\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    ## Not required\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            print(sampled_char, 'sampled_char')\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " sampled_char\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Vanez vous !\n",
      "\n",
      "\n",
      " sampled_char\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Appelle-moi !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(2):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.0000000e+00,  9.9727798e-01,  7.3152319e-02, ...,\n",
       "         -2.2900421e-02, -8.5353502e-05,  8.3148248e-02],\n",
       "        [-2.1999063e-02,  7.8671139e-01,  2.8085017e-01, ...,\n",
       "         -8.6984085e-03, -3.8662799e-02,  1.2595142e-01],\n",
       "        [-2.1999063e-02,  7.8671139e-01,  2.8085017e-01, ...,\n",
       "         -8.6984085e-03, -3.8662799e-02,  1.2595142e-01],\n",
       "        ...,\n",
       "        [-8.2788771e-01, -1.3016815e-01, -1.0624073e-01, ...,\n",
       "          7.8373319e-01, -7.0865506e-01,  8.0582792e-01],\n",
       "        [-8.2788771e-01, -1.3016815e-01, -1.0624073e-01, ...,\n",
       "          7.8373319e-01, -7.0865506e-01,  8.0582792e-01],\n",
       "        [-7.9371059e-01, -5.8884572e-02, -8.0344379e-02, ...,\n",
       "          7.4868083e-01, -6.2814301e-01,  7.6303524e-01]], dtype=float32),\n",
       " array([[-4.6603739e-01,  3.2990930e+00,  2.3122531e-01, ...,\n",
       "         -5.3472046e-02, -1.6242655e-03,  3.6086869e-01],\n",
       "        [-1.4004925e-01,  1.0627428e+00,  1.5270593e+00, ...,\n",
       "         -1.6727488e-02, -3.1406999e-01,  3.1245118e-01],\n",
       "        [-1.4004925e-01,  1.0627428e+00,  1.5270593e+00, ...,\n",
       "         -1.6727488e-02, -3.1406999e-01,  3.1245118e-01],\n",
       "        ...,\n",
       "        [-1.3415442e+00, -3.1713909e-01, -6.3547764e+00, ...,\n",
       "          1.5920277e+00, -1.4131223e+00,  1.7034196e+00],\n",
       "        [-1.3415442e+00, -3.1713909e-01, -6.3547764e+00, ...,\n",
       "          1.5920277e+00, -1.4131223e+00,  1.7034196e+00],\n",
       "        [-1.2946978e+00, -1.3315959e-01, -6.3590813e+00, ...,\n",
       "          1.3054514e+00, -1.0419675e+00,  1.6103653e+00]], dtype=float32)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_enc_state = encoder_model.predict(encoder_input_data)\n",
    "test_enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "\n",
    "test_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "test_seq[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output_tokens, h, c = decoder_model.predict(\n",
    "    [test_seq] + test_enc_state)\n",
    "\n",
    "test_output_tokens\n",
    "\n",
    "# Sample a token\n",
    "sampled_token_index = np.argmax(test_output_tokens[0, -1, :])\n",
    "sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "decoded_sentence += sampled_char\n",
    "\n",
    "# # Exit condition: either hit max length\n",
    "# # or find stop character.\n",
    "# if (sampled_char == '\\n' or\n",
    "#    len(decoded_sentence) > max_decoder_seq_length):\n",
    "#     stop_condition = True\n",
    "\n",
    "# # Update the target sequence (of length 1).\n",
    "# target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "# target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "# # Update states\n",
    "# states_value = [h, c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.73300201e-07, 3.03275538e-05, 1.27218418e-05, 3.30568640e-04,\n",
       "       2.09549080e-06, 3.71295619e-06, 2.17011871e-06, 2.02225827e-04,\n",
       "       1.98703447e-06, 2.15120690e-06, 1.36676463e-04, 4.23320744e-05,\n",
       "       2.46914933e-05, 1.85746794e-05, 5.50231789e-06, 2.17610182e-06,\n",
       "       7.31857244e-06, 2.72943021e-06, 3.20451272e-06, 1.08500144e-05,\n",
       "       2.06395671e-05, 3.79672070e-04, 2.17986792e-01, 3.85626704e-02,\n",
       "       3.89185026e-02, 1.72262266e-02, 2.15417072e-02, 6.39404729e-03,\n",
       "       1.25134792e-02, 1.51080813e-03, 1.18286000e-03, 1.20434063e-02,\n",
       "       2.13664644e-05, 1.02319298e-02, 6.03328459e-03, 7.93812238e-03,\n",
       "       7.57724792e-03, 3.89381982e-02, 1.07045064e-03, 1.17902964e-01,\n",
       "       1.40626729e-01, 3.41476686e-02, 1.73304885e-04, 2.26715490e-01,\n",
       "       1.00500954e-04, 8.00746318e-04, 1.03203296e-04, 7.24172220e-04,\n",
       "       4.33673762e-04, 2.60727102e-04, 3.49547365e-04, 5.21993570e-05,\n",
       "       2.62967078e-04, 4.02412697e-04, 2.83126516e-04, 3.38969039e-05,\n",
       "       3.59246624e-04, 2.13839041e-04, 7.40392716e-05, 1.29453911e-04,\n",
       "       1.09963643e-03, 2.74526858e-04, 2.10841943e-04, 2.99357867e-04,\n",
       "       2.32307648e-04, 5.99519408e-04, 8.93852266e-04, 1.92609470e-04,\n",
       "       1.01466991e-04, 2.61340374e-05, 7.86599339e-05, 1.48353602e-05,\n",
       "       6.05774676e-06, 8.22272338e-03, 7.89230107e-04, 1.82645526e-02,\n",
       "       7.78256363e-05, 3.45502893e-04, 3.31934949e-04, 2.93544261e-04,\n",
       "       4.58984112e-04, 1.23095547e-03, 1.56571943e-04, 8.44721626e-07,\n",
       "       9.23459666e-05, 7.62977143e-05, 5.32767408e-05, 6.34749824e-07,\n",
       "       7.30186512e-05, 4.13285088e-05, 5.38776621e-05, 4.37590570e-05,\n",
       "       2.78777414e-04], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output_tokens[0, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'V'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_token_index\n",
    "\n",
    "sampled_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### seq2seq trial on `explain`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_input = Input((4,))\n",
    "\n",
    "model1_output = Dense(units=3, activation='sigmoid')(model1_input)\n",
    "\n",
    "model1 = Model(model1_input, model1_output)\n",
    "\n",
    "model1.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 15\n",
      "Trainable params: 15\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "150/150 [==============================] - 0s 34us/step - loss: 1.2415 - acc: 0.3667\n",
      "Epoch 2/100\n",
      "150/150 [==============================] - 0s 47us/step - loss: 1.2189 - acc: 0.3667\n",
      "Epoch 3/100\n",
      "150/150 [==============================] - 0s 30us/step - loss: 1.1996 - acc: 0.3733\n",
      "Epoch 4/100\n",
      "150/150 [==============================] - 0s 27us/step - loss: 1.1808 - acc: 0.3733\n",
      "Epoch 5/100\n",
      "150/150 [==============================] - 0s 45us/step - loss: 1.1613 - acc: 0.3733\n",
      "Epoch 6/100\n",
      "150/150 [==============================] - 0s 31us/step - loss: 1.1439 - acc: 0.3733\n",
      "Epoch 7/100\n",
      "150/150 [==============================] - 0s 19us/step - loss: 1.1264 - acc: 0.3733\n",
      "Epoch 8/100\n",
      "150/150 [==============================] - 0s 22us/step - loss: 1.1110 - acc: 0.3800\n",
      "Epoch 9/100\n",
      "150/150 [==============================] - 0s 24us/step - loss: 1.0946 - acc: 0.3800\n",
      "Epoch 10/100\n",
      "150/150 [==============================] - 0s 27us/step - loss: 1.0789 - acc: 0.3800\n",
      "Epoch 11/100\n",
      "150/150 [==============================] - 0s 32us/step - loss: 1.0652 - acc: 0.3800\n",
      "Epoch 12/100\n",
      "150/150 [==============================] - 0s 21us/step - loss: 1.0514 - acc: 0.3800\n",
      "Epoch 13/100\n",
      "150/150 [==============================] - 0s 25us/step - loss: 1.0383 - acc: 0.4000\n",
      "Epoch 14/100\n",
      "150/150 [==============================] - 0s 25us/step - loss: 1.0260 - acc: 0.4067\n",
      "Epoch 15/100\n",
      "150/150 [==============================] - 0s 32us/step - loss: 1.0146 - acc: 0.4133\n",
      "Epoch 16/100\n",
      "150/150 [==============================] - 0s 24us/step - loss: 1.0028 - acc: 0.4200\n",
      "Epoch 17/100\n",
      "150/150 [==============================] - 0s 40us/step - loss: 0.9919 - acc: 0.4333\n",
      "Epoch 18/100\n",
      "150/150 [==============================] - 0s 41us/step - loss: 0.9821 - acc: 0.4533\n",
      "Epoch 19/100\n",
      "150/150 [==============================] - 0s 36us/step - loss: 0.9719 - acc: 0.4667\n",
      "Epoch 20/100\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.9616 - acc: 0.4800\n",
      "Epoch 21/100\n",
      "150/150 [==============================] - 0s 31us/step - loss: 0.9521 - acc: 0.4933\n",
      "Epoch 22/100\n",
      "150/150 [==============================] - 0s 40us/step - loss: 0.9427 - acc: 0.5200\n",
      "Epoch 23/100\n",
      "150/150 [==============================] - 0s 42us/step - loss: 0.9331 - acc: 0.5267\n",
      "Epoch 24/100\n",
      "150/150 [==============================] - 0s 33us/step - loss: 0.9245 - acc: 0.5533\n",
      "Epoch 25/100\n",
      "150/150 [==============================] - 0s 25us/step - loss: 0.9153 - acc: 0.5867\n",
      "Epoch 26/100\n",
      "150/150 [==============================] - 0s 29us/step - loss: 0.9064 - acc: 0.5933\n",
      "Epoch 27/100\n",
      "150/150 [==============================] - 0s 25us/step - loss: 0.8978 - acc: 0.6000\n",
      "Epoch 28/100\n",
      "150/150 [==============================] - 0s 28us/step - loss: 0.8892 - acc: 0.6133\n",
      "Epoch 29/100\n",
      "150/150 [==============================] - 0s 38us/step - loss: 0.8810 - acc: 0.6200\n",
      "Epoch 30/100\n",
      "150/150 [==============================] - 0s 31us/step - loss: 0.8723 - acc: 0.6400\n",
      "Epoch 31/100\n",
      "150/150 [==============================] - 0s 23us/step - loss: 0.8643 - acc: 0.6533\n",
      "Epoch 32/100\n",
      "150/150 [==============================] - 0s 22us/step - loss: 0.8560 - acc: 0.6533\n",
      "Epoch 33/100\n",
      "150/150 [==============================] - 0s 21us/step - loss: 0.8484 - acc: 0.6533\n",
      "Epoch 34/100\n",
      "150/150 [==============================] - 0s 37us/step - loss: 0.8404 - acc: 0.6600\n",
      "Epoch 35/100\n",
      "150/150 [==============================] - 0s 36us/step - loss: 0.8326 - acc: 0.6600\n",
      "Epoch 36/100\n",
      "150/150 [==============================] - 0s 25us/step - loss: 0.8251 - acc: 0.6600\n",
      "Epoch 37/100\n",
      "150/150 [==============================] - 0s 31us/step - loss: 0.8177 - acc: 0.6600\n",
      "Epoch 38/100\n",
      "150/150 [==============================] - 0s 29us/step - loss: 0.8101 - acc: 0.6600\n",
      "Epoch 39/100\n",
      "150/150 [==============================] - 0s 31us/step - loss: 0.8032 - acc: 0.6600\n",
      "Epoch 40/100\n",
      "150/150 [==============================] - 0s 53us/step - loss: 0.7962 - acc: 0.6600\n",
      "Epoch 41/100\n",
      "150/150 [==============================] - 0s 37us/step - loss: 0.7893 - acc: 0.6600\n",
      "Epoch 42/100\n",
      "150/150 [==============================] - 0s 27us/step - loss: 0.7826 - acc: 0.6600\n",
      "Epoch 43/100\n",
      "150/150 [==============================] - 0s 50us/step - loss: 0.7763 - acc: 0.6600\n",
      "Epoch 44/100\n",
      "150/150 [==============================] - 0s 27us/step - loss: 0.7699 - acc: 0.6600\n",
      "Epoch 45/100\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.7638 - acc: 0.6600\n",
      "Epoch 46/100\n",
      "150/150 [==============================] - 0s 31us/step - loss: 0.7580 - acc: 0.6600\n",
      "Epoch 47/100\n",
      "150/150 [==============================] - 0s 24us/step - loss: 0.7519 - acc: 0.6600\n",
      "Epoch 48/100\n",
      "150/150 [==============================] - 0s 32us/step - loss: 0.7467 - acc: 0.6600\n",
      "Epoch 49/100\n",
      "150/150 [==============================] - 0s 33us/step - loss: 0.7411 - acc: 0.6667\n",
      "Epoch 50/100\n",
      "150/150 [==============================] - 0s 41us/step - loss: 0.7368 - acc: 0.6667\n",
      "Epoch 51/100\n",
      "150/150 [==============================] - 0s 37us/step - loss: 0.7315 - acc: 0.6667\n",
      "Epoch 52/100\n",
      "150/150 [==============================] - 0s 28us/step - loss: 0.7267 - acc: 0.6667\n",
      "Epoch 53/100\n",
      "150/150 [==============================] - 0s 28us/step - loss: 0.7227 - acc: 0.6800\n",
      "Epoch 54/100\n",
      "150/150 [==============================] - 0s 28us/step - loss: 0.7182 - acc: 0.6800\n",
      "Epoch 55/100\n",
      "150/150 [==============================] - 0s 27us/step - loss: 0.7144 - acc: 0.6933\n",
      "Epoch 56/100\n",
      "150/150 [==============================] - 0s 33us/step - loss: 0.7106 - acc: 0.6867\n",
      "Epoch 57/100\n",
      "150/150 [==============================] - 0s 50us/step - loss: 0.7070 - acc: 0.7000\n",
      "Epoch 58/100\n",
      "150/150 [==============================] - 0s 41us/step - loss: 0.7037 - acc: 0.6933\n",
      "Epoch 59/100\n",
      "150/150 [==============================] - 0s 40us/step - loss: 0.7002 - acc: 0.6933\n",
      "Epoch 60/100\n",
      "150/150 [==============================] - 0s 51us/step - loss: 0.6971 - acc: 0.7000\n",
      "Epoch 61/100\n",
      "150/150 [==============================] - 0s 26us/step - loss: 0.6940 - acc: 0.6933\n",
      "Epoch 62/100\n",
      "150/150 [==============================] - 0s 34us/step - loss: 0.6908 - acc: 0.7000\n",
      "Epoch 63/100\n",
      "150/150 [==============================] - 0s 47us/step - loss: 0.6880 - acc: 0.6933\n",
      "Epoch 64/100\n",
      "150/150 [==============================] - 0s 53us/step - loss: 0.6852 - acc: 0.6867\n",
      "Epoch 65/100\n",
      "150/150 [==============================] - 0s 36us/step - loss: 0.6823 - acc: 0.6933\n",
      "Epoch 66/100\n",
      "150/150 [==============================] - 0s 42us/step - loss: 0.6795 - acc: 0.6933\n",
      "Epoch 67/100\n",
      "150/150 [==============================] - 0s 31us/step - loss: 0.6768 - acc: 0.6933\n",
      "Epoch 68/100\n",
      "150/150 [==============================] - 0s 35us/step - loss: 0.6741 - acc: 0.6933\n",
      "Epoch 69/100\n",
      "150/150 [==============================] - 0s 24us/step - loss: 0.6714 - acc: 0.6933\n",
      "Epoch 70/100\n",
      "150/150 [==============================] - 0s 24us/step - loss: 0.6688 - acc: 0.7000\n",
      "Epoch 71/100\n",
      "150/150 [==============================] - 0s 50us/step - loss: 0.6663 - acc: 0.7067\n",
      "Epoch 72/100\n",
      "150/150 [==============================] - 0s 23us/step - loss: 0.6637 - acc: 0.7067\n",
      "Epoch 73/100\n",
      "150/150 [==============================] - 0s 38us/step - loss: 0.6611 - acc: 0.7067\n",
      "Epoch 74/100\n",
      "150/150 [==============================] - 0s 23us/step - loss: 0.6586 - acc: 0.7067\n",
      "Epoch 75/100\n",
      "150/150 [==============================] - 0s 33us/step - loss: 0.6560 - acc: 0.7000\n",
      "Epoch 76/100\n",
      "150/150 [==============================] - 0s 23us/step - loss: 0.6536 - acc: 0.7067\n",
      "Epoch 77/100\n",
      "150/150 [==============================] - 0s 37us/step - loss: 0.6513 - acc: 0.7067\n",
      "Epoch 78/100\n",
      "150/150 [==============================] - 0s 26us/step - loss: 0.6488 - acc: 0.7067\n",
      "Epoch 79/100\n",
      "150/150 [==============================] - 0s 29us/step - loss: 0.6463 - acc: 0.7067\n",
      "Epoch 80/100\n",
      "150/150 [==============================] - 0s 37us/step - loss: 0.6439 - acc: 0.7067\n",
      "Epoch 81/100\n",
      "150/150 [==============================] - 0s 47us/step - loss: 0.6417 - acc: 0.7067\n",
      "Epoch 82/100\n",
      "150/150 [==============================] - 0s 58us/step - loss: 0.6393 - acc: 0.7067\n",
      "Epoch 83/100\n",
      "150/150 [==============================] - 0s 67us/step - loss: 0.6371 - acc: 0.7067\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 34us/step - loss: 0.6348 - acc: 0.7067\n",
      "Epoch 85/100\n",
      "150/150 [==============================] - 0s 27us/step - loss: 0.6327 - acc: 0.7133\n",
      "Epoch 86/100\n",
      "150/150 [==============================] - 0s 28us/step - loss: 0.6304 - acc: 0.7133\n",
      "Epoch 87/100\n",
      "150/150 [==============================] - 0s 33us/step - loss: 0.6282 - acc: 0.7200\n",
      "Epoch 88/100\n",
      "150/150 [==============================] - 0s 31us/step - loss: 0.6260 - acc: 0.7133\n",
      "Epoch 89/100\n",
      "150/150 [==============================] - 0s 28us/step - loss: 0.6239 - acc: 0.7200\n",
      "Epoch 90/100\n",
      "150/150 [==============================] - 0s 37us/step - loss: 0.6218 - acc: 0.7333\n",
      "Epoch 91/100\n",
      "150/150 [==============================] - 0s 24us/step - loss: 0.6197 - acc: 0.7333\n",
      "Epoch 92/100\n",
      "150/150 [==============================] - 0s 27us/step - loss: 0.6177 - acc: 0.7400\n",
      "Epoch 93/100\n",
      "150/150 [==============================] - 0s 28us/step - loss: 0.6155 - acc: 0.7467\n",
      "Epoch 94/100\n",
      "150/150 [==============================] - 0s 37us/step - loss: 0.6134 - acc: 0.7467\n",
      "Epoch 95/100\n",
      "150/150 [==============================] - 0s 34us/step - loss: 0.6115 - acc: 0.7533\n",
      "Epoch 96/100\n",
      "150/150 [==============================] - 0s 36us/step - loss: 0.6096 - acc: 0.7533\n",
      "Epoch 97/100\n",
      "150/150 [==============================] - 0s 32us/step - loss: 0.6075 - acc: 0.7533\n",
      "Epoch 98/100\n",
      "150/150 [==============================] - 0s 27us/step - loss: 0.6059 - acc: 0.7467\n",
      "Epoch 99/100\n",
      "150/150 [==============================] - 0s 23us/step - loss: 0.6038 - acc: 0.7533\n",
      "Epoch 100/100\n",
      "150/150 [==============================] - 0s 22us/step - loss: 0.6018 - acc: 0.7600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f33e5158240>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "model1.fit(X, to_categorical(y), epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = max([len(txt) for txt in path_sequence])\n",
    "unique_tokens = 13 ## START STOP LEFT RIGHT\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['START LEFT END', 'START LEFT END', 'START LEFT END',\n",
       "       'START LEFT END', 'START LEFT END', 'START LEFT END',\n",
       "       'START LEFT END', 'START LEFT END', 'START LEFT END',\n",
       "       'START LEFT END', 'START LEFT END', 'START LEFT END',\n",
       "       'START LEFT END', 'START LEFT END', 'START LEFT END',\n",
       "       'START LEFT END', 'START LEFT END', 'START LEFT END',\n",
       "       'START LEFT END', 'START LEFT END', 'START LEFT END',\n",
       "       'START LEFT END', 'START LEFT END', 'START LEFT END',\n",
       "       'START LEFT END', 'START LEFT END', 'START LEFT END',\n",
       "       'START LEFT END', 'START LEFT END', 'START LEFT END',\n",
       "       'START LEFT END', 'START LEFT END', 'START LEFT END',\n",
       "       'START LEFT END', 'START LEFT END', 'START LEFT END',\n",
       "       'START LEFT END', 'START LEFT END', 'START LEFT END',\n",
       "       'START LEFT END', 'START LEFT END', 'START LEFT END',\n",
       "       'START LEFT END', 'START LEFT END', 'START LEFT END',\n",
       "       'START LEFT END', 'START LEFT END', 'START LEFT END',\n",
       "       'START LEFT END', 'START LEFT END',\n",
       "       'START RIGHT LEFT LEFT LEFT END', 'START RIGHT LEFT LEFT LEFT END',\n",
       "       'START RIGHT LEFT LEFT LEFT END', 'START RIGHT LEFT LEFT LEFT END',\n",
       "       'START RIGHT LEFT LEFT LEFT END', 'START RIGHT LEFT LEFT LEFT END',\n",
       "       'START RIGHT LEFT LEFT LEFT END', 'START RIGHT LEFT LEFT LEFT END',\n",
       "       'START RIGHT LEFT LEFT LEFT END', 'START RIGHT LEFT LEFT LEFT END',\n",
       "       'START RIGHT LEFT LEFT LEFT END', 'START RIGHT LEFT LEFT LEFT END',\n",
       "       'START RIGHT LEFT LEFT LEFT END', 'START RIGHT LEFT LEFT LEFT END',\n",
       "       'START RIGHT LEFT LEFT LEFT END', 'START RIGHT LEFT LEFT LEFT END',\n",
       "       'START RIGHT LEFT LEFT LEFT END', 'START RIGHT LEFT LEFT LEFT END',\n",
       "       'START RIGHT LEFT LEFT LEFT END', 'START RIGHT LEFT LEFT LEFT END',\n",
       "       'START RIGHT RIGHT LEFT RIGHT END',\n",
       "       'START RIGHT LEFT LEFT LEFT END', 'START RIGHT LEFT LEFT LEFT END',\n",
       "       'START RIGHT LEFT LEFT LEFT END', 'START RIGHT LEFT LEFT LEFT END',\n",
       "       'START RIGHT LEFT LEFT LEFT END', 'START RIGHT LEFT LEFT LEFT END',\n",
       "       'START RIGHT LEFT RIGHT RIGHT LEFT END',\n",
       "       'START RIGHT LEFT LEFT LEFT END', 'START RIGHT LEFT LEFT LEFT END',\n",
       "       'START RIGHT LEFT LEFT LEFT END', 'START RIGHT LEFT LEFT LEFT END',\n",
       "       'START RIGHT LEFT LEFT LEFT END',\n",
       "       'START RIGHT LEFT RIGHT RIGHT LEFT END',\n",
       "       'START RIGHT LEFT LEFT LEFT END', 'START RIGHT LEFT LEFT LEFT END',\n",
       "       'START RIGHT LEFT LEFT LEFT END', 'START RIGHT LEFT LEFT LEFT END',\n",
       "       'START RIGHT LEFT LEFT LEFT END', 'START RIGHT LEFT LEFT LEFT END',\n",
       "       'START RIGHT LEFT LEFT LEFT END', 'START RIGHT LEFT LEFT LEFT END',\n",
       "       'START RIGHT LEFT LEFT LEFT END', 'START RIGHT LEFT LEFT LEFT END',\n",
       "       'START RIGHT LEFT LEFT LEFT END', 'START RIGHT LEFT LEFT LEFT END',\n",
       "       'START RIGHT LEFT LEFT LEFT END', 'START RIGHT LEFT LEFT LEFT END',\n",
       "       'START RIGHT LEFT LEFT LEFT END', 'START RIGHT LEFT LEFT LEFT END',\n",
       "       'START RIGHT RIGHT RIGHT END', 'START RIGHT RIGHT RIGHT END',\n",
       "       'START RIGHT RIGHT RIGHT END', 'START RIGHT RIGHT RIGHT END',\n",
       "       'START RIGHT RIGHT RIGHT END', 'START RIGHT RIGHT RIGHT END',\n",
       "       'START RIGHT LEFT LEFT RIGHT END', 'START RIGHT RIGHT RIGHT END',\n",
       "       'START RIGHT RIGHT RIGHT END', 'START RIGHT RIGHT RIGHT END',\n",
       "       'START RIGHT RIGHT RIGHT END', 'START RIGHT RIGHT RIGHT END',\n",
       "       'START RIGHT RIGHT RIGHT END', 'START RIGHT RIGHT RIGHT END',\n",
       "       'START RIGHT RIGHT RIGHT END', 'START RIGHT RIGHT RIGHT END',\n",
       "       'START RIGHT RIGHT RIGHT END', 'START RIGHT RIGHT RIGHT END',\n",
       "       'START RIGHT RIGHT RIGHT END', 'START RIGHT LEFT RIGHT LEFT END',\n",
       "       'START RIGHT RIGHT RIGHT END', 'START RIGHT RIGHT RIGHT END',\n",
       "       'START RIGHT RIGHT RIGHT END', 'START RIGHT RIGHT RIGHT END',\n",
       "       'START RIGHT RIGHT RIGHT END', 'START RIGHT RIGHT RIGHT END',\n",
       "       'START RIGHT RIGHT LEFT LEFT END', 'START RIGHT RIGHT RIGHT END',\n",
       "       'START RIGHT RIGHT RIGHT END',\n",
       "       'START RIGHT LEFT RIGHT RIGHT RIGHT END',\n",
       "       'START RIGHT RIGHT RIGHT END', 'START RIGHT RIGHT RIGHT END',\n",
       "       'START RIGHT RIGHT RIGHT END', 'START RIGHT LEFT RIGHT LEFT END',\n",
       "       'START RIGHT LEFT RIGHT LEFT END', 'START RIGHT RIGHT RIGHT END',\n",
       "       'START RIGHT RIGHT RIGHT END', 'START RIGHT RIGHT RIGHT END',\n",
       "       'START RIGHT RIGHT LEFT LEFT END', 'START RIGHT RIGHT RIGHT END',\n",
       "       'START RIGHT RIGHT RIGHT END', 'START RIGHT RIGHT RIGHT END',\n",
       "       'START RIGHT RIGHT RIGHT END', 'START RIGHT RIGHT RIGHT END',\n",
       "       'START RIGHT RIGHT RIGHT END', 'START RIGHT RIGHT RIGHT END',\n",
       "       'START RIGHT RIGHT RIGHT END', 'START RIGHT RIGHT RIGHT END',\n",
       "       'START RIGHT RIGHT RIGHT END', 'START RIGHT RIGHT RIGHT END'],\n",
       "      dtype='<U38')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_words = ['START', 'LEFT', 'RIGHT', 'END']\n",
    "uniq_chars = set()\n",
    "for txt in uniq_words:\n",
    "    for i, char in enumerate(txt):\n",
    "        if char not in uniq_chars:\n",
    "            uniq_chars.add(char)\n",
    "            \n",
    "in_token_index = dict([(char, i) for i, char in enumerate(uniq_chars)])\n",
    "out_token_index = dict([(char, i+2) for i, char in enumerate(uniq_chars)])\n",
    "\n",
    "out_token_index.update({'\\n':0, '\\t':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_token_index.update({' ': 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_token_index.update({' ': 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Decoder part\n",
    "\n",
    "# # Define an input sequence and process it.\n",
    "# encoder_inputs = Input(shape=(None, num_encoder_tokens)) ## Layer\n",
    "\n",
    "# encoder = LSTM(latent_dim, return_state=True) ## Layer\n",
    "\n",
    "# encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# # We discard `encoder_outputs` and only keep the states.\n",
    "# ## Tensor\n",
    "# encoder_states = [state_h, state_c]\n",
    "\n",
    "# # Set up the decoder, using `encoder_states` as initial state.\n",
    "# decoder_inputs = Input(shape=(None, num_decoder_tokens)) ## Layer\n",
    "\n",
    "inputs = Input(shape=(None, unique_tokens)) ## `explain layer`\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the \n",
    "# return states in the training model, but we will use them in inference.\n",
    "# decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) ## Layer\n",
    "\n",
    "lstm_layer = LSTM(latent_dim, return_sequences=True, return_state=True) ## explain layer\n",
    "\n",
    "## The right latent_dim. What's its impact? -- IMP\n",
    "\n",
    "## Tensor\n",
    "# decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "outputs, _, _ = lstm_layer(inputs) ## explain layer\n",
    "\n",
    "# decoder_dense = Dense(num_decoder_tokens, activation='softmax') ## Layer\n",
    "\n",
    "final_dense = Dense(unique_tokens, activation='softmax') ## explain\n",
    "## Tensor\n",
    "# decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "outputs = final_dense(outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "# target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "# token_index = {'START':1, 'LEFT':2, 'RIGHT':3, 'END':4}\n",
    "\n",
    "# encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "# decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "# decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "\n",
    "input_data = np.zeros((len(path_sequence), max_seq_length, unique_tokens), dtype='float32')\n",
    "target_data = np.zeros((len(path_sequence), max_seq_length, unique_tokens), dtype='float32')\n",
    "\n",
    "for i, input_text in enumerate(path_sequence):\n",
    "#     for t, char in enumerate(input_text):\n",
    "#         encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "#     encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(input_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        input_data[i, t, in_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            target_data[i, t - 1, in_token_index[char]] = 1.\n",
    "    input_data[i, t + 1:, in_token_index[' ']] = 1.\n",
    "    target_data[i, t:, in_token_index[' ']] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        (None, None, 13)          0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                [(None, None, 256), (None 276480    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, None, 13)          3341      \n",
      "=================================================================\n",
      "Total params: 279,821\n",
      "Trainable params: 279,821\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 30 samples\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 1.9410 - acc: 0.5557 - val_loss: 1.8396 - val_acc: 0.4553\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 1.4914 - acc: 0.5680 - val_loss: 1.8293 - val_acc: 0.4289\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 1.5060 - acc: 0.5539 - val_loss: 2.0067 - val_acc: 0.4746\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 1.5920 - acc: 0.5645 - val_loss: 1.8886 - val_acc: 0.4553\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 1.4855 - acc: 0.5625 - val_loss: 1.8189 - val_acc: 0.4289\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 1.4377 - acc: 0.5417 - val_loss: 1.8006 - val_acc: 0.4289\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 1.4152 - acc: 0.5417 - val_loss: 1.8291 - val_acc: 0.4553\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 1.4550 - acc: 0.5557 - val_loss: 2.0434 - val_acc: 0.4561\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 1.5110 - acc: 0.5627 - val_loss: 1.8560 - val_acc: 0.4816\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 1.4303 - acc: 0.5752 - val_loss: 1.8413 - val_acc: 0.4816\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 1.3844 - acc: 0.5763 - val_loss: 1.7687 - val_acc: 0.4553\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 1.3511 - acc: 0.5680 - val_loss: 1.7612 - val_acc: 0.4816\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 1.3289 - acc: 0.5895 - val_loss: 1.8120 - val_acc: 0.4289\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 1.7843 - acc: 0.4366 - val_loss: 2.0756 - val_acc: 0.4544\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 1.4385 - acc: 0.5897 - val_loss: 1.7242 - val_acc: 0.4553\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 1.3548 - acc: 0.5800 - val_loss: 1.7046 - val_acc: 0.4553\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 1.3333 - acc: 0.5803 - val_loss: 1.6998 - val_acc: 0.4553\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 1.3165 - acc: 0.5873 - val_loss: 1.6973 - val_acc: 0.4553\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 1.3200 - acc: 0.5890 - val_loss: 1.7079 - val_acc: 0.4553\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 1.3440 - acc: 0.5831 - val_loss: 1.6858 - val_acc: 0.4553\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 1.2800 - acc: 0.5886 - val_loss: 1.6792 - val_acc: 0.4553\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 1.2692 - acc: 0.6000 - val_loss: 1.6899 - val_acc: 0.4816\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 1.3252 - acc: 0.5965 - val_loss: 1.7054 - val_acc: 0.4816\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 1.3230 - acc: 0.6039 - val_loss: 1.6520 - val_acc: 0.4860\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 1.2456 - acc: 0.6086 - val_loss: 1.6441 - val_acc: 0.4833\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 1.2247 - acc: 0.6048 - val_loss: 1.6265 - val_acc: 0.5333\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 1.2200 - acc: 0.6272 - val_loss: 1.6381 - val_acc: 0.5439\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 1.2335 - acc: 0.6329 - val_loss: 1.6663 - val_acc: 0.4886\n",
      "Epoch 29/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 1.2462 - acc: 0.6086 - val_loss: 1.6369 - val_acc: 0.5167\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 1.2261 - acc: 0.6276 - val_loss: 1.5809 - val_acc: 0.5132\n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 1.1885 - acc: 0.6303 - val_loss: 1.5597 - val_acc: 0.5351\n",
      "Epoch 32/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 1.1766 - acc: 0.6546 - val_loss: 1.5401 - val_acc: 0.5351\n",
      "Epoch 33/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 1.1653 - acc: 0.6430 - val_loss: 1.5355 - val_acc: 0.5386\n",
      "Epoch 34/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 1.1395 - acc: 0.6522 - val_loss: 1.8446 - val_acc: 0.4184\n",
      "Epoch 35/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 1.2487 - acc: 0.6215 - val_loss: 1.5902 - val_acc: 0.5640\n",
      "Epoch 36/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 1.1748 - acc: 0.6570 - val_loss: 1.5336 - val_acc: 0.5404\n",
      "Epoch 37/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 1.1382 - acc: 0.6564 - val_loss: 1.4826 - val_acc: 0.5237\n",
      "Epoch 38/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 1.1059 - acc: 0.6515 - val_loss: 1.4833 - val_acc: 0.5649\n",
      "Epoch 39/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 1.1244 - acc: 0.6509 - val_loss: 1.4750 - val_acc: 0.5649\n",
      "Epoch 40/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 1.1092 - acc: 0.6787 - val_loss: 1.4132 - val_acc: 0.4904\n",
      "Epoch 41/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 1.0383 - acc: 0.6871 - val_loss: 1.4370 - val_acc: 0.5026\n",
      "Epoch 42/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 1.2018 - acc: 0.6112 - val_loss: 1.7904 - val_acc: 0.4675\n",
      "Epoch 43/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 1.2740 - acc: 0.6638 - val_loss: 1.4681 - val_acc: 0.6053\n",
      "Epoch 44/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 1.0774 - acc: 0.7211 - val_loss: 1.4230 - val_acc: 0.5535\n",
      "Epoch 45/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 1.0401 - acc: 0.6890 - val_loss: 1.3850 - val_acc: 0.5807\n",
      "Epoch 46/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 1.0081 - acc: 0.6895 - val_loss: 1.3522 - val_acc: 0.5544\n",
      "Epoch 47/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.9743 - acc: 0.6686 - val_loss: 1.2975 - val_acc: 0.5518\n",
      "Epoch 48/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.9525 - acc: 0.6605 - val_loss: 1.4154 - val_acc: 0.5684\n",
      "Epoch 49/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 1.0135 - acc: 0.6735 - val_loss: 1.2860 - val_acc: 0.5579\n",
      "Epoch 50/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.8941 - acc: 0.7002 - val_loss: 1.1967 - val_acc: 0.6351\n",
      "Epoch 51/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.8583 - acc: 0.7151 - val_loss: 1.2294 - val_acc: 0.6737\n",
      "Epoch 52/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.9619 - acc: 0.7195 - val_loss: 1.1382 - val_acc: 0.6728\n",
      "Epoch 53/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.8052 - acc: 0.7489 - val_loss: 1.0291 - val_acc: 0.6544\n",
      "Epoch 54/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.7195 - acc: 0.7502 - val_loss: 0.9815 - val_acc: 0.6921\n",
      "Epoch 55/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.7263 - acc: 0.7919 - val_loss: 0.9360 - val_acc: 0.7789\n",
      "Epoch 56/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.7751 - acc: 0.7846 - val_loss: 0.9360 - val_acc: 0.7246\n",
      "Epoch 57/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.9368 - acc: 0.7230 - val_loss: 1.2873 - val_acc: 0.4904\n",
      "Epoch 58/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.9019 - acc: 0.7461 - val_loss: 1.0877 - val_acc: 0.7105\n",
      "Epoch 59/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.7074 - acc: 0.8127 - val_loss: 0.9595 - val_acc: 0.7044\n",
      "Epoch 60/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.6248 - acc: 0.8272 - val_loss: 0.8614 - val_acc: 0.7605\n",
      "Epoch 61/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.5779 - acc: 0.8546 - val_loss: 0.8005 - val_acc: 0.7614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.5381 - acc: 0.8621 - val_loss: 0.8115 - val_acc: 0.7605\n",
      "Epoch 63/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.5304 - acc: 0.8441 - val_loss: 1.0441 - val_acc: 0.6070\n",
      "Epoch 64/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.6387 - acc: 0.8000 - val_loss: 0.7733 - val_acc: 0.7474\n",
      "Epoch 65/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.4894 - acc: 0.8787 - val_loss: 0.6511 - val_acc: 0.7939\n",
      "Epoch 66/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.4449 - acc: 0.8818 - val_loss: 0.5669 - val_acc: 0.8412\n",
      "Epoch 67/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.4144 - acc: 0.8888 - val_loss: 0.5244 - val_acc: 0.8623\n",
      "Epoch 68/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.4013 - acc: 0.8792 - val_loss: 0.6482 - val_acc: 0.8535\n",
      "Epoch 69/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.5606 - acc: 0.8344 - val_loss: 0.7713 - val_acc: 0.8456\n",
      "Epoch 70/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.4773 - acc: 0.9200 - val_loss: 0.5165 - val_acc: 0.9167\n",
      "Epoch 71/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.3742 - acc: 0.9491 - val_loss: 0.4628 - val_acc: 0.9167\n",
      "Epoch 72/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.3527 - acc: 0.9432 - val_loss: 0.4604 - val_acc: 0.8947\n",
      "Epoch 73/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.3508 - acc: 0.9533 - val_loss: 0.5102 - val_acc: 0.8711\n",
      "Epoch 74/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.3727 - acc: 0.9432 - val_loss: 0.7777 - val_acc: 0.7842\n",
      "Epoch 75/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.8127 - acc: 0.7596 - val_loss: 0.7828 - val_acc: 0.8386\n",
      "Epoch 76/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.4846 - acc: 0.9303 - val_loss: 0.5445 - val_acc: 0.9000\n",
      "Epoch 77/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.3581 - acc: 0.9596 - val_loss: 0.4700 - val_acc: 0.9219\n",
      "Epoch 78/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.3217 - acc: 0.9686 - val_loss: 0.4170 - val_acc: 0.9439\n",
      "Epoch 79/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2969 - acc: 0.9772 - val_loss: 0.3849 - val_acc: 0.9439\n",
      "Epoch 80/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2799 - acc: 0.9765 - val_loss: 0.3772 - val_acc: 0.9447\n",
      "Epoch 81/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2710 - acc: 0.9732 - val_loss: 0.3616 - val_acc: 0.9447\n",
      "Epoch 82/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2666 - acc: 0.9752 - val_loss: 0.3413 - val_acc: 0.9632\n",
      "Epoch 83/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.3317 - acc: 0.9434 - val_loss: 0.5555 - val_acc: 0.8912\n",
      "Epoch 84/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.4805 - acc: 0.9007 - val_loss: 0.5000 - val_acc: 0.8781\n",
      "Epoch 85/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.4074 - acc: 0.9272 - val_loss: 0.4999 - val_acc: 0.9158\n",
      "Epoch 86/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.3108 - acc: 0.9614 - val_loss: 0.3711 - val_acc: 0.9430\n",
      "Epoch 87/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2611 - acc: 0.9715 - val_loss: 0.3279 - val_acc: 0.9430\n",
      "Epoch 88/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2407 - acc: 0.9774 - val_loss: 0.3017 - val_acc: 0.9456\n",
      "Epoch 89/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2262 - acc: 0.9776 - val_loss: 0.2905 - val_acc: 0.9456\n",
      "Epoch 90/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2158 - acc: 0.9776 - val_loss: 0.2695 - val_acc: 0.9658\n",
      "Epoch 91/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2067 - acc: 0.9818 - val_loss: 0.2723 - val_acc: 0.9658\n",
      "Epoch 92/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1981 - acc: 0.9818 - val_loss: 0.2541 - val_acc: 0.9658\n",
      "Epoch 93/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1901 - acc: 0.9818 - val_loss: 0.2569 - val_acc: 0.9675\n",
      "Epoch 94/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1826 - acc: 0.9822 - val_loss: 0.2431 - val_acc: 0.9658\n",
      "Epoch 95/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1764 - acc: 0.9820 - val_loss: 0.2489 - val_acc: 0.9667\n",
      "Epoch 96/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2105 - acc: 0.9623 - val_loss: 0.4796 - val_acc: 0.9096\n",
      "Epoch 97/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.3785 - acc: 0.8991 - val_loss: 0.6472 - val_acc: 0.8246\n",
      "Epoch 98/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2836 - acc: 0.9577 - val_loss: 0.2598 - val_acc: 0.9465\n",
      "Epoch 99/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1812 - acc: 0.9803 - val_loss: 0.2357 - val_acc: 0.9675\n",
      "Epoch 100/100\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1653 - acc: 0.9822 - val_loss: 0.2192 - val_acc: 0.9649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3330118ba8>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(input_data, target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=100,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Many-to-many and one-to-many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample approach for one-to-many\n",
    "## Does RepeatVector merely alter the dimensions?\n",
    "\n",
    "from keras.layers import RepeatVector\n",
    "\n",
    "model.add(RepeatVector(number_of_times, input_shape=input_shape))\n",
    "model.add(LSTM(output_size, return_sequences=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample approach for many-to-many\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(1, input_shape=(timesteps, data_dim), return_sequences=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-f4045649b240>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# # Embedding layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m model.add(\n\u001b[1;32m      5\u001b[0m     Embedding(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# # Embedding layer\n",
    "model.add(\n",
    "    Embedding(\n",
    "        input_dim=5,\n",
    "        output_dim=100,\n",
    "        weights=None,\n",
    "        trainable=True))\n",
    "\n",
    "# Recurrent layer\n",
    "model.add(\n",
    "    LSTM(\n",
    "        64, return_sequences=False, dropout=0.1,\n",
    "        recurrent_dropout=0.1))\n",
    "\n",
    "# # Fully connected layer\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# # Dropout for regularization\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial notes(please ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "estimator = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)\n",
    "estimator.fit(X_train, y_train)\n",
    "\n",
    "# The decision estimator has an attribute called tree_  which stores the entire\n",
    "# tree structure and allows access to low level attributes. The binary tree\n",
    "# tree_ is represented as a number of parallel arrays. The i-th element of each\n",
    "# array holds information about the node `i`. Node 0 is the tree's root. NOTE:\n",
    "# Some of the arrays only apply to either leaves or split nodes, resp. In this\n",
    "# case the values of nodes of the other type are arbitrary!\n",
    "#\n",
    "# Among those arrays, we have:\n",
    "#   - left_child, id of the left child of the node\n",
    "#   - right_child, id of the right child of the node\n",
    "#   - feature, feature used for splitting the node\n",
    "#   - threshold, threshold value at the node\n",
    "#\n",
    "\n",
    "# Using those arrays, we can parse the tree structure:\n",
    "\n",
    "n_nodes = estimator.tree_.node_count\n",
    "children_left = estimator.tree_.children_left\n",
    "children_right = estimator.tree_.children_right\n",
    "feature = estimator.tree_.feature\n",
    "threshold = estimator.tree_.threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The binary tree structure has 5 nodes and has the following tree structure:\n",
      "node=0 test node: go to node 1 if X[:, 3] <= 0.800000011920929 else to node 2.\n",
      "\tnode=1 leaf node.\n",
      "\tnode=2 test node: go to node 3 if X[:, 2] <= 4.950000047683716 else to node 4.\n",
      "\t\tnode=3 leaf node.\n",
      "\t\tnode=4 leaf node.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The tree structure can be traversed to compute various properties such\n",
    "# as the depth of each node and whether or not it is a leaf.\n",
    "node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "stack = [(0, -1)]  # seed is the root node id and its parent depth\n",
    "while len(stack) > 0:\n",
    "    node_id, parent_depth = stack.pop()\n",
    "    node_depth[node_id] = parent_depth + 1\n",
    "\n",
    "    # If we have a test node\n",
    "    if (children_left[node_id] != children_right[node_id]):\n",
    "        stack.append((children_left[node_id], parent_depth + 1))\n",
    "        stack.append((children_right[node_id], parent_depth + 1))\n",
    "    else:\n",
    "        is_leaves[node_id] = True\n",
    "\n",
    "print(\"The binary tree structure has %s nodes and has \"\n",
    "      \"the following tree structure:\"\n",
    "      % n_nodes)\n",
    "for i in range(n_nodes):\n",
    "    if is_leaves[i]:\n",
    "        print(\"%snode=%s leaf node.\" % (node_depth[i] * \"\\t\", i))\n",
    "    else:\n",
    "        print(\"%snode=%s test node: go to node %s if X[:, %s] <= %s else to \"\n",
    "              \"node %s.\"\n",
    "              % (node_depth[i] * \"\\t\",\n",
    "                 i,\n",
    "                 children_left[i],\n",
    "                 feature[i],\n",
    "                 threshold[i],\n",
    "                 children_right[i],\n",
    "                 ))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rules used to predict sample 0: \n",
      "decision id node 0 : (X_test[0, 3] (= 2.4) > 0.800000011920929)\n",
      "decision id node 2 : (X_test[0, 2] (= 5.1) > 4.950000047683716)\n",
      "\n",
      "The following samples [0, 1] share the node [0 2] in the tree\n",
      "It is 40.0 % of all nodes.\n"
     ]
    }
   ],
   "source": [
    "# First let's retrieve the decision path of each sample. The decision_path\n",
    "# method allows to retrieve the node indicator functions. A non zero element of\n",
    "# indicator matrix at the position (i, j) indicates that the sample i goes\n",
    "# through the node j.\n",
    "\n",
    "node_indicator = estimator.decision_path(X_test)\n",
    "\n",
    "# Similarly, we can also have the leaves ids reached by each sample.\n",
    "\n",
    "leave_id = estimator.apply(X_test)\n",
    "\n",
    "# Now, it's possible to get the tests that were used to predict a sample or\n",
    "# a group of samples. First, let's make it for the sample.\n",
    "\n",
    "sample_id = 0\n",
    "node_index = node_indicator.indices[node_indicator.indptr[sample_id]:\n",
    "                                    node_indicator.indptr[sample_id + 1]]\n",
    "\n",
    "print('Rules used to predict sample %s: ' % sample_id)\n",
    "for node_id in node_index:\n",
    "    if leave_id[sample_id] == node_id:\n",
    "        continue\n",
    "\n",
    "    if (X_test[sample_id, feature[node_id]] <= threshold[node_id]):\n",
    "        threshold_sign = \"<=\"\n",
    "    else:\n",
    "        threshold_sign = \">\"\n",
    "\n",
    "    print(\"decision id node %s : (X_test[%s, %s] (= %s) %s %s)\"\n",
    "          % (node_id,\n",
    "             sample_id,\n",
    "             feature[node_id],\n",
    "             X_test[sample_id, feature[node_id]],\n",
    "             threshold_sign,\n",
    "             threshold[node_id]))\n",
    "\n",
    "# For a group of samples, we have the following common node.\n",
    "sample_ids = [0, 1]\n",
    "common_nodes = (node_indicator.toarray()[sample_ids].sum(axis=0) ==\n",
    "                len(sample_ids))\n",
    "\n",
    "common_node_id = np.arange(n_nodes)[common_nodes]\n",
    "\n",
    "print(\"\\nThe following samples %s share the node %s in the tree\"\n",
    "      % (sample_ids, common_node_id))\n",
    "print(\"It is %s %% of all nodes.\" % (100 * len(common_node_id) / n_nodes,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(shape=n_nodes, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(shape=n_nodes, dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 2, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False,  True,  True])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1,  3, -1, -1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "children_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, -1,  4, -1, -1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "children_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydotplus\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/bf/62567830b700d9f6930e9ab6831d6ba256f7b0b730acb37278b0ccdffacf/pydotplus-2.0.2.tar.gz (278kB)\n",
      "\u001b[K     |████████████████████████████████| 286kB 746kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.1 in /home/shakkeel/anaconda3/envs/test_imly/lib/python3.6/site-packages (from pydotplus) (2.4.0)\n",
      "Building wheels for collected packages: pydotplus\n",
      "  Building wheel for pydotplus (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/shakkeel/.cache/pip/wheels/35/7b/ab/66fb7b2ac1f6df87475b09dc48e707b6e0de80a6d8444e3628\n",
      "Successfully built pydotplus\n",
      "Installing collected packages: pydotplus\n",
      "Successfully installed pydotplus-2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image  \n",
    "from sklearn import tree\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DOT data\n",
    "dot_data = tree.export_graphviz(estimator, out_file=None, \n",
    "                                feature_names=iris.feature_names,  \n",
    "                                class_names=iris.target_names)\n",
    "\n",
    "# Draw graph\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "\n",
    "# Show graph\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<38x5 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 101 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 1, 4, 1, 4, 1, 3, 3, 3, 4, 3, 3, 3, 3, 1, 3, 3, 1, 1, 3, 3,\n",
       "       1, 1, 3, 1, 1, 3, 3, 1, 4, 3, 1, 3, 4, 3, 1, 4])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leave_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 3], dtype=int32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Collecting path - loop through this\n",
    "node_indicator.indices[node_indicator.indptr[20]:node_indicator.indptr[21]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting path to strings\n",
    "left_nodes = children_left[children_left>0]\n",
    "right_nodes = children_right[children_right>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 3], dtype=int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_indicator.indices[node_indicator.indptr[31]:\n",
    "                                    node_indicator.indptr[32]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   3,   6,   8,  11,  13,  16,  18,  21,  24,  27,  30,  33,\n",
       "        36,  39,  42,  44,  47,  50,  52,  54,  57,  60,  62,  64,  67,\n",
       "        69,  71,  74,  77,  79,  82,  85,  87,  90,  93,  96,  98, 101],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_indicator.indptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, -2,  2, -2, -2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.80000001, -2.        ,  4.95000005, -2.        , -2.        ])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 's'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = a.join('l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['start'], dtype='<U32')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.array([])\n",
    "\n",
    "np.append(test, 'start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['end'], dtype='<U32')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(test, 'end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample RNN code from this [tutorial](https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tok = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
