{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOPE\n",
    "---\n",
    "\n",
    "With `dope`, our goal is to make all existing standard machine learning frameworks(say sklearn, suprislib, pytorch, tensorflow etc) interoperable. That is, one can devlop and train a model, say, using Linear Regression in sklearn, and score it using a TensorFlow server.\n",
    "\n",
    "In this tutorial, we walk through an example demonstrating one such scenario.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "---\n",
    "Setting the context(Terminologies used) -  \n",
    "1) Primal model - Primal model refers to the base model provided by the user. For example, the primal model in the scenario demonstrated below would be the `LogisticRegression()` class instance from sklearn.  \n",
    "2) dope - The dope function converts your primal model to it's dnn equivalent. Also, dope ensures that the functional and behavioural aspects of your primal model is retained when it's \"dope\"d.\n",
    "\n",
    "\n",
    "*Note - The usage of `dope` is pretty straightforward as long as the user has a decent understanding of basic Sklearn and Keras functionalities.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading and preprocessing dataset\n",
    "---\n",
    "In this example we will use the iris dataset. The primal model used here is sklearn's Logistic Regression class. The `dope` function converts sklearn's Logistic Regression model to it's Neural Network equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "\n",
    "# Split the data in to test and train batches\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Instantiate the primal model\n",
    "---\n",
    "Instantiate the model you wish to convert in to a Neural network. Here, we use sklearn's logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: \"Dope\" your primal model!\n",
    "---\n",
    "The `dope` function lets you convert your primal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "2019-11-07 16:21:56,983\tINFO node.py:423 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-11-07_16-21-56_789/logs.\n",
      "2019-11-07 16:21:57,092\tINFO services.py:363 -- Waiting for redis server at 127.0.0.1:18579 to respond...\n",
      "2019-11-07 16:21:57,202\tINFO services.py:363 -- Waiting for redis server at 127.0.0.1:30532 to respond...\n",
      "2019-11-07 16:21:57,204\tINFO services.py:760 -- Starting Redis shard with 20.0 GB max memory.\n",
      "2019-11-07 16:21:57,224\tINFO services.py:1384 -- Starting the Plasma object store with 1.0 GB memory using /dev/shm.\n",
      "Transpiling your model to it's Deep Neural Network equivalent...\n"
     ]
    }
   ],
   "source": [
    "from mlsquare import dope\n",
    "m = dope(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "__Note - The warning message you see about redis server is a part of the optimization process `dope` does. The details about this will be covered in the upcoming tutorials(yet to be published). So fret not! These warning messages can be safely ignored.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Voila! You have successfully Doped your model\n",
    "---\n",
    "Once you have successfully run the `dope` function by passing your primal model, the returned model(the variable `m` here) would behave like any other sklearn models. The only difference being that the model is not a standard sklearn model but a dnn equivalent of the model provided by you.\n",
    "\n",
    "The below mentioned methods demonstrate the resemblance of an \"dope'd\" model with sklearn models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakkeel/anaconda3/envs/test_imly/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/shakkeel/anaconda3/envs/test_imly/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "2019-11-07 16:22:01,058\tINFO tune.py:60 -- Tip: to resume incomplete experiments, pass resume='prompt' or resume=True to run()\n",
      "2019-11-07 16:22:01,058\tINFO tune.py:211 -- Starting a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.2 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-07 16:22:01,333\tWARNING util.py:62 -- The `start_trial` operation took 0.26877903938293457 seconds to complete, which may be a performance bottleneck.\n",
      "2019-11-07 16:22:02,343\tWARNING util.py:62 -- The `experiment_checkpoint` operation took 1.009018898010254 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - train_model_0:\tRUNNING\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=874)\u001b[0m Using TensorFlow backend.\n",
      "\u001b[2m\u001b[36m(pid=874)\u001b[0m 2019-11-07 16:22:05,605\tERROR worker.py:1412 -- Calling ray.init() again after it has already been called.\n",
      "\u001b[2m\u001b[36m(pid=874)\u001b[0m WARNING:tensorflow:From /home/shakkeel/anaconda3/envs/test_imly/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=874)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=874)\u001b[0m Colocations handled automatically by placer.\n",
      "\u001b[2m\u001b[36m(pid=874)\u001b[0m WARNING:tensorflow:From /home/shakkeel/anaconda3/envs/test_imly/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=874)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=874)\u001b[0m Use tf.cast instead.\n",
      "\u001b[2m\u001b[36m(pid=874)\u001b[0m 2019-11-07 16:22:05.961197: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=874)\u001b[0m 2019-11-07 16:22:05.964389: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz\n",
      "\u001b[2m\u001b[36m(pid=874)\u001b[0m 2019-11-07 16:22:05.964852: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5650e5ae1450 executing computations on platform Host. Devices:\n",
      "\u001b[2m\u001b[36m(pid=874)\u001b[0m 2019-11-07 16:22:05.964870: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-07 16:22:06,542\tWARNING util.py:62 -- The `experiment_checkpoint` operation took 0.10720324516296387 seconds to complete, which may be a performance bottleneck.\n",
      "2019-11-07 16:22:06,545\tINFO ray_trial_executor.py:178 -- Destroying actor for trial train_model_0. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_model_0:\n",
      "  checkpoint: 'weights_tune_{''layer_1.units'': 1, ''layer_1.l1'': 0, ''layer_1.l2'':\n",
      "    0, ''layer_1.activation'': ''sigmoid'', ''optimizer'': ''adam'', ''loss'': ''binary_crossentropy''}.h5'\n",
      "  date: 2019-11-07_16-22-06\n",
      "  done: false\n",
      "  experiment_id: 64b24926e4c24e0487a82cb412ada368\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.21666667262713116\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 874\n",
      "  time_since_restore: 0.6463437080383301\n",
      "  time_this_iter_s: 0.6463437080383301\n",
      "  time_total_s: 0.6463437080383301\n",
      "  timestamp: 1573123926\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=874)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=874)\u001b[0m 32/60 [===============>..............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\u001b[2m\u001b[36m(pid=874)\u001b[0m 60/60 [==============================] - 0s 271us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-07 16:22:07,107\tWARNING util.py:62 -- The `experiment_checkpoint` operation took 0.5611865520477295 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "Number of trials: 1 ({'TERMINATED': 1})\n",
      "TERMINATED trials:\n",
      " - train_model_0:\tTERMINATED, [4 CPUs, 0 GPUs], [pid=874], 0 s, 1 iter, 0.217 acc\n",
      "\n",
      "Creating model...\n",
      "WARNING:tensorflow:From /home/shakkeel/anaconda3/envs/test_imly/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Loading from /home/shakkeel/ray_results/experiment_name/train_model_0_2019-11-07_16-22-01b80k77v4/weights_tune_{'layer_1.units': 1, 'layer_1.l1': 0, 'layer_1.l2': 0, 'layer_1.activation': 'sigmoid', 'optimizer': 'adam', 'loss': 'binary_crossentropy'}.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7f479f845b00>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Fit your model ##\n",
    "m.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 149us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10076400770081415, 0.3666666699780358]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Score your model ##\n",
    "m.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The maximum opset needed by this model is only 7.\n"
     ]
    }
   ],
   "source": [
    "## Save your model ##\n",
    "m.save('demo_path')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note - The save method expects a single argument - filename. You will be able to find the saved model in the directory you're running your script from. The model by default is saved in three formats - h5, onnx and a serialized pickle file.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
