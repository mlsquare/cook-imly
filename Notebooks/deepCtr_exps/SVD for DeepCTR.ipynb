{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'enable_eager_execution'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ddf3115bdcc7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_eager_execution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'enable_eager_execution'"
     ]
    }
   ],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-rc1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__#current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__#old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "#from keras.models import Model\n",
    "#from keras.layers import Input, Dense\n",
    "#from keras import metrics\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of dataset: (100000, 4)\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.expanduser('C:\\\\Users\\\\might\\\\Desktop\\\\jupyter notebooks\\\\u.data')\n",
    "df = pd.read_csv(data_path, sep='\\t', header=None)#used for keras NN_SVD\n",
    "\n",
    "df2= pd.read_csv(data_path, sep='\\t',names= 'user_id\tmovie_id\trating\ttimestamp'.split('\t'))#, header=None)#used for DeepCTR\n",
    "\n",
    "\n",
    "#df[2]= df[2].apply(lambda x: int(x>3))#since in df, no. vals holding 1 ratings:6110, 2 ratings:11370, 3 ratings:27145, 4 ratings:34174, 5 ratings: 21201 \n",
    "\n",
    "shuffled_df = df.loc[np.random.randint(0, 100000, size=df.shape[0])]\n",
    "\n",
    "#sm_train_df = shuffled_df[:80000]#small dataframe, trimming data points to random 2000 datapoints\n",
    "#test_df= shuffled_df[-20000:]\n",
    "\n",
    "#print('new trimmed dataset:', sm_train_df.shape, '\\nnew test dataset:', test_df.shape)\n",
    "print('shape of dataset:', shuffled_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train_user_in: (100000, 944) \n",
      "shape of x_train_movie_in: (100000, 1683) \n",
      "shape of x_train_user_in: (100000,)\n"
     ]
    }
   ],
   "source": [
    "##code from previous keras implementation\n",
    "x_train_user_in = to_categorical(shuffled_df[0])#contains the one-hot encoded user_id data, shaped (batch_size, max(sm_df[0]))\n",
    "x_train_movie_in = to_categorical(shuffled_df[1])#contains the one-hot encoded movie_id data, shaped (batch_size, max(sm_df[1]))\n",
    "\n",
    "y_ratings= shuffled_df[2]\n",
    "print('shape of x_train_user_in:', x_train_user_in.shape,'\\nshape of x_train_movie_in:', x_train_movie_in.shape,\n",
    "      '\\nshape of x_train_user_in:',y_ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_user_in[0]#derived from df[0] --correspoding to first user in uids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([455], dtype=int64),)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(x_train_user_in[0]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "##drawing code from deepCTR\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from deepctr.models import DeepFM\n",
    "from deepctr.inputs import SparseFeat, DenseFeat, get_fixlen_feature_names, VarLenSparseFeat\n",
    "\n",
    "df2= pd.read_csv(data_path, sep='\\t',names= 'user_id\tmovie_id\trating\ttimestamp'.split('\t'))#, header=None)#used for DeepCTR\n",
    "\n",
    "sparse_features = [\"movie_id\", \"user_id\"]\n",
    "y= ['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating  timestamp\n",
       "0      196       242       3  881250949\n",
       "1      186       302       3  891717742\n",
       "2       22       377       1  878887116\n",
       "3      244        51       2  880606923"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(np.sort(np.unique(df2['user_id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    195\n",
       "1    185\n",
       "2     21\n",
       "3    243\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for feat in sparse_features:\n",
    "        lbe = LabelEncoder()\n",
    "        df2[feat] = lbe.fit_transform(df2[feat])\n",
    "    ##unique features for each sparse field\n",
    "    \n",
    "df2[sparse_features[1]].head(4)#derived from df2['user_id'] --correspoding to first user in uids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixlen_feature_columns = [SparseFeat(feat, df2[feat].nunique()) for feat in sparse_features]\n",
    "\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "\n",
    "dnn_feature_columns = fixlen_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse feats & Dense feats:\n",
      " [SparseFeat(name='movie_id', dimension=1682, use_hash=False, dtype='int32', embedding_name='movie_id', embedding=True), SparseFeat(name='user_id', dimension=943, use_hash=False, dtype='int32', embedding_name='user_id', embedding=True)] \n",
      " [SparseFeat(name='movie_id', dimension=1682, use_hash=False, dtype='int32', embedding_name='movie_id', embedding=True), SparseFeat(name='user_id', dimension=943, use_hash=False, dtype='int32', embedding_name='user_id', embedding=True)]\n"
     ]
    }
   ],
   "source": [
    "print('Sparse feats & Dense feats:\\n',linear_feature_columns,'\\n', dnn_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from collections import namedtuple#testing inheritence with namedtuple class & super() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#person = namedtuple('Person','name age gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('name', '_1', 'gender', 'age', '_4')\n"
     ]
    }
   ],
   "source": [
    "#with_class = namedtuple('Person', 'name class gender age gender', rename=True)\n",
    "#print(with_class._fields)\n",
    "\n",
    "#two_ages = collections.namedtuple('Person', 'name age gender age', rename=True)\n",
    "#print two_ages._fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class MyParentClass():\n",
    " #   def __init__(self):\n",
    "  #      self.var = 7\n",
    "  #  def printout(self):\n",
    "   #     print('output is :', self.var)\n",
    "\n",
    "#class SubClass(MyParentClass):\n",
    " #   def __init__(self):\n",
    "  #      super().__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a= SubClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output is : 7\n"
     ]
    }
   ],
   "source": [
    "#a.printout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseFeat(name='movie_id', dimension=1682, use_hash=False, dtype='int32', embedding_name='movie_id', embedding=True),\n",
       " SparseFeat(name='user_id', dimension=943, use_hash=False, dtype='int32', embedding_name='user_id', embedding=True),\n",
       " SparseFeat(name='movie_id', dimension=1682, use_hash=False, dtype='int32', embedding_name='movie_id', embedding=True),\n",
       " SparseFeat(name='user_id', dimension=943, use_hash=False, dtype='int32', embedding_name='user_id', embedding=True)]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#features = build_input_features(linear_feature_columns + dnn_feature_columns)\n",
    "feature_columns= linear_feature_columns + dnn_feature_columns\n",
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Following is line by line execution of deepctr.models.DeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DeepFM(linear_feature_columns, dnn_feature_columns, embedding_size=8, use_fm=True, dnn_hidden_units=(128, 128),\n",
    "#l2_reg_linear=0.00001, l2_reg_embedding=0.00001, l2_reg_dnn=0, init_std=0.0001, seed=1024,dnn_dropout=0, dnn_activation='relu',\n",
    "#dnn_use_bn=False, task='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features[feature_columns[0].name] = Input(shape=(1,), name='' + feature_columns[0].name, dtype=feature_columns[0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'movie_id_1:0' shape=(None, 1) dtype=int32>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features[feature_columns[0].name]#Inpute layer corresponding to 1st SparseFeat object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from tensorflow.python.keras.layers import  Embedding, Input, Flatten\n",
    "#from tensorflow.python.keras.regularizers import l2\n",
    "\n",
    "#from .layers.sequence import SequencePoolingLayer\n",
    "\n",
    "\n",
    "def build_input_features(feature_columns, include_varlen=True, mask_zero=True, prefix='',include_fixlen=True):\n",
    "    input_features = OrderedDict()\n",
    "    if include_fixlen:\n",
    "        for fc in feature_columns:\n",
    "            if isinstance(fc,SparseFeat):\n",
    "                input_features[fc.name] = Input(\n",
    "                    shape=(1,), name=prefix+fc.name, dtype=fc.dtype)\n",
    "            elif isinstance(fc,DenseFeat):\n",
    "                input_features[fc.name] = Input(\n",
    "                    shape=(fc.dimension,), name=prefix + fc.name, dtype=fc.dtype)\n",
    "    if include_varlen:\n",
    "        for fc in feature_columns:\n",
    "            if isinstance(fc,VarLenSparseFeat):\n",
    "                input_features[fc.name] = Input(shape=(fc.maxlen,), name=prefix + 'seq_' + fc.name,\n",
    "                                                      dtype=fc.dtype)\n",
    "        if not mask_zero:\n",
    "            for fc in feature_columns:\n",
    "                input_features[fc.name+\"_seq_length\"] = Input(shape=(\n",
    "                    1,), name=prefix + 'seq_length_' + fc.name)\n",
    "                input_features[fc.name+\"_seq_max_length\"] = fc.maxlen\n",
    "\n",
    "\n",
    "    return input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('movie_id',\n",
       "              <tf.Tensor 'movie_id_11:0' shape=(None, 1) dtype=int32>),\n",
       "             ('user_id',\n",
       "              <tf.Tensor 'user_id_9:0' shape=(None, 1) dtype=int32>)])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features= build_input_features(feature_columns)\n",
    "features #Input feature layers obatined from SparseFeat parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'movie_id_5:0' shape=(None, 1) dtype=int32>,\n",
       " <tf.Tensor 'user_id_3:0' shape=(None, 1) dtype=int32>]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_list = list(features.values())\n",
    "inputs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseFeat(name='movie_id', dimension=1682, use_hash=False, dtype='int32', embedding_name='movie_id', embedding=True),\n",
       " SparseFeat(name='user_id', dimension=943, use_hash=False, dtype='int32', embedding_name='user_id', embedding=True)]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_feature_columns = list(filter(lambda x:isinstance(x,SparseFeat), dnn_feature_columns)) if dnn_feature_columns else []\n",
    "\n",
    "sparse_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.initializers import RandomNormal\n",
    "\n",
    "def create_embedding_dict(sparse_feature_columns, varlen_sparse_feature_columns, embedding_size, init_std, seed, l2_reg,\n",
    "                          prefix='sparse_', seq_mask_zero=True):\n",
    "    if embedding_size == 'auto':\n",
    "        print(\"Notice:Do not use auto embedding in models other than DCN\")\n",
    "        sparse_embedding = {feat.embedding_name: Embedding(feat.dimension, 6 * int(pow(feat.dimension, 0.25)),\n",
    "                                                 embeddings_initializer=RandomNormal(\n",
    "                                                     mean=0.0, stddev=init_std, seed=seed),\n",
    "                                                 embeddings_regularizer=l2(l2_reg),\n",
    "                                                 name=prefix + '_emb_' + feat.name) for feat in\n",
    "                            sparse_feature_columns}\n",
    "    else:\n",
    "\n",
    "        sparse_embedding = {feat.embedding_name: Embedding(feat.dimension, embedding_size,\n",
    "                                                 embeddings_initializer=RandomNormal(\n",
    "                                                     mean=0.0, stddev=init_std, seed=seed),\n",
    "                                                 embeddings_regularizer=l2(\n",
    "                                                     l2_reg),\n",
    "                                                 name=prefix + '_emb_'  + feat.name) for feat in\n",
    "                            sparse_feature_columns}\n",
    "\n",
    "    if varlen_sparse_feature_columns and len(varlen_sparse_feature_columns) > 0:\n",
    "        for feat in varlen_sparse_feature_columns:\n",
    "            # if feat.name not in sparse_embedding:\n",
    "            if embedding_size == \"auto\":\n",
    "                sparse_embedding[feat.embedding_name] = Embedding(feat.dimension, 6 * int(pow(feat.dimension, 0.25)),\n",
    "                                                        embeddings_initializer=RandomNormal(\n",
    "                                                            mean=0.0, stddev=init_std, seed=seed),\n",
    "                                                        embeddings_regularizer=l2(\n",
    "                                                            l2_reg),\n",
    "                                                        name=prefix + '_seq_emb_' + feat.name,\n",
    "                                                        mask_zero=seq_mask_zero)\n",
    "\n",
    "            else:\n",
    "                sparse_embedding[feat.embedding_name] = Embedding(feat.dimension, embedding_size,\n",
    "                                                        embeddings_initializer=RandomNormal(\n",
    "                                                            mean=0.0, stddev=init_std, seed=seed),\n",
    "                                                        embeddings_regularizer=l2(\n",
    "                                                            l2_reg),\n",
    "                                                        name=prefix + '_seq_emb_' + feat.name,\n",
    "                                                        mask_zero=seq_mask_zero)\n",
    "\n",
    "\n",
    "    return sparse_embedding\n",
    "\n",
    "def create_embedding_matrix(feature_columns,l2_reg,init_std,seed,embedding_size, prefix=\"\",seq_mask_zero=True):\n",
    "    sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, SparseFeat) and x.embedding, feature_columns)) if feature_columns else []\n",
    "    varlen_sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, VarLenSparseFeat) and x.embedding, feature_columns)) if feature_columns else []\n",
    "    sparse_emb_dict = create_embedding_dict(sparse_feature_columns, varlen_sparse_feature_columns, embedding_size, init_std, seed,\n",
    "                                                 l2_reg, prefix=prefix + 'sparse',seq_mask_zero=seq_mask_zero)\n",
    "    return sparse_emb_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_id': <tensorflow.python.keras.layers.embeddings.Embedding at 0x254edbea208>,\n",
       " 'user_id': <tensorflow.python.keras.layers.embeddings.Embedding at 0x254eba407f0>}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#line 3 in input_from_feature_columns( ) function\n",
    "embedding_dict = create_embedding_matrix(dnn_feature_columns, 0.00001, 0.0001, 1024, 8, prefix='',seq_mask_zero=True)\n",
    "embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#line 4 in input_from_feature_columns( ) function\n",
    "def embedding_lookup(sparse_embedding_dict,sparse_input_dict,sparse_feature_columns,return_feat_list=(), mask_feat_list=()):\n",
    "    embedding_vec_list = []\n",
    "    for fc in sparse_feature_columns:\n",
    "        feature_name = fc.name\n",
    "        embedding_name = fc.embedding_name\n",
    "        if len(return_feat_list) == 0  or feature_name in return_feat_list and fc.embedding:\n",
    "            #if fc.use_hash:\n",
    "            #    lookup_idx = Hash(fc.dimension,mask_zero=(feature_name in mask_feat_list))(sparse_input_dict[feature_name])\n",
    "            lookup_idx = sparse_input_dict[feature_name]\n",
    "\n",
    "            embedding_vec_list.append(sparse_embedding_dict[embedding_name](lookup_idx))\n",
    "\n",
    "    return embedding_vec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse_feature_columns:\n",
      " [SparseFeat(name='movie_id', dimension=1682, use_hash=False, dtype='int32', embedding_name='movie_id', embedding=True), SparseFeat(name='user_id', dimension=943, use_hash=False, dtype='int32', embedding_name='user_id', embedding=True)]\n",
      "\n",
      "sparse_embedding_list:\n",
      " [<tf.Tensor 'sparse_emb_movie_id/Identity:0' shape=(None, 1, 8) dtype=float32>, <tf.Tensor 'sparse_emb_user_id/Identity:0' shape=(None, 1, 8) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat) and x.embedding, dnn_feature_columns)) if dnn_feature_columns else []\n",
    "\n",
    "#features: OrderedDict([('movie_id',<tf.Tensor 'movie_id_7:0' shape=(None, 1) dtype=int32>), ...])\n",
    "# output from build_input_features( ), line 1 in DeepFM() function\n",
    "\n",
    "sparse_embedding_list = embedding_lookup(embedding_dict, features, sparse_feature_columns)\n",
    "print('sparse_feature_columns:\\n',sparse_feature_columns)\n",
    "print('\\nsparse_embedding_list:\\n',sparse_embedding_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#line 6 in input_from_Feature_Columns( )\n",
    "def get_dense_input(features,feature_columns):\n",
    "    dense_feature_columns = list(filter(lambda x:isinstance(x,DenseFeat),feature_columns)) if feature_columns else []\n",
    "    dense_input_list = []\n",
    "    for fc in dense_feature_columns:\n",
    "        dense_input_list.append(features[fc.name])\n",
    "    return dense_input_list\n",
    "\n",
    "dense_value_list = get_dense_input(features,feature_columns)\n",
    "dense_value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('movie_id',\n",
       "              <tf.Tensor 'movie_id_7:0' shape=(None, 1) dtype=int32>),\n",
       "             ('user_id',\n",
       "              <tf.Tensor 'user_id_5:0' shape=(None, 1) dtype=int32>)])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from deepctr.inputs \n",
    "def varlen_embedding_lookup(embedding_dict, sequence_input_dict, varlen_sparse_feature_columns):\n",
    "    varlen_embedding_vec_dict = {}\n",
    "    for fc in varlen_sparse_feature_columns:\n",
    "        feature_name = fc.name\n",
    "        embedding_name = fc.embedding_name\n",
    "        if fc.use_hash:\n",
    "            lookup_idx = Hash(fc.dimension, mask_zero=True)(sequence_input_dict[feature_name])\n",
    "        else:\n",
    "            lookup_idx = sequence_input_dict[feature_name]\n",
    "        varlen_embedding_vec_dict[feature_name] = embedding_dict[embedding_name](lookup_idx)\n",
    "\n",
    "    return varlen_embedding_vec_dict\n",
    "\n",
    "def get_varlen_pooling_list(embedding_dict, features, varlen_sparse_feature_columns):\n",
    "    pooling_vec_list = []\n",
    "    for fc in varlen_sparse_feature_columns:\n",
    "        feature_name = fc.name\n",
    "        combiner = fc.combiner\n",
    "        feature_length_name = feature_name + '_seq_length'\n",
    "        if feature_length_name in features:\n",
    "            vec = SequencePoolingLayer(combiner, supports_masking=False)(\n",
    "            [embedding_dict[feature_name], features[feature_length_name]])\n",
    "        else:\n",
    "            vec = SequencePoolingLayer(combiner, supports_masking=True)(\n",
    "            embedding_dict[feature_name])\n",
    "        pooling_vec_list.append(vec)\n",
    "    return pooling_vec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "varlen_sparse_feature_columns:  []\n",
      "\n",
      "sequence_embed_dict:  {}\n",
      "\n",
      "sequence_embed_list:  []\n"
     ]
    }
   ],
   "source": [
    "#line 2 in input_from_featre_columns( )\n",
    "varlen_sparse_feature_columns = list(filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if feature_columns else []\n",
    "print('varlen_sparse_feature_columns: ', varlen_sparse_feature_columns)\n",
    "\n",
    "sequence_embed_dict = varlen_embedding_lookup(embedding_dict,features,varlen_sparse_feature_columns)\n",
    "print('\\nsequence_embed_dict: ',sequence_embed_dict)\n",
    "\n",
    "sequence_embed_list = get_varlen_pooling_list(sequence_embed_dict, features, varlen_sparse_feature_columns)\n",
    "print('\\nsequence_embed_list: ', sequence_embed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#line 3 in DeepFM( ) function\n",
    "#features: output from build_input_features( ) from line 1 in DeepFM()\n",
    "#feature_columns: dnn_feature_columns\n",
    "def input_from_feature_columns(features,feature_columns, embedding_size, l2_reg, init_std, seed,prefix='',seq_mask_zero=True,support_dense=True):\n",
    "\n",
    "\n",
    "    sparse_feature_columns = list(filter(lambda x:isinstance(x,SparseFeat),feature_columns)) if feature_columns else []\n",
    "    varlen_sparse_feature_columns = list(filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if feature_columns else []\n",
    "\n",
    "    embedding_dict = create_embedding_matrix(feature_columns,l2_reg,init_std,seed,embedding_size, prefix=prefix,seq_mask_zero=seq_mask_zero)\n",
    "    \n",
    "    sparse_embedding_list = embedding_lookup(\n",
    "        embedding_dict, features, sparse_feature_columns)\n",
    "    dense_value_list = get_dense_input(features,feature_columns)\n",
    "    if not support_dense and len(dense_value_list) >0:\n",
    "        raise ValueError(\"DenseFeat is not supported in dnn_feature_columns\")\n",
    "\n",
    "    sequence_embed_dict = varlen_embedding_lookup(embedding_dict,features,varlen_sparse_feature_columns)\n",
    "    sequence_embed_list = get_varlen_pooling_list(sequence_embed_dict, features, varlen_sparse_feature_columns)\n",
    "    sparse_embedding_list += sequence_embed_list\n",
    "\n",
    "    return sparse_embedding_list, dense_value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse_embedding_list:  [<tf.Tensor 'sparse_emb_movie_id_1/Identity:0' shape=(None, 1, 8) dtype=float32>, <tf.Tensor 'sparse_emb_user_id_1/Identity:0' shape=(None, 1, 8) dtype=float32>]\n",
      "\n",
      "dense_value_list:  []\n"
     ]
    }
   ],
   "source": [
    "sparse_embedding_list, dense_value_list = input_from_feature_columns(features,dnn_feature_columns,\n",
    "     embedding_size=8, l2_reg=0.00001,init_std=0.0001,seed=1024)\n",
    "print('sparse_embedding_list: ', sparse_embedding_list)\n",
    "print('\\ndense_value_list: ', dense_value_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now define a SVD_NN() along the lines with DeepFM() function, using/customising Some of the above methods/functionalities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from tensorflow.python.keras.layers import  Embedding, Input, Dense\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#from deepctr.models import DeepFM\n",
    "\n",
    "from deepctr.inputs import SparseFeat#DenseFeat, get_fixlen_feature_names, VarLenSparseFeat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#insert this to from .layers.sequence import SequencePoolingLayer\n",
    "def build_input_features(feature_columns, include_varlen=True, prefix='',include_fixlen=True):\n",
    "    input_features = OrderedDict()\n",
    "    if include_fixlen:\n",
    "        for fc in feature_columns:\n",
    "            if isinstance(fc,SparseFeat):\n",
    "                input_features[fc.name] = Input(\n",
    "                    shape=(fc.dimension,), name=prefix+fc.name, dtype=fc.dtype)# Here shape changed from shape=(1,) to shape= (fc.dimesnion,)\n",
    "            #elif isinstance(fc,DenseFeat):\n",
    "             #   input_features[fc.name] = Input(\n",
    "              #      shape=(fc.dimension,), name=prefix + fc.name, dtype=fc.dtype)\n",
    "                \n",
    "    return input_features\n",
    "\n",
    "\n",
    "def NN_svd(fixlen_feature_columns, num_factors=100):\n",
    "    features = build_input_features(fixlen_feature_columns)\n",
    "    \n",
    "    inputs= list(features.values())\n",
    "    \n",
    "    hid_layer_1= Dense(num_factors)(inputs[0])\n",
    "    hid_layer_2= Dense(num_factors)(inputs[1])\n",
    "    \n",
    "    merge_layer = tf.keras.layers.dot([hid_layer_1, hid_layer_2], axes=1)\n",
    "    \n",
    "    predictions = Dense(5, activation='softmax')(merge_layer)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=predictions)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('movie_id',\n",
       "              <tf.Tensor 'movie_id_11:0' shape=(None, 1) dtype=int32>),\n",
       "             ('user_id',\n",
       "              <tf.Tensor 'user_id_9:0' shape=(None, 1) dtype=int32>)])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature= build_input_features(fixlen_feature_columns)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.expanduser('C:\\\\Users\\\\might\\\\Desktop\\\\jupyter notebooks\\\\u.data')\n",
    "df2= pd.read_csv(data_path, sep='\\t',names= 'user_id\tmovie_id\trating\ttimestamp'.split('\t'))#, header=None)#used for DeepCTR\n",
    "\n",
    "sparse_features = [\"movie_id\", \"user_id\"]\n",
    "y= ['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparseFeat(name='movie_id', dimension=1682, use_hash=False, dtype='int32', embedding_name='movie_id', embedding=True), SparseFeat(name='user_id', dimension=943, use_hash=False, dtype='int32', embedding_name='user_id', embedding=True)]\n"
     ]
    }
   ],
   "source": [
    "#This counts unique values & encodes existing value to new lable in progression\n",
    "for feat in sparse_features:\n",
    "        lbe = LabelEncoder()\n",
    "        df2[feat] = lbe.fit_transform(df2[feat])\n",
    "    ##unique features for each sparse field\n",
    "    \n",
    "fixlen_feature_columns = [SparseFeat(feat, df2[feat].nunique()) for feat in sparse_features]\n",
    "print(fixlen_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train, test = train_test_split(df2, test_size=0.2)\n",
    "\n",
    "train_model_input = [to_categorical(train[fc.name].values, num_classes= fc.dimension) for fc in fixlen_feature_columns]#includes values from only data[user_id], data[movie_id]\n",
    "test_model_input = [to_categorical(test[fc.name].values, num_classes= fc.dimension) for fc in fixlen_feature_columns]#includes values from only data[user_id], data[movie_id]\n",
    "\n",
    "\n",
    "#abc= to_categorical(train_model_input[0], num_classes= fixlen_feature_columns[0].dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "movie_id (InputLayer)           [(None, 1682)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_id (InputLayer)            [(None, 943)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 100)          168300      movie_id[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 100)          94400       user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_4 (Dot)                     (None, 1)            0           dense_12[0][0]                   \n",
      "                                                                 dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 5)            10          dot_4[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 262,710\n",
      "Trainable params: 262,710\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = NN_svd(fixlen_feature_columns)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc= model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1682, 100)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss= 'categorical_crossentropy',optimizer=\"sgd\", metrics=['mae', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical labels for softmax\n",
    "y_cat = to_categorical(train[y])\n",
    "\n",
    "y_cat= y_cat[:,1:]#stripping 0th column to shape (None,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64000 samples, validate on 16000 samples\n",
      "Epoch 1/8\n",
      "64000/64000 - 2s - loss: 1.4678 - mae: 0.2994 - accuracy: 0.3417 - val_loss: 1.4694 - val_mae: 0.2995 - val_accuracy: 0.3406\n",
      "Epoch 2/8\n",
      "64000/64000 - 2s - loss: 1.4675 - mae: 0.2993 - accuracy: 0.3417 - val_loss: 1.4693 - val_mae: 0.2994 - val_accuracy: 0.3406\n",
      "Epoch 3/8\n",
      "64000/64000 - 2s - loss: 1.4674 - mae: 0.2992 - accuracy: 0.3417 - val_loss: 1.4691 - val_mae: 0.2993 - val_accuracy: 0.3406\n",
      "Epoch 4/8\n",
      "64000/64000 - 2s - loss: 1.4672 - mae: 0.2991 - accuracy: 0.3417 - val_loss: 1.4690 - val_mae: 0.2993 - val_accuracy: 0.3406\n",
      "Epoch 5/8\n",
      "64000/64000 - 2s - loss: 1.4670 - mae: 0.2991 - accuracy: 0.3417 - val_loss: 1.4688 - val_mae: 0.2993 - val_accuracy: 0.3406\n",
      "Epoch 6/8\n",
      "64000/64000 - 2s - loss: 1.4668 - mae: 0.2991 - accuracy: 0.3417 - val_loss: 1.4686 - val_mae: 0.2992 - val_accuracy: 0.3406\n",
      "Epoch 7/8\n",
      "64000/64000 - 2s - loss: 1.4665 - mae: 0.2991 - accuracy: 0.3417 - val_loss: 1.4683 - val_mae: 0.2992 - val_accuracy: 0.3406\n",
      "Epoch 8/8\n",
      "64000/64000 - 2s - loss: 1.4662 - mae: 0.2990 - accuracy: 0.3417 - val_loss: 1.4680 - val_mae: 0.2993 - val_accuracy: 0.3406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24b5aa655c0>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_model_input, y_cat, batch_size=64, epochs=8, verbose=2, validation_split=0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVD as a module**\n",
    "    * Following svd.SVD is just the method to output model, whereas the data preparation has tp be done usual beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\might\\\\Desktop\\\\jupyter notebooks'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\might\\appdata\\local\\programs\\python\\python36\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#from deepctr.models import DeepFM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.expanduser('C:\\\\Users\\\\might\\\\Desktop\\\\jupyter notebooks\\\\u.data')\n",
    "df2= pd.read_csv(data_path, sep='\\t',names= 'user_id\tmovie_id\trating\ttimestamp'.split('\t'))#, header=None)#used for DeepCTR\n",
    "\n",
    "sparse_features = [\"movie_id\", \"user_id\"]\n",
    "y= ['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparseFeat(name='movie_id', dimension=1682, use_hash=False, dtype='int32', embedding_name='movie_id', embedding=True), SparseFeat(name='user_id', dimension=943, use_hash=False, dtype='int32', embedding_name='user_id', embedding=True)]\n"
     ]
    }
   ],
   "source": [
    "from deepctr.inputs import SparseFeat##Choosing SpareFeat Class for fixlen_feature_Column creates different input shape (None,1)\n",
    "fixlen_feature_columns = [SparseFeat(feat, df2[feat].nunique()) for feat in sparse_features]\n",
    "print(fixlen_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "movie_id (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_id (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 100)          200         movie_id[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 100)          200         user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 1)            0           dense_9[0][0]                    \n",
      "                                                                 dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 5)            10          dot_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 410\n",
      "Trainable params: 410\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from deepctr.models import svd\n",
    "\n",
    "model = svd.SVD(fixlen_feature_columns, task='multiclass')\n",
    "model.summary()\n",
    "#Here the Input layer is shaped (None, 1) with fixlen_feature_columns created using SparseFeat class\n",
    "#(due to structure of DeepCTR.Inputs.build_feature_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Changed the fixlen_feature_columns definition using DenseFeat instead of SparseFeat**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DenseFeat(name='movie_id', dimension=1682, dtype='float32'), DenseFeat(name='user_id', dimension=943, dtype='float32')]\n"
     ]
    }
   ],
   "source": [
    "from deepctr.inputs import DenseFeat\n",
    "fixlen_feature_columns = [DenseFeat(feat, df2[feat].nunique()) for feat in sparse_features]\n",
    "print(fixlen_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "movie_id (InputLayer)           [(None, 1682)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_id (InputLayer)            [(None, 943)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 100)          168300      movie_id[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 100)          94400       user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_4 (Dot)                     (None, 1)            0           dense_12[0][0]                   \n",
      "                                                                 dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 5)            10          dot_4[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 262,710\n",
      "Trainable params: 262,710\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from deepctr.models import svd\n",
    "\n",
    "model = svd.SVD(fixlen_feature_columns, task='multiclass')\n",
    "model.summary()\n",
    "#Now the Input layer is shaped (None, 1682) with fixlen_feature_columns created using DenseFeat class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss= 'categorical_crossentropy',optimizer=\"sgd\", metrics=['mae', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "train, test = train_test_split(df2, test_size=0.2)\n",
    "\n",
    "train_model_input = [to_categorical(train[fc.name].values, num_classes= fc.dimension) for fc in fixlen_feature_columns]#includes values from only data[user_id], data[movie_id]\n",
    "test_model_input = [to_categorical(test[fc.name].values, num_classes= fc.dimension) for fc in fixlen_feature_columns]#includes values from only data[user_id], data[movie_id]\n",
    "\n",
    "#categorical labels for softmax\n",
    "y_cat = to_categorical(train[y])\n",
    "\n",
    "y_cat= y_cat[:,1:]#stripping 0th column to shape (None,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64000 samples, validate on 16000 samples\n",
      "Epoch 1/8\n",
      "64000/64000 - 3s - loss: 1.5100 - mae: 0.3086 - accuracy: 0.3396 - val_loss: 1.4689 - val_mae: 0.3023 - val_accuracy: 0.3479\n",
      "Epoch 2/8\n",
      "64000/64000 - 2s - loss: 1.4723 - mae: 0.3013 - accuracy: 0.3412 - val_loss: 1.4607 - val_mae: 0.2997 - val_accuracy: 0.3479\n",
      "Epoch 3/8\n",
      "64000/64000 - 2s - loss: 1.4695 - mae: 0.2999 - accuracy: 0.3412 - val_loss: 1.4592 - val_mae: 0.2991 - val_accuracy: 0.3479\n",
      "Epoch 4/8\n",
      "64000/64000 - 2s - loss: 1.4688 - mae: 0.2996 - accuracy: 0.3412 - val_loss: 1.4586 - val_mae: 0.2988 - val_accuracy: 0.3479\n",
      "Epoch 5/8\n",
      "64000/64000 - 2s - loss: 1.4685 - mae: 0.2994 - accuracy: 0.3412 - val_loss: 1.4583 - val_mae: 0.2987 - val_accuracy: 0.3479\n",
      "Epoch 6/8\n",
      "64000/64000 - 2s - loss: 1.4682 - mae: 0.2993 - accuracy: 0.3412 - val_loss: 1.4580 - val_mae: 0.2986 - val_accuracy: 0.3479\n",
      "Epoch 7/8\n",
      "64000/64000 - 2s - loss: 1.4679 - mae: 0.2993 - accuracy: 0.3412 - val_loss: 1.4577 - val_mae: 0.2986 - val_accuracy: 0.3479\n",
      "Epoch 8/8\n",
      "64000/64000 - 2s - loss: 1.4676 - mae: 0.2992 - accuracy: 0.3412 - val_loss: 1.4575 - val_mae: 0.2985 - val_accuracy: 0.3479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e0af807a90>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_model_input, y_cat, batch_size=64, epochs=8, verbose=2, validation_split=0.2,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing svd_complete.py**\n",
    "\n",
    "* Following svd_complete.SVD_C is a class containing two methods\n",
    "      1. To prepare dataset & abstract the initial few steps done as rotuine to feed model.fit\n",
    "      2. To output model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepctr.models import svd_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data_path:  C:\\Users\\might\\Desktop\\jupyter notebooks\\u.data \n",
      "sparse_features:  ['movie_id', 'user_id']\n"
     ]
    }
   ],
   "source": [
    "print('Data_path: ',data_path, '\\nsparse_features: ',sparse_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-7acab8ef561a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtr_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtr_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mt_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvd_complete\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSVD_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msparse_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\might\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\deepctr\\models\\svd_complete.py\u001b[0m in \u001b[0;36mprepare_data\u001b[1;34m(cls, path, sparse_features, task)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mtrainset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mtrain_model_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mfc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdimension\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeature_columns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;31m#includes values from only data[user_id], data[movie_id]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0mtest_model_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mfc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdimension\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeature_columns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;31m#includes values from only data[user_id], data[movie_id]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtask\u001b[0m \u001b[1;33m==\u001b[0m\u001b[1;34m'binary'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\might\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\deepctr\\models\\svd_complete.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mtrainset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mtrain_model_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mfc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdimension\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeature_columns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;31m#includes values from only data[user_id], data[movie_id]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0mtest_model_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mfc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdimension\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeature_columns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;31m#includes values from only data[user_id], data[movie_id]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtask\u001b[0m \u001b[1;33m==\u001b[0m\u001b[1;34m'binary'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\might\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\utils\\np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mcategorical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[0mcategorical\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, (tr_x,tr_y), (t_x,t_y) = svd_complete.SVD_C.prepare_data(data_path, sparse_features=sparse_features, task='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
