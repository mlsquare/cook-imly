{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributing Sklearn's decompositon SVD to mlsquare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fork mlsquare repository to your account and clone.**\n",
    "\n",
    "**Or just Clone https://github.com/mlsquare/mlsquare.git**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Navigate to `src/mlsquare/architectures` folder, Where the code for mapping `TruncatedSVD()` to `tf.linalg.svd()` resides.\n",
    "* The code for mapping primal model(SVD) to corresponding TF equivalent is saved in `sklearn.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following notebook may serve as walkthough procedure/tutorial.**\n",
    "* Tutorial for how one may contribute new methods to mlsquare framework.\n",
    "* Walkthrough procedure for evaluating results with contributed svd method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Register the proxy SVD model in `mlsquare/architecture/sklearn.py` as follows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ..base import registry, BaseModel\n",
    "from mlsquare.base import registry, BaseModel\n",
    "from mlsquare.adapters.sklearn import SklearnKerasRegressor\n",
    "from mlsquare.architectures.sklearn import GeneralizedLinearModel\n",
    "\n",
    "from abc import abstractmethod\n",
    "import tensorflow as tf\n",
    "import pandas\n",
    "\n",
    "class DimensionalityReductionModel:\n",
    "    \"\"\"\n",
    "\tA base class for all matrix decomposition models.\n",
    "\n",
    "    This class can be used as a base class for any dimensionality reduction models.\n",
    "    While implementing ensure all required methods are implemented or over written\n",
    "    Please refer to sklearn decomposition module for more details.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "\tfit(input_args)\n",
    "        fits the model to output singular decomposed values.\n",
    "        But outputs an object to further transform.\n",
    "\n",
    "\tfir_transform(input_args)\n",
    "        fits the model to output input values with reduced dimensions.\n",
    "    \"\"\"\n",
    "    #@abstractmethod\n",
    "    #def fit(self, X, y= None, **kwargs):\n",
    "    #    \"\"\"Needs Implementation in sub classes\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit_transform(self, X, y=None, **kwargs):\n",
    "        \"\"\"Needs Implementation in sub classes\"\"\"\n",
    "\n",
    "@registry.register\n",
    "class SVD(DimensionalityReductionModel, GeneralizedLinearModel):\n",
    "    def __init__(self):\n",
    "        self.adapter = SklearnTfTransformer\n",
    "        self.module_name = 'sklearn'\n",
    "        self.name = 'TruncatedSVD'\n",
    "        self.version = 'default'\n",
    "        model_params = {'full_matrices': False, 'compute_uv': True, 'name':None}\n",
    "        self.set_params(params=model_params, set_by='model_init')\n",
    "\n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "        self.fit_transform(X)\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, y=None,**kwargs):\n",
    "        model_params= _parse_params(self._model_params, return_as='nested')\n",
    "\n",
    "        #changing to recommended dtype, accomodating dataframe & numpy array\n",
    "        X = np.array(X, dtype= np.float32 if str(X.values.dtype)==\n",
    "        'float32' else np.float64) if isinstance(X,\n",
    "        pandas.core.frame.DataFrame) else np.array(X, dtype= np.float32\n",
    "        if str(X.dtype)=='float32' else np.float64)\n",
    "\n",
    "        n_components= self.primal.n_components#using primal attributes passed from adapter\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        if n_components>= n_features:\n",
    "                raise ValueError(\"n_components must be < n_features;\"\n",
    "                                 \" got %d >= %d\" % (n_components, n_features))\n",
    "\n",
    "        sess= tf.Session()#for TF  1.13\n",
    "        s,u,v= sess.run(tf.linalg.svd(X, full_matrices=model_params['full_matrices'], compute_uv=model_params['compute_uv']))#for TF  1.13\n",
    "        #s: singular values\n",
    "        #u: normalised projection distances\n",
    "        #v: decomposition/projection orthogonal axes\n",
    "\n",
    "        self.components_= v[:n_components,:]\n",
    "        X_transformed = u[:,:n_components] * s[:n_components]\n",
    "\n",
    "        self.explained_variance_= np.var(X_transformed, axis=0)\n",
    "        self.singular_values_ = s[:n_components]\n",
    "\n",
    "        #passing sigma & vh to adapter for subsequent access from adapter object itself.\n",
    "        model_params={'singular_values_':self.singular_values_,'components_':self.components_}\n",
    "        self.update_params(model_params)\n",
    "\n",
    "        return X_transformed\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.dot(X, self.components_.T)\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        return np.dot(X, self.components_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define a new adapter `SklearnTfTransformer` mapping `sklearn.decomposition.TruncatedSVD`  to `tensorflow.linalg.svd` in `mlsquare/adapters/sklearn.py`  and work with usual sklearn methods. \n",
    "* The adapter serves as a wrapper to perform operations underlying `proxy_model` in architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlsquare.utils.functions import _parse_params\n",
    "import numpy as np\n",
    "from ..architectures import sklearn\n",
    "\n",
    "class SklearnTfTransformer():\n",
    "    \"\"\"\n",
    "\tAdapter to connect sklearn decomposition methods to respective TF implementations.\n",
    "\n",
    "    This class can be used as an adapter for primal decomposition methods that can\n",
    "    utilise TF backend for proxy model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    proxy_model : proxy model instance\n",
    "        The proxy model passed from dope.\n",
    "\n",
    "    primal_model : primal model instance\n",
    "        The primal model passed from dope.\n",
    "\n",
    "    params : dict, optional\n",
    "        Additional model params passed by the user.\n",
    "\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "\tfit(X, y)\n",
    "        Method to train a transpiled model\n",
    "\n",
    "\ttransform(X)\n",
    "        Method to transform the input matrix to truncated dimensions;\n",
    "        Only once the decomposed values are computed.\n",
    "\n",
    "\tfit_transform(X)\n",
    "        Method to right away transform the input matrix to truncated dimensions.\n",
    "\n",
    "\tinverse_transform(X)\n",
    "        This method returns Original values from the resulting decomposed matrices.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, proxy_model, primal_model, **kwargs):\n",
    "        self.primal_model = primal_model\n",
    "        self.proxy_model = proxy_model\n",
    "        self.proxy_model.primal = self.primal_model\n",
    "        #self.proxy_model(primal_model)#to access proxy_model.n_components\n",
    "        self.params = None\n",
    "\n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "        self.proxy_model.X = X\n",
    "        self.proxy_model.y = y\n",
    "\n",
    "        if self.params != None: ## Validate implementation with different types of tune input\n",
    "            if not isinstance(self.params, dict):\n",
    "                raise TypeError(\"Params should be of type 'dict'\")\n",
    "            self.params = _parse_params(self.params, return_as='flat')\n",
    "            self.proxy_model.update_params(self.params)\n",
    "\n",
    "        #if self.proxy_model.__class__.__name in ['SVD', 'PCA']:\n",
    "        if isinstance(self.proxy_model, (sklearn.DimensionalityReductionModel)):\n",
    "            self.fit_transform(X)\n",
    "\n",
    "            self.params = self.proxy_model.get_params()\n",
    "            #to avoid calling model.fit(X).proxy_model for sigma & Vh\n",
    "            self.components_= self.params['components_']\n",
    "            self.singular_values_= self.params['singular_values_']\n",
    "            return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if not isinstance(self.proxy_model, (sklearn.DimensionalityReductionModel)):\n",
    "            raise AttributeError(\"'SklearnTfTransformer' object has no attribute 'transform'\")\n",
    "        return self.proxy_model.transform(X)\n",
    "\n",
    "    def fit_transform(self, X,y=None):\n",
    "        if not isinstance(self.proxy_model, (sklearn.DimensionalityReductionModel)):\n",
    "            raise AttributeError(\"'SklearnTfTransformer' object has no attribute 'fit_transform'\")\n",
    "        self.proxy_model.primal = self.primal_model\n",
    "        return self.proxy_model.fit_transform(X)\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        if not isinstance(self.proxy_model, (sklearn.DimensionalityReductionModel)):\n",
    "            raise AttributeError(\"'SklearnTfTransformer' object has no attribute 'inverse_transform'\")\n",
    "        return self.proxy_model.inverse_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Registered methods so far:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/kev/Desktop/pyvirtual2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kev/Desktop/pyvirtual2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kev/Desktop/pyvirtual2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kev/Desktop/pyvirtual2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kev/Desktop/pyvirtual2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kev/Desktop/pyvirtual2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "2019-12-04 22:16:46,838\tINFO node.py:423 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-12-04_22-16-46_14956/logs.\n",
      "2019-12-04 22:16:46,947\tINFO services.py:363 -- Waiting for redis server at 127.0.0.1:50854 to respond...\n",
      "2019-12-04 22:16:47,064\tINFO services.py:363 -- Waiting for redis server at 127.0.0.1:63690 to respond...\n",
      "2019-12-04 22:16:47,066\tINFO services.py:760 -- Starting Redis shard with 20.0 GB max memory.\n",
      "2019-12-04 22:16:47,101\tINFO services.py:1384 -- Starting the Plasma object store with 1.0 GB memory using /dev/shm.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('sklearn',\n",
       "  'TruncatedSVD'): {'default': [<mlsquare.architectures.sklearn.SVD at 0x7fe568ff5ba8>,\n",
       "   mlsquare.adapters.sklearn.SklearnTfTransformer]},\n",
       " ('sklearn',\n",
       "  'LogisticRegression'): {'default': [<mlsquare.architectures.sklearn.LogisticRegression at 0x7fe568f870b8>,\n",
       "   mlsquare.adapters.sklearn.SklearnKerasClassifier]},\n",
       " ('sklearn',\n",
       "  'LinearRegression'): {'default': [<mlsquare.architectures.sklearn.LinearRegression at 0x7fe568f87278>,\n",
       "   mlsquare.adapters.sklearn.SklearnKerasRegressor]},\n",
       " ('sklearn',\n",
       "  'Ridge'): {'default': [<mlsquare.architectures.sklearn.Ridge at 0x7fe568f87438>,\n",
       "   mlsquare.adapters.sklearn.SklearnKerasRegressor]},\n",
       " ('sklearn',\n",
       "  'Lasso'): {'default': [<mlsquare.architectures.sklearn.Lasso at 0x7fe568f875f8>,\n",
       "   mlsquare.adapters.sklearn.SklearnKerasRegressor]},\n",
       " ('sklearn',\n",
       "  'ElasticNet'): {'default': [<mlsquare.architectures.sklearn.ElasticNet at 0x7fe568f877f0>,\n",
       "   mlsquare.adapters.sklearn.SklearnKerasRegressor]},\n",
       " ('sklearn',\n",
       "  'LinearSVC'): {'default': [<mlsquare.architectures.sklearn.LinearSVC at 0x7fe568f879b0>,\n",
       "   mlsquare.adapters.sklearn.SklearnKerasClassifier]},\n",
       " ('sklearn',\n",
       "  'SVC'): {'default': [<mlsquare.architectures.sklearn.SVC at 0x7fe568f87cf8>,\n",
       "   mlsquare.adapters.sklearn.SklearnKerasClassifier]},\n",
       " ('sklearn',\n",
       "  'DecisionTreeClassifier'): {'default': [<mlsquare.architectures.sklearn.DecisionTreeClassifier at 0x7fe568f93080>,\n",
       "   mlsquare.adapters.sklearn.SklearnKerasClassifier]}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlsquare.base import registry\n",
    "registry.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(**Once the new model is registered & corresponding adapter is defined in mlsquare framework.**)\n",
    "#### User Interaction with `dope` with sklearn SVD preference & intent to utilise underlying TF SVD \n",
    "\n",
    "    \n",
    "\n",
    "    1. a) User instantiates a primal model `sklearn.decomposition.TruncatedSVD` with args --`n_components` as number of required singular components.\n",
    "    b) User loads the data & proceed with necessary data preparation steps. \n",
    "    \n",
    "    \n",
    "    2. Now, import `dope` from mlsquare & `dope` the primal model by passing primal model to dope function. The `dope` function equips above primal model with standard sklearn methods--`fit, fit_transform, save, explain.`\n",
    "    \n",
    "    3.  Carry on with usual sklearn SVD methods; Try out sklearn \n",
    "    methods -- `.fit( )`, `.fit_transform( )`, `.transform( )` with the doped model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a Instantiate primal module\n",
    "* n_components: 10 (number of reduced dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "primal = TruncatedSVD(n_components=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'randomized',\n",
       " 'n_components': 10,\n",
       " 'n_iter': 5,\n",
       " 'random_state': None,\n",
       " 'tol': 0.0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primal.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b Following are data preparation steps required to instantiate a svd model\n",
    "* Also evaluating the regression results at various stages with varying dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original df_x dims: (506, 13)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "reg= linear_model.LinearRegression()\n",
    "\n",
    "boston =load_boston()\n",
    "df_x= pd.DataFrame(boston.data, columns= boston.feature_names)\n",
    "lbe= LabelEncoder()\n",
    "df_x = df_x.apply(lambda x: lbe.fit_transform(x))#df_x[col]))\n",
    "df_y= df_y= pd.DataFrame(boston.target)\n",
    "\n",
    "\n",
    "print('original df_x dims:', df_x.shape)\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(df_x, df_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>320</td>\n",
       "      <td>172</td>\n",
       "      <td>297</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>9</td>\n",
       "      <td>356</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>279</td>\n",
       "      <td>225</td>\n",
       "      <td>333</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>356</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>400</td>\n",
       "      <td>159</td>\n",
       "      <td>333</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>271</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CRIM  ZN  INDUS  CHAS  NOX   RM  AGE  DIS  RAD  TAX  PTRATIO    B  LSTAT\n",
       "0     0   3     19     0   51  320  172  297    0   34        9  356     53\n",
       "1    23   0     56     0   36  279  225  333    1   11       23  356    161\n",
       "2    22   0     56     0   36  400  159  333    1   11       23  271     28"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Validating results with full dimensionality.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7102619689909073\n"
     ]
    }
   ],
   "source": [
    "reg= linear_model.LinearRegression()\n",
    "reg.fit(xtrain, ytrain)\n",
    "print(reg.score(xtest, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Validating results with reduced dimensionality through primal model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn_svd truncated dims: (506, 10)\n",
      "0.7406353348247017\n"
     ]
    }
   ],
   "source": [
    "skl_truncated_x = primal.fit(df_x).transform(df_x)\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(skl_truncated_x, df_y, test_size=0.2)\n",
    "print('sklearn_svd truncated dims:', skl_truncated_x.shape)\n",
    "reg= linear_model.LinearRegression()\n",
    "reg.fit(xtrain, ytrain)\n",
    "print(reg.score(xtest, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. dope the model to obtain keras svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transpiling your model to it's Deep Neural Network equivalent...\n"
     ]
    }
   ],
   "source": [
    "from mlsquare import dope\n",
    "\n",
    "model = dope(primal)# adapter(proxy_model=proxy_model, primal_model=primal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proxy model object from registry:\n",
      " <mlsquare.architectures.sklearn.SVD object at 0x7fe568ff5ba8> \n",
      "\n",
      "correspnding adapter:\n",
      " <mlsquare.adapters.sklearn.SklearnTfTransformer object at 0x7fe5402cab38>\n"
     ]
    }
   ],
   "source": [
    "print('proxy model object from registry:\\n', model.proxy_model, '\\n\\ncorrespnding adapter:\\n', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "??model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Try out sklearn methods-- `.fit( )`, `.transform( )`& `.fit_transform( )` to obtain reduced dimensionality, with sklearn's `boston_dataset` from `1.b` above.\n",
    "* Fitting the doped model with -- Dataframe input Or Numpy array inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp= np.array(df_x.values, dtype= np.float64)\n",
    "\n",
    "#dope_truncated_x=model.fit_transform(df_x) #takes in dataframe input\n",
    "dope_truncated_x= model.fit_transform(inp)\n",
    "\n",
    "dope_truncated_x.shape\n",
    "#dimensionality reduced to n_components using tf.linalg.svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Validating results with reduced dimensionality through doped model & ascertaining approximately faithful results through underlying TF method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doped_svd truncated dims: (506, 10)\n",
      "0.7128582648403458\n"
     ]
    }
   ],
   "source": [
    "#truncated_x= model.fit(df_x).fit_transform(df_x)\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(dope_truncated_x, df_y, test_size=0.2)\n",
    "\n",
    "print('doped_svd truncated dims:', dope_truncated_x.shape)\n",
    "\n",
    "reg= linear_model.LinearRegression()\n",
    "reg.fit(xtrain, ytrain)\n",
    "print(reg.score(xtest, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks on accessing/evaluating adapter methods & attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "* `model` : adapter/`SklearnTfTransformer` object (Cell#8).\n",
    "* `primal` : sklearn/`TruncatedSVD` object (Cell#7)\n",
    "* `proxy_model` : architecture/`SVD` object (Cell#8).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `model.fit` implicitly calls adapter's `.fit_transform` which then routes to archiecture's `proxy_model.fit_transform()` where -- `components_`, `singular_values_` & `X_transformed` is computed.\n",
    "* model fit returns adapter object only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlsquare.adapters.sklearn.SklearnTfTransformer at 0x7fe5402cab38>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In sklearn's context, `primal_model.fit()` leads to computation of intrinsic state/attributes such as `components_` or `vh` and `singular_values_` or Sigma along with truncated input values.\n",
    "* In sklearn `.fit( )` enables user to call for values of Sigma & Vh as `primal_model.singular_values_` & `primal_model.components_` respectively; So should be the case post doping primal_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12795.41792279,  5233.02454574,  2860.09836322,  2199.84308866,\n",
       "        1596.72603145,  1118.21187031,   369.75399984,   304.29119115,\n",
       "         245.93492619,   193.30180866])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primal.singular_values_#Output from primal model post fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12795.41792279,  5233.02454574,  2860.09836322,  2199.84308866,\n",
       "        1596.72603145,  1118.21187031,   369.75399984,   304.29119115,\n",
       "         245.93492619,   193.30180866])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.singular_values_#Output from proxy_model post fit on dope object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* further Chaining/subsequent method calls from `model.fit()` to `model.transform()` as in context of sklearn's `primal_model.fit(inp).transform(inp)` executes on same sklearn object.\n",
    "* Since the adapter serves as a wrapper to access architecture's methods for whatever operations required on proxy_model. In case of chained calls/subsequent methods, it is ensured that respective operations are accessed via adapter object(model) ONLY.\n",
    "* So all defined methods in architecture-- `fit`, `fit_transform`, `transform`, `inverse_transform` should be availed through respective adapter methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SklearnTfTransformer.transform of <mlsquare.adapters.sklearn.SklearnTfTransformer object at 0x7fe5402cab38>>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(inp).transform#chained call on adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SklearnTfTransformer.fit_transform of <mlsquare.adapters.sklearn.SklearnTfTransformer object at 0x7fe5402cab38>>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_transform"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
