{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments using IMLY ###\n",
    "\n",
    "This notebook contains experimental runs of IMLY with different datasets.  \n",
    "The readings of these experiments can be referred to in this [sheet](https://docs.google.com/spreadsheets/d/1E5jcq2w42gN8bMIaeaRJpAdhgSVN-2XDJ_YTHe4qfwY/edit?usp=sharing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #1\n",
    "\n",
    "#### Diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:16<00:00, 16.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "266/266 [==============================] - 0s 85us/step\n",
      "Uploading ../data/diabetes_linear_regression.pdf to Amazon S3 bucket mlsquare-pdf\n",
      "..."
     ]
    }
   ],
   "source": [
    "import automation_script\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dataset_info = automation_script.get_dataset_info(\"diabetes\")\n",
    "url = \"../data/diabetes.csv\" if path.exists(\"../data/diabetes.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "sc = StandardScaler()\n",
    "data = sc.fit_transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "X = data.iloc[:,:-1] #Why? \n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "# X = preprocessing.scale(X)\n",
    "# Y = preprocessing.normalize(Y)\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'linear_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-26 10:33:05,836\tINFO tune.py:135 -- Tip: to resume incomplete experiments, pass resume='prompt' or resume=True to run_experiments()\n",
      "2019-02-26 10:33:05,836\tINFO tune.py:145 -- Starting a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.6/8.2 GB\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.6/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "PENDING trials:\n",
      " - train_model_1_epochs=200,optimizer=adam:\tPENDING\n",
      " - train_model_2_epochs=100,optimizer=nadam:\tPENDING\n",
      " - train_model_3_epochs=200,optimizer=nadam:\tPENDING\n",
      "RUNNING trials:\n",
      " - train_model_0_epochs=100,optimizer=adam:\tRUNNING\n",
      "\n",
      "Result for train_model_0_epochs=100,optimizer=adam:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''linear'', ''optimizer'':\n",
      "    ''adam'', ''losses'': ''mse'', ''epochs'': 100}.h5'\n",
      "  date: 2019-02-26_10-33-07\n",
      "  done: false\n",
      "  experiment_id: 33b729d05df24758ba5fe85671148c37\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.0\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 3218\n",
      "  time_since_restore: 1.001121997833252\n",
      "  time_this_iter_s: 1.001121997833252\n",
      "  time_total_s: 1.001121997833252\n",
      "  timestamp: 1551157387\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "Result for train_model_1_epochs=200,optimizer=adam:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''linear'', ''optimizer'':\n",
      "    ''adam'', ''losses'': ''mse'', ''epochs'': 200}.h5'\n",
      "  date: 2019-02-26_10-33-07\n",
      "  done: false\n",
      "  experiment_id: 153eae477db94beb80f2c7b34fff5ea0\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.0\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 3194\n",
      "  time_since_restore: 1.0011217594146729\n",
      "  time_this_iter_s: 1.0011217594146729\n",
      "  time_total_s: 1.0011217594146729\n",
      "  timestamp: 1551157387\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "Result for train_model_0_epochs=100,optimizer=adam:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''linear'', ''optimizer'':\n",
      "    ''adam'', ''losses'': ''mse'', ''epochs'': 100}.h5'\n",
      "  date: 2019-02-26_10-33-08\n",
      "  done: true\n",
      "  experiment_id: 33b729d05df24758ba5fe85671148c37\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 2\n",
      "  mean_accuracy: 0.0\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 3218\n",
      "  time_since_restore: 2.0026519298553467\n",
      "  time_this_iter_s: 1.0015299320220947\n",
      "  time_total_s: 2.0026519298553467\n",
      "  timestamp: 1551157388\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  \n",
      "Result for train_model_1_epochs=200,optimizer=adam:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''linear'', ''optimizer'':\n",
      "    ''adam'', ''losses'': ''mse'', ''epochs'': 200}.h5'\n",
      "  date: 2019-02-26_10-33-08\n",
      "  done: true\n",
      "  experiment_id: 153eae477db94beb80f2c7b34fff5ea0\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 2\n",
      "  mean_accuracy: 0.0\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 3194\n",
      "  time_since_restore: 2.002711534500122\n",
      "  time_this_iter_s: 1.0015897750854492\n",
      "  time_total_s: 2.002711534500122\n",
      "  timestamp: 1551157388\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  \n",
      "Result for train_model_3_epochs=200,optimizer=nadam:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''linear'', ''optimizer'':\n",
      "    ''nadam'', ''losses'': ''mse'', ''epochs'': 200}.h5'\n",
      "  date: 2019-02-26_10-33-11\n",
      "  done: false\n",
      "  experiment_id: 00831bcce9684e87ba4c7349b86725d0\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.0\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 3190\n",
      "  time_since_restore: 1.0000834465026855\n",
      "  time_this_iter_s: 1.0000834465026855\n",
      "  time_total_s: 1.0000834465026855\n",
      "  timestamp: 1551157391\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.7/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "RUNNING trials:\n",
      " - train_model_2_epochs=100,optimizer=nadam:\tRUNNING\n",
      " - train_model_3_epochs=200,optimizer=nadam:\tRUNNING [pid=3190], 1 s, 1 iter, 0 acc\n",
      "TERMINATED trials:\n",
      " - train_model_0_epochs=100,optimizer=adam:\tTERMINATED [pid=3218], 2 s, 2 iter, 0 acc\n",
      " - train_model_1_epochs=200,optimizer=adam:\tTERMINATED [pid=3194], 2 s, 2 iter, 0 acc\n",
      "\n",
      "Result for train_model_2_epochs=100,optimizer=nadam:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''linear'', ''optimizer'':\n",
      "    ''nadam'', ''losses'': ''mse'', ''epochs'': 100}.h5'\n",
      "  date: 2019-02-26_10-33-11\n",
      "  done: false\n",
      "  experiment_id: ab8c1afc8f9d4832a7a4b704f44cf2f5\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.0\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 3193\n",
      "  time_since_restore: 1.00111985206604\n",
      "  time_this_iter_s: 1.00111985206604\n",
      "  time_total_s: 1.00111985206604\n",
      "  timestamp: 1551157391\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "Result for train_model_3_epochs=200,optimizer=nadam:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''linear'', ''optimizer'':\n",
      "    ''nadam'', ''losses'': ''mse'', ''epochs'': 200}.h5'\n",
      "  date: 2019-02-26_10-33-12\n",
      "  done: true\n",
      "  experiment_id: 00831bcce9684e87ba4c7349b86725d0\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 2\n",
      "  mean_accuracy: 0.0\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 3190\n",
      "  time_since_restore: 2.0016064643859863\n",
      "  time_this_iter_s: 1.0015230178833008\n",
      "  time_total_s: 2.0016064643859863\n",
      "  timestamp: 1551157392\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  \n",
      "Result for train_model_2_epochs=100,optimizer=nadam:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''linear'', ''optimizer'':\n",
      "    ''nadam'', ''losses'': ''mse'', ''epochs'': 100}.h5'\n",
      "  date: 2019-02-26_10-33-12\n",
      "  done: true\n",
      "  experiment_id: ab8c1afc8f9d4832a7a4b704f44cf2f5\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 2\n",
      "  mean_accuracy: 0.0\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 3193\n",
      "  time_since_restore: 2.0026187896728516\n",
      "  time_this_iter_s: 1.0014989376068115\n",
      "  time_total_s: 2.0026187896728516\n",
      "  timestamp: 1551157392\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.7/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "TERMINATED trials:\n",
      " - train_model_0_epochs=100,optimizer=adam:\tTERMINATED [pid=3218], 2 s, 2 iter, 0 acc\n",
      " - train_model_1_epochs=200,optimizer=adam:\tTERMINATED [pid=3194], 2 s, 2 iter, 0 acc\n",
      " - train_model_2_epochs=100,optimizer=nadam:\tTERMINATED [pid=3193], 2 s, 2 iter, 0 acc\n",
      " - train_model_3_epochs=200,optimizer=nadam:\tTERMINATED [pid=3190], 2 s, 2 iter, 0 acc\n",
      "\n",
      "Creating model...\n",
      "Loading from /home/shakkeel/ray_results/experiment_name/train_model_0_epochs=100,optimizer=adam_2019-02-26_10-33-05nxat66bh/weights_tune_{'units': 1, 'activation': 'linear', 'optimizer': 'adam', 'losses': 'mse', 'epochs': 100}.h5\n"
     ]
    }
   ],
   "source": [
    "## Testing with Tune ##\n",
    "\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ray import tune\n",
    "from hyperopt import hp\n",
    "from imly import dope\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "url = \"../data/diabetes.csv\"\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "sc = StandardScaler()\n",
    "data = sc.fit_transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.60, random_state=0)\n",
    "glm_1 = {\n",
    "    \"epochs\": tune.grid_search([100, 200]),\n",
    "    \"optimizer\": tune.grid_search([\"adam\", \"nadam\"])\n",
    "}\n",
    "\n",
    "space = {\n",
    "    \"lr\": hp.uniform(\"lr\", 0.001, 0.1),\n",
    "    'activation': hp.choice(\"activation\", [\"relu\", \"tanh\"])\n",
    "}\n",
    "\n",
    "m = dope(LinearRegression())\n",
    "m.fit(x_train, y_train, params=glm_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #2\n",
    "\n",
    "#### UCI Abalone dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "794/794 [==============================] - ETA:  - 0s 77us/step\n",
      "Confusion matrix, without normalization\n",
      "Uploading ../data/uci_abalone_logistic_regression.pdf to Amazon S3 bucket mlsquare-datasets\n",
      "..."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "dataset_info = automation_script.get_dataset_info(\"uci_abalone\")\n",
    "\n",
    "names = [\"sex\", \"length\", \"diameter\", \"height\", \"whole weight\",\n",
    "        \"shucked weight\", \"viscera weight\", \"shell weight\", \"rings\"]\n",
    "url = \"../data/abalone.data.csv\" if path.exists(\"../data/abalone.data.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, names=names, index_col=False)\n",
    "data.head()\n",
    "\n",
    "# Check for columns that contain missing values #\n",
    "col_names = data.columns\n",
    "\n",
    "num_data = data.shape[0]\n",
    "\n",
    "categorical_col = ['sex']\n",
    "for col in categorical_col:\n",
    "    b, c = np.unique(data[col], return_inverse=True)\n",
    "    data[col] = c\n",
    "\n",
    "    \n",
    "# Filter dataset to contain 'rings' 9 and 10 #\n",
    "data = data[data['rings'].isin([9,10])]\n",
    "data['rings'] = data['rings'].map({9: 0, 10: 1})\n",
    "\n",
    "\n",
    "feature_list = names[:7]\n",
    "X = data.loc[:, feature_list]\n",
    "Y = data[['rings']]\n",
    "\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "2019-02-26 10:36:34,487\tINFO tune.py:135 -- Tip: to resume incomplete experiments, pass resume='prompt' or resume=True to run_experiments()\n",
      "2019-02-26 10:36:34,487\tINFO tune.py:145 -- Starting a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.4/8.2 GB\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.4/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "PENDING trials:\n",
      " - train_model_1_epochs=200,optimizer=adam:\tPENDING\n",
      " - train_model_2_epochs=100,optimizer=nadam:\tPENDING\n",
      " - train_model_3_epochs=200,optimizer=nadam:\tPENDING\n",
      "RUNNING trials:\n",
      " - train_model_0_epochs=100,optimizer=adam:\tRUNNING\n",
      "\n",
      "Result for train_model_0_epochs=100,optimizer=adam:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''sigmoid'', ''optimizer'':\n",
      "    ''adam'', ''losses'': ''binary_crossentropy'', ''epochs'': 100}.h5'\n",
      "  date: 2019-02-26_10-36-36\n",
      "  done: false\n",
      "  experiment_id: 2f33c4ae29914a75be6aca1bbc061721\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.5103969754534994\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 3195\n",
      "  time_since_restore: 1.0003676414489746\n",
      "  time_this_iter_s: 1.0003676414489746\n",
      "  time_total_s: 1.0003676414489746\n",
      "  timestamp: 1551157596\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "Result for train_model_1_epochs=200,optimizer=adam:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''sigmoid'', ''optimizer'':\n",
      "    ''adam'', ''losses'': ''binary_crossentropy'', ''epochs'': 200}.h5'\n",
      "  date: 2019-02-26_10-36-36\n",
      "  done: false\n",
      "  experiment_id: ce5fa672f8d541b9af1d59de6de3f570\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.5822306242128852\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 3214\n",
      "  time_since_restore: 1.0010576248168945\n",
      "  time_this_iter_s: 1.0010576248168945\n",
      "  time_total_s: 1.0010576248168945\n",
      "  timestamp: 1551157596\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "Result for train_model_0_epochs=100,optimizer=adam:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''sigmoid'', ''optimizer'':\n",
      "    ''adam'', ''losses'': ''binary_crossentropy'', ''epochs'': 100}.h5'\n",
      "  date: 2019-02-26_10-36-37\n",
      "  done: true\n",
      "  experiment_id: 2f33c4ae29914a75be6aca1bbc061721\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 2\n",
      "  mean_accuracy: 0.5103969754534994\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 3195\n",
      "  time_since_restore: 2.00187349319458\n",
      "  time_this_iter_s: 1.0015058517456055\n",
      "  time_total_s: 2.00187349319458\n",
      "  timestamp: 1551157597\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  \n",
      "Result for train_model_1_epochs=200,optimizer=adam:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''sigmoid'', ''optimizer'':\n",
      "    ''adam'', ''losses'': ''binary_crossentropy'', ''epochs'': 200}.h5'\n",
      "  date: 2019-02-26_10-36-37\n",
      "  done: true\n",
      "  experiment_id: ce5fa672f8d541b9af1d59de6de3f570\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 2\n",
      "  mean_accuracy: 0.5822306242128852\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 3214\n",
      "  time_since_restore: 2.002570629119873\n",
      "  time_this_iter_s: 1.0015130043029785\n",
      "  time_total_s: 2.002570629119873\n",
      "  timestamp: 1551157597\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  \n",
      "Result for train_model_2_epochs=100,optimizer=nadam:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''sigmoid'', ''optimizer'':\n",
      "    ''nadam'', ''losses'': ''binary_crossentropy'', ''epochs'': 100}.h5'\n",
      "  date: 2019-02-26_10-36-39\n",
      "  done: false\n",
      "  experiment_id: b398bb8b96db4d3bbe46bea961f0c515\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.62570888547681\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 3192\n",
      "  time_since_restore: 1.0011212825775146\n",
      "  time_this_iter_s: 1.0011212825775146\n",
      "  time_total_s: 1.0011212825775146\n",
      "  timestamp: 1551157599\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.5/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "RUNNING trials:\n",
      " - train_model_2_epochs=100,optimizer=nadam:\tRUNNING [pid=3192], 1 s, 1 iter, 0.626 acc\n",
      " - train_model_3_epochs=200,optimizer=nadam:\tRUNNING\n",
      "TERMINATED trials:\n",
      " - train_model_0_epochs=100,optimizer=adam:\tTERMINATED [pid=3195], 2 s, 2 iter, 0.51 acc\n",
      " - train_model_1_epochs=200,optimizer=adam:\tTERMINATED [pid=3214], 2 s, 2 iter, 0.582 acc\n",
      "\n",
      "Result for train_model_3_epochs=200,optimizer=nadam:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''sigmoid'', ''optimizer'':\n",
      "    ''nadam'', ''losses'': ''binary_crossentropy'', ''epochs'': 200}.h5'\n",
      "  date: 2019-02-26_10-36-39\n",
      "  done: false\n",
      "  experiment_id: ba1f0b97d391484f92cf2f0fef975a86\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.6049149342881258\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 3191\n",
      "  time_since_restore: 1.0011262893676758\n",
      "  time_this_iter_s: 1.0011262893676758\n",
      "  time_total_s: 1.0011262893676758\n",
      "  timestamp: 1551157599\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "Result for train_model_2_epochs=100,optimizer=nadam:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''sigmoid'', ''optimizer'':\n",
      "    ''nadam'', ''losses'': ''binary_crossentropy'', ''epochs'': 100}.h5'\n",
      "  date: 2019-02-26_10-36-40\n",
      "  done: true\n",
      "  experiment_id: b398bb8b96db4d3bbe46bea961f0c515\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 2\n",
      "  mean_accuracy: 0.62570888547681\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 3192\n",
      "  time_since_restore: 2.0026702880859375\n",
      "  time_this_iter_s: 1.0015490055084229\n",
      "  time_total_s: 2.0026702880859375\n",
      "  timestamp: 1551157600\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  \n",
      "Result for train_model_3_epochs=200,optimizer=nadam:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''sigmoid'', ''optimizer'':\n",
      "    ''nadam'', ''losses'': ''binary_crossentropy'', ''epochs'': 200}.h5'\n",
      "  date: 2019-02-26_10-36-40\n",
      "  done: true\n",
      "  experiment_id: ba1f0b97d391484f92cf2f0fef975a86\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 2\n",
      "  mean_accuracy: 0.6049149342881258\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 3191\n",
      "  time_since_restore: 2.0023508071899414\n",
      "  time_this_iter_s: 1.0012245178222656\n",
      "  time_total_s: 2.0023508071899414\n",
      "  timestamp: 1551157600\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.5/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "TERMINATED trials:\n",
      " - train_model_0_epochs=100,optimizer=adam:\tTERMINATED [pid=3195], 2 s, 2 iter, 0.51 acc\n",
      " - train_model_1_epochs=200,optimizer=adam:\tTERMINATED [pid=3214], 2 s, 2 iter, 0.582 acc\n",
      " - train_model_2_epochs=100,optimizer=nadam:\tTERMINATED [pid=3192], 2 s, 2 iter, 0.626 acc\n",
      " - train_model_3_epochs=200,optimizer=nadam:\tTERMINATED [pid=3191], 2 s, 2 iter, 0.605 acc\n",
      "\n",
      "Creating model...\n",
      "Loading from /home/shakkeel/ray_results/experiment_name/train_model_2_epochs=100,optimizer=nadam_2019-02-26_10-36-3761kmoh9x/weights_tune_{'units': 1, 'activation': 'sigmoid', 'optimizer': 'nadam', 'losses': 'binary_crossentropy', 'epochs': 100}.h5\n"
     ]
    }
   ],
   "source": [
    "## Testing with Tune integration ##\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "names = [\"sex\", \"length\", \"diameter\", \"height\", \"whole weight\",\n",
    "        \"shucked weight\", \"viscera weight\", \"shell weight\", \"rings\"]\n",
    "url = \"../data/abalone.data.csv\"\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, names=names, index_col=False)\n",
    "data.head()\n",
    "\n",
    "# Check for columns that contain missing values #\n",
    "col_names = data.columns\n",
    "\n",
    "num_data = data.shape[0]\n",
    "\n",
    "categorical_col = ['sex']\n",
    "for col in categorical_col:\n",
    "    b, c = np.unique(data[col], return_inverse=True)\n",
    "    data[col] = c\n",
    "\n",
    "    \n",
    "# Filter dataset to contain 'rings' 9 and 10 #\n",
    "data = data[data['rings'].isin([9,10])]\n",
    "data['rings'] = data['rings'].map({9: 0, 10: 1})\n",
    "\n",
    "\n",
    "feature_list = names[:7]\n",
    "X = data.loc[:, feature_list]\n",
    "Y = data[['rings']]\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.60, random_state=0)\n",
    "glm_1 = {\n",
    "    \"epochs\": tune.grid_search([100, 200]),\n",
    "    \"optimizer\": tune.grid_search([\"adam\", \"nadam\"])\n",
    "}\n",
    "\n",
    "space = {\n",
    "    \"lr\": hp.uniform(\"lr\", 0.001, 0.1),\n",
    "    'activation': hp.choice(\"activation\", [\"relu\", \"tanh\"])\n",
    "}\n",
    "\n",
    "m = dope(LogisticRegression())\n",
    "m.fit(x_train, y_train, params=glm_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "794/794 [==============================] - 0s 75us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5856423176055591"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #3\n",
    "\n",
    "#### UCI Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl'\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "/home/shakkeel/anaconda3/envs/test_env36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "2019-04-06 15:34:52,441\tINFO node.py:423 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-04-06_15-34-52_18309/logs.\n",
      "2019-04-06 15:34:52,553\tINFO services.py:363 -- Waiting for redis server at 127.0.0.1:29153 to respond...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transpiling your model to it's Deep Neural Network equivalent\n",
      "WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-06 15:34:52,681\tINFO services.py:363 -- Waiting for redis server at 127.0.0.1:48914 to respond...\n",
      "2019-04-06 15:34:52,684\tINFO services.py:760 -- Starting Redis shard with 20.0 GB max memory.\n",
      "2019-04-06 15:34:52,721\tINFO services.py:1384 -- Starting the Plasma object store with 1.0 GB memory using /dev/shm.\n",
      "2019-04-06 15:34:52,746\tWARNING services.py:863 -- Failed to start the reporter. The reporter requires 'pip install psutil'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shakkeel/anaconda3/envs/test_env36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/shakkeel/anaconda3/envs/test_env36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 3.7556 - acc: 0.5000\n",
      "Epoch 2/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 3.7483 - acc: 0.5000\n",
      "Epoch 3/500\n",
      "40/40 [==============================] - 0s 49us/step - loss: 3.7420 - acc: 0.5000\n",
      "Epoch 4/500\n",
      "40/40 [==============================] - 0s 38us/step - loss: 3.7347 - acc: 0.5000\n",
      "Epoch 5/500\n",
      "40/40 [==============================] - 0s 48us/step - loss: 3.7274 - acc: 0.5000\n",
      "Epoch 6/500\n",
      "40/40 [==============================] - 0s 32us/step - loss: 3.7201 - acc: 0.5000\n",
      "Epoch 7/500\n",
      "40/40 [==============================] - 0s 40us/step - loss: 3.7128 - acc: 0.5000\n",
      "Epoch 8/500\n",
      "40/40 [==============================] - 0s 31us/step - loss: 3.7055 - acc: 0.5000\n",
      "Epoch 9/500\n",
      "40/40 [==============================] - 0s 43us/step - loss: 3.6982 - acc: 0.5000\n",
      "Epoch 10/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 3.6910 - acc: 0.5000\n",
      "Epoch 11/500\n",
      "40/40 [==============================] - 0s 32us/step - loss: 3.6837 - acc: 0.5000\n",
      "Epoch 12/500\n",
      "40/40 [==============================] - 0s 63us/step - loss: 3.6764 - acc: 0.5000\n",
      "Epoch 13/500\n",
      "40/40 [==============================] - 0s 111us/step - loss: 3.6692 - acc: 0.5000\n",
      "Epoch 14/500\n",
      "40/40 [==============================] - 0s 85us/step - loss: 3.6618 - acc: 0.5000\n",
      "Epoch 15/500\n",
      "40/40 [==============================] - 0s 42us/step - loss: 3.6546 - acc: 0.5000\n",
      "Epoch 16/500\n",
      "40/40 [==============================] - 0s 53us/step - loss: 3.6473 - acc: 0.5000\n",
      "Epoch 17/500\n",
      "40/40 [==============================] - 0s 35us/step - loss: 3.6400 - acc: 0.5000\n",
      "Epoch 18/500\n",
      "40/40 [==============================] - 0s 49us/step - loss: 3.6328 - acc: 0.5000\n",
      "Epoch 19/500\n",
      "40/40 [==============================] - 0s 45us/step - loss: 3.6255 - acc: 0.5000\n",
      "Epoch 20/500\n",
      "40/40 [==============================] - 0s 43us/step - loss: 3.6183 - acc: 0.5000\n",
      "Epoch 21/500\n",
      "40/40 [==============================] - 0s 41us/step - loss: 3.6110 - acc: 0.5000\n",
      "Epoch 22/500\n",
      "40/40 [==============================] - 0s 45us/step - loss: 3.6038 - acc: 0.5000\n",
      "Epoch 23/500\n",
      "40/40 [==============================] - 0s 46us/step - loss: 3.5965 - acc: 0.5000\n",
      "Epoch 24/500\n",
      "40/40 [==============================] - 0s 35us/step - loss: 3.5893 - acc: 0.5000\n",
      "Epoch 25/500\n",
      "40/40 [==============================] - 0s 40us/step - loss: 3.5820 - acc: 0.5000\n",
      "Epoch 26/500\n",
      "40/40 [==============================] - 0s 93us/step - loss: 3.5747 - acc: 0.5000\n",
      "Epoch 27/500\n",
      "40/40 [==============================] - 0s 55us/step - loss: 3.5675 - acc: 0.5000\n",
      "Epoch 28/500\n",
      "40/40 [==============================] - 0s 48us/step - loss: 3.5602 - acc: 0.5000\n",
      "Epoch 29/500\n",
      "40/40 [==============================] - 0s 48us/step - loss: 3.5529 - acc: 0.5000\n",
      "Epoch 30/500\n",
      "40/40 [==============================] - 0s 32us/step - loss: 3.5457 - acc: 0.5000\n",
      "Epoch 31/500\n",
      "40/40 [==============================] - 0s 42us/step - loss: 3.5385 - acc: 0.5000\n",
      "Epoch 32/500\n",
      "40/40 [==============================] - 0s 45us/step - loss: 3.5313 - acc: 0.5000\n",
      "Epoch 33/500\n",
      "40/40 [==============================] - 0s 27us/step - loss: 3.5241 - acc: 0.5000\n",
      "Epoch 34/500\n",
      "40/40 [==============================] - 0s 35us/step - loss: 3.5169 - acc: 0.5000\n",
      "Epoch 35/500\n",
      "40/40 [==============================] - 0s 34us/step - loss: 3.5097 - acc: 0.5000\n",
      "Epoch 36/500\n",
      "40/40 [==============================] - 0s 31us/step - loss: 3.5025 - acc: 0.5000\n",
      "Epoch 37/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 3.4953 - acc: 0.5000\n",
      "Epoch 38/500\n",
      "40/40 [==============================] - 0s 35us/step - loss: 3.4881 - acc: 0.5000\n",
      "Epoch 39/500\n",
      "40/40 [==============================] - 0s 24us/step - loss: 3.4809 - acc: 0.5000\n",
      "Epoch 40/500\n",
      "40/40 [==============================] - 0s 33us/step - loss: 3.4737 - acc: 0.5000\n",
      "Epoch 41/500\n",
      "40/40 [==============================] - 0s 29us/step - loss: 3.4665 - acc: 0.5000\n",
      "Epoch 42/500\n",
      "40/40 [==============================] - 0s 31us/step - loss: 3.4594 - acc: 0.5000\n",
      "Epoch 43/500\n",
      "40/40 [==============================] - 0s 24us/step - loss: 3.4522 - acc: 0.5000\n",
      "Epoch 44/500\n",
      "40/40 [==============================] - 0s 29us/step - loss: 3.4451 - acc: 0.5000\n",
      "Epoch 45/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 3.4379 - acc: 0.5000\n",
      "Epoch 46/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 3.4308 - acc: 0.5000\n",
      "Epoch 47/500\n",
      "40/40 [==============================] - 0s 32us/step - loss: 3.4236 - acc: 0.5000\n",
      "Epoch 48/500\n",
      "40/40 [==============================] - 0s 29us/step - loss: 3.4165 - acc: 0.5000\n",
      "Epoch 49/500\n",
      "40/40 [==============================] - 0s 27us/step - loss: 3.4094 - acc: 0.5000\n",
      "Epoch 50/500\n",
      "40/40 [==============================] - 0s 35us/step - loss: 3.4023 - acc: 0.5000\n",
      "Epoch 51/500\n",
      "40/40 [==============================] - 0s 30us/step - loss: 3.3951 - acc: 0.5000\n",
      "Epoch 52/500\n",
      "40/40 [==============================] - 0s 27us/step - loss: 3.3880 - acc: 0.5000\n",
      "Epoch 53/500\n",
      "40/40 [==============================] - 0s 32us/step - loss: 3.3809 - acc: 0.5000\n",
      "Epoch 54/500\n",
      "40/40 [==============================] - 0s 34us/step - loss: 3.3738 - acc: 0.5000\n",
      "Epoch 55/500\n",
      "40/40 [==============================] - 0s 30us/step - loss: 3.3667 - acc: 0.5000\n",
      "Epoch 56/500\n",
      "40/40 [==============================] - 0s 32us/step - loss: 3.3595 - acc: 0.5000\n",
      "Epoch 57/500\n",
      "40/40 [==============================] - 0s 30us/step - loss: 3.3524 - acc: 0.5000\n",
      "Epoch 58/500\n",
      "40/40 [==============================] - 0s 29us/step - loss: 3.3454 - acc: 0.5000\n",
      "Epoch 59/500\n",
      "40/40 [==============================] - 0s 27us/step - loss: 3.3383 - acc: 0.5000\n",
      "Epoch 60/500\n",
      "40/40 [==============================] - 0s 36us/step - loss: 3.3312 - acc: 0.5000\n",
      "Epoch 61/500\n",
      "40/40 [==============================] - 0s 32us/step - loss: 3.3242 - acc: 0.5000\n",
      "Epoch 62/500\n",
      "40/40 [==============================] - 0s 31us/step - loss: 3.3171 - acc: 0.5000\n",
      "Epoch 63/500\n",
      "40/40 [==============================] - 0s 27us/step - loss: 3.3101 - acc: 0.5000\n",
      "Epoch 64/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 3.3030 - acc: 0.5000\n",
      "Epoch 65/500\n",
      "40/40 [==============================] - 0s 36us/step - loss: 3.2960 - acc: 0.5000\n",
      "Epoch 66/500\n",
      "40/40 [==============================] - 0s 36us/step - loss: 3.2890 - acc: 0.5000\n",
      "Epoch 67/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 3.2820 - acc: 0.5000\n",
      "Epoch 68/500\n",
      "40/40 [==============================] - 0s 29us/step - loss: 3.2749 - acc: 0.5000\n",
      "Epoch 69/500\n",
      "40/40 [==============================] - 0s 33us/step - loss: 3.2679 - acc: 0.5000\n",
      "Epoch 70/500\n",
      "40/40 [==============================] - 0s 133us/step - loss: 3.2609 - acc: 0.5000\n",
      "Epoch 71/500\n",
      "40/40 [==============================] - 0s 49us/step - loss: 3.2539 - acc: 0.5000\n",
      "Epoch 72/500\n",
      "40/40 [==============================] - 0s 47us/step - loss: 3.2469 - acc: 0.5000\n",
      "Epoch 73/500\n",
      "40/40 [==============================] - 0s 35us/step - loss: 3.2399 - acc: 0.5000\n",
      "Epoch 74/500\n",
      "40/40 [==============================] - 0s 35us/step - loss: 3.2330 - acc: 0.5000\n",
      "Epoch 75/500\n",
      "40/40 [==============================] - 0s 33us/step - loss: 3.2260 - acc: 0.5000\n",
      "Epoch 76/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 3.2190 - acc: 0.5000\n",
      "Epoch 77/500\n",
      "40/40 [==============================] - 0s 30us/step - loss: 3.2121 - acc: 0.5000\n",
      "Epoch 78/500\n",
      "40/40 [==============================] - 0s 37us/step - loss: 3.2051 - acc: 0.5000\n",
      "Epoch 79/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 3.1982 - acc: 0.5000\n",
      "Epoch 80/500\n",
      "40/40 [==============================] - 0s 26us/step - loss: 3.1913 - acc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "40/40 [==============================] - 0s 26us/step - loss: 3.1844 - acc: 0.5000\n",
      "Epoch 82/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 3.1774 - acc: 0.5000\n",
      "Epoch 83/500\n",
      "40/40 [==============================] - 0s 31us/step - loss: 3.1705 - acc: 0.5000\n",
      "Epoch 84/500\n",
      "40/40 [==============================] - 0s 44us/step - loss: 3.1636 - acc: 0.5000\n",
      "Epoch 85/500\n",
      "40/40 [==============================] - 0s 34us/step - loss: 3.1567 - acc: 0.5000\n",
      "Epoch 86/500\n",
      "40/40 [==============================] - 0s 30us/step - loss: 3.1498 - acc: 0.5000\n",
      "Epoch 87/500\n",
      "40/40 [==============================] - 0s 34us/step - loss: 3.1429 - acc: 0.5000\n",
      "Epoch 88/500\n",
      "40/40 [==============================] - 0s 25us/step - loss: 3.1360 - acc: 0.5000\n",
      "Epoch 89/500\n",
      "40/40 [==============================] - 0s 30us/step - loss: 3.1292 - acc: 0.5000\n",
      "Epoch 90/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 3.1223 - acc: 0.5000\n",
      "Epoch 91/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 3.1155 - acc: 0.5000\n",
      "Epoch 92/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 3.1086 - acc: 0.5000\n",
      "Epoch 93/500\n",
      "40/40 [==============================] - 0s 39us/step - loss: 3.1018 - acc: 0.5000\n",
      "Epoch 94/500\n",
      "40/40 [==============================] - 0s 27us/step - loss: 3.0950 - acc: 0.5000\n",
      "Epoch 95/500\n",
      "40/40 [==============================] - 0s 29us/step - loss: 3.0882 - acc: 0.5000\n",
      "Epoch 96/500\n",
      "40/40 [==============================] - 0s 33us/step - loss: 3.0814 - acc: 0.5000\n",
      "Epoch 97/500\n",
      "40/40 [==============================] - 0s 31us/step - loss: 3.0745 - acc: 0.5000\n",
      "Epoch 98/500\n",
      "40/40 [==============================] - 0s 31us/step - loss: 3.0677 - acc: 0.5000\n",
      "Epoch 99/500\n",
      "40/40 [==============================] - 0s 41us/step - loss: 3.0610 - acc: 0.5000\n",
      "Epoch 100/500\n",
      "40/40 [==============================] - 0s 37us/step - loss: 3.0542 - acc: 0.5000\n",
      "Epoch 101/500\n",
      "40/40 [==============================] - 0s 38us/step - loss: 3.0474 - acc: 0.5000\n",
      "Epoch 102/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 3.0406 - acc: 0.5000\n",
      "Epoch 103/500\n",
      "40/40 [==============================] - 0s 33us/step - loss: 3.0339 - acc: 0.5000\n",
      "Epoch 104/500\n",
      "40/40 [==============================] - 0s 26us/step - loss: 3.0272 - acc: 0.5000\n",
      "Epoch 105/500\n",
      "40/40 [==============================] - 0s 32us/step - loss: 3.0205 - acc: 0.5000\n",
      "Epoch 106/500\n",
      "40/40 [==============================] - 0s 25us/step - loss: 3.0137 - acc: 0.5000\n",
      "Epoch 107/500\n",
      "40/40 [==============================] - 0s 35us/step - loss: 3.0070 - acc: 0.5000\n",
      "Epoch 108/500\n",
      "40/40 [==============================] - 0s 30us/step - loss: 3.0003 - acc: 0.5000\n",
      "Epoch 109/500\n",
      "40/40 [==============================] - 0s 37us/step - loss: 2.9936 - acc: 0.5000\n",
      "Epoch 110/500\n",
      "40/40 [==============================] - 0s 33us/step - loss: 2.9869 - acc: 0.5000\n",
      "Epoch 111/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 2.9802 - acc: 0.5000\n",
      "Epoch 112/500\n",
      "40/40 [==============================] - 0s 39us/step - loss: 2.9736 - acc: 0.5000\n",
      "Epoch 113/500\n",
      "40/40 [==============================] - 0s 27us/step - loss: 2.9669 - acc: 0.5000\n",
      "Epoch 114/500\n",
      "40/40 [==============================] - 0s 33us/step - loss: 2.9603 - acc: 0.5000\n",
      "Epoch 115/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: 2.9536 - acc: 0.5000\n",
      "Epoch 116/500\n",
      "40/40 [==============================] - 0s 40us/step - loss: 2.9470 - acc: 0.5000\n",
      "Epoch 117/500\n",
      "40/40 [==============================] - 0s 55us/step - loss: 2.9404 - acc: 0.5000\n",
      "Epoch 118/500\n",
      "40/40 [==============================] - 0s 39us/step - loss: 2.9338 - acc: 0.5000\n",
      "Epoch 119/500\n",
      "40/40 [==============================] - 0s 36us/step - loss: 2.9272 - acc: 0.5000\n",
      "Epoch 120/500\n",
      "40/40 [==============================] - 0s 46us/step - loss: 2.9206 - acc: 0.5000\n",
      "Epoch 121/500\n",
      "40/40 [==============================] - 0s 38us/step - loss: 2.9140 - acc: 0.5000\n",
      "Epoch 122/500\n",
      "40/40 [==============================] - 0s 48us/step - loss: 2.9075 - acc: 0.5000\n",
      "Epoch 123/500\n",
      "40/40 [==============================] - 0s 42us/step - loss: 2.9009 - acc: 0.5000\n",
      "Epoch 124/500\n",
      "40/40 [==============================] - 0s 42us/step - loss: 2.8944 - acc: 0.5000\n",
      "Epoch 125/500\n",
      "40/40 [==============================] - 0s 39us/step - loss: 2.8878 - acc: 0.5000\n",
      "Epoch 126/500\n",
      "40/40 [==============================] - 0s 47us/step - loss: 2.8813 - acc: 0.5000\n",
      "Epoch 127/500\n",
      "40/40 [==============================] - 0s 40us/step - loss: 2.8748 - acc: 0.5000\n",
      "Epoch 128/500\n",
      "40/40 [==============================] - 0s 32us/step - loss: 2.8683 - acc: 0.5000\n",
      "Epoch 129/500\n",
      "40/40 [==============================] - 0s 33us/step - loss: 2.8618 - acc: 0.5000\n",
      "Epoch 130/500\n",
      "40/40 [==============================] - 0s 48us/step - loss: 2.8553 - acc: 0.5000\n",
      "Epoch 131/500\n",
      "40/40 [==============================] - 0s 31us/step - loss: 2.8489 - acc: 0.5000\n",
      "Epoch 132/500\n",
      "40/40 [==============================] - 0s 39us/step - loss: 2.8424 - acc: 0.5000\n",
      "Epoch 133/500\n",
      "40/40 [==============================] - 0s 29us/step - loss: 2.8360 - acc: 0.5000\n",
      "Epoch 134/500\n",
      "40/40 [==============================] - 0s 30us/step - loss: 2.8296 - acc: 0.5000\n",
      "Epoch 135/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 2.8232 - acc: 0.5000\n",
      "Epoch 136/500\n",
      "40/40 [==============================] - 0s 27us/step - loss: 2.8168 - acc: 0.5000\n",
      "Epoch 137/500\n",
      "40/40 [==============================] - 0s 30us/step - loss: 2.8104 - acc: 0.5000\n",
      "Epoch 138/500\n",
      "40/40 [==============================] - 0s 55us/step - loss: 2.8040 - acc: 0.5000\n",
      "Epoch 139/500\n",
      "40/40 [==============================] - 0s 35us/step - loss: 2.7976 - acc: 0.5000\n",
      "Epoch 140/500\n",
      "40/40 [==============================] - 0s 37us/step - loss: 2.7913 - acc: 0.5000\n",
      "Epoch 141/500\n",
      "40/40 [==============================] - 0s 29us/step - loss: 2.7849 - acc: 0.5000\n",
      "Epoch 142/500\n",
      "40/40 [==============================] - 0s 47us/step - loss: 2.7786 - acc: 0.5000\n",
      "Epoch 143/500\n",
      "40/40 [==============================] - 0s 27us/step - loss: 2.7723 - acc: 0.5000\n",
      "Epoch 144/500\n",
      "40/40 [==============================] - 0s 27us/step - loss: 2.7660 - acc: 0.5000\n",
      "Epoch 145/500\n",
      "40/40 [==============================] - 0s 30us/step - loss: 2.7597 - acc: 0.5000\n",
      "Epoch 146/500\n",
      "40/40 [==============================] - 0s 30us/step - loss: 2.7534 - acc: 0.5000\n",
      "Epoch 147/500\n",
      "40/40 [==============================] - 0s 26us/step - loss: 2.7471 - acc: 0.5000\n",
      "Epoch 148/500\n",
      "40/40 [==============================] - 0s 43us/step - loss: 2.7409 - acc: 0.5000\n",
      "Epoch 149/500\n",
      "40/40 [==============================] - 0s 29us/step - loss: 2.7346 - acc: 0.5000\n",
      "Epoch 150/500\n",
      "40/40 [==============================] - 0s 45us/step - loss: 2.7284 - acc: 0.5000\n",
      "Epoch 151/500\n",
      "40/40 [==============================] - 0s 40us/step - loss: 2.7222 - acc: 0.5000\n",
      "Epoch 152/500\n",
      "40/40 [==============================] - 0s 33us/step - loss: 2.7159 - acc: 0.5000\n",
      "Epoch 153/500\n",
      "40/40 [==============================] - 0s 27us/step - loss: 2.7097 - acc: 0.5000\n",
      "Epoch 154/500\n",
      "40/40 [==============================] - 0s 41us/step - loss: 2.7035 - acc: 0.5000\n",
      "Epoch 155/500\n",
      "40/40 [==============================] - 0s 31us/step - loss: 2.6974 - acc: 0.5000\n",
      "Epoch 156/500\n",
      "40/40 [==============================] - 0s 33us/step - loss: 2.6912 - acc: 0.5000\n",
      "Epoch 157/500\n",
      "40/40 [==============================] - 0s 31us/step - loss: 2.6850 - acc: 0.5000\n",
      "Epoch 158/500\n",
      "40/40 [==============================] - 0s 56us/step - loss: 2.6789 - acc: 0.5000\n",
      "Epoch 159/500\n",
      "40/40 [==============================] - 0s 178us/step - loss: 2.6728 - acc: 0.5000\n",
      "Epoch 160/500\n",
      "40/40 [==============================] - 0s 32us/step - loss: 2.6667 - acc: 0.5000\n",
      "Epoch 161/500\n",
      "40/40 [==============================] - 0s 43us/step - loss: 2.6606 - acc: 0.5000\n",
      "Epoch 162/500\n",
      "40/40 [==============================] - 0s 31us/step - loss: 2.6545 - acc: 0.5000\n",
      "Epoch 163/500\n",
      "40/40 [==============================] - 0s 44us/step - loss: 2.6484 - acc: 0.5000\n",
      "Epoch 164/500\n",
      "40/40 [==============================] - 0s 41us/step - loss: 2.6424 - acc: 0.5000\n",
      "Epoch 165/500\n",
      "40/40 [==============================] - 0s 37us/step - loss: 2.6364 - acc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166/500\n",
      "40/40 [==============================] - 0s 43us/step - loss: 2.6303 - acc: 0.5000\n",
      "Epoch 167/500\n",
      "40/40 [==============================] - 0s 31us/step - loss: 2.6243 - acc: 0.5000\n",
      "Epoch 168/500\n",
      "40/40 [==============================] - 0s 40us/step - loss: 2.6183 - acc: 0.5000\n",
      "Epoch 169/500\n",
      "40/40 [==============================] - 0s 67us/step - loss: 2.6123 - acc: 0.5000\n",
      "Epoch 170/500\n",
      "40/40 [==============================] - 0s 53us/step - loss: 2.6064 - acc: 0.5000\n",
      "Epoch 171/500\n",
      "40/40 [==============================] - 0s 29us/step - loss: 2.6004 - acc: 0.5000\n",
      "Epoch 172/500\n",
      "40/40 [==============================] - 0s 33us/step - loss: 2.5945 - acc: 0.5000\n",
      "Epoch 173/500\n",
      "40/40 [==============================] - 0s 30us/step - loss: 2.5885 - acc: 0.5000\n",
      "Epoch 174/500\n",
      "40/40 [==============================] - 0s 26us/step - loss: 2.5826 - acc: 0.5000\n",
      "Epoch 175/500\n",
      "40/40 [==============================] - 0s 26us/step - loss: 2.5767 - acc: 0.5000\n",
      "Epoch 176/500\n",
      "40/40 [==============================] - 0s 32us/step - loss: 2.5709 - acc: 0.5000\n",
      "Epoch 177/500\n",
      "40/40 [==============================] - 0s 40us/step - loss: 2.5650 - acc: 0.5000\n",
      "Epoch 178/500\n",
      "40/40 [==============================] - 0s 43us/step - loss: 2.5591 - acc: 0.5000\n",
      "Epoch 179/500\n",
      "40/40 [==============================] - 0s 32us/step - loss: 2.5533 - acc: 0.5000\n",
      "Epoch 180/500\n",
      "40/40 [==============================] - 0s 25us/step - loss: 2.5475 - acc: 0.5000\n",
      "Epoch 181/500\n",
      "40/40 [==============================] - 0s 35us/step - loss: 2.5417 - acc: 0.5000\n",
      "Epoch 182/500\n",
      "40/40 [==============================] - 0s 35us/step - loss: 2.5359 - acc: 0.5000\n",
      "Epoch 183/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 2.5301 - acc: 0.5000\n",
      "Epoch 184/500\n",
      "40/40 [==============================] - 0s 40us/step - loss: 2.5243 - acc: 0.5000\n",
      "Epoch 185/500\n",
      "40/40 [==============================] - 0s 33us/step - loss: 2.5186 - acc: 0.5000\n",
      "Epoch 186/500\n",
      "40/40 [==============================] - 0s 26us/step - loss: 2.5129 - acc: 0.5000\n",
      "Epoch 187/500\n",
      "40/40 [==============================] - 0s 30us/step - loss: 2.5071 - acc: 0.5000\n",
      "Epoch 188/500\n",
      "40/40 [==============================] - 0s 23us/step - loss: 2.5014 - acc: 0.5000\n",
      "Epoch 189/500\n",
      "40/40 [==============================] - 0s 47us/step - loss: 2.4957 - acc: 0.5000\n",
      "Epoch 190/500\n",
      "40/40 [==============================] - 0s 31us/step - loss: 2.4901 - acc: 0.5000\n",
      "Epoch 191/500\n",
      "40/40 [==============================] - 0s 37us/step - loss: 2.4844 - acc: 0.5000\n",
      "Epoch 192/500\n",
      "40/40 [==============================] - 0s 35us/step - loss: 2.4788 - acc: 0.5000\n",
      "Epoch 193/500\n",
      "40/40 [==============================] - 0s 43us/step - loss: 2.4731 - acc: 0.5000\n",
      "Epoch 194/500\n",
      "40/40 [==============================] - 0s 34us/step - loss: 2.4675 - acc: 0.5000\n",
      "Epoch 195/500\n",
      "40/40 [==============================] - 0s 27us/step - loss: 2.4619 - acc: 0.5000\n",
      "Epoch 196/500\n",
      "40/40 [==============================] - 0s 27us/step - loss: 2.4564 - acc: 0.5000\n",
      "Epoch 197/500\n",
      "40/40 [==============================] - 0s 63us/step - loss: 2.4508 - acc: 0.5000\n",
      "Epoch 198/500\n",
      "40/40 [==============================] - 0s 84us/step - loss: 2.4453 - acc: 0.5000\n",
      "Epoch 199/500\n",
      "40/40 [==============================] - 0s 62us/step - loss: 2.4397 - acc: 0.5000\n",
      "Epoch 200/500\n",
      "40/40 [==============================] - 0s 88us/step - loss: 2.4342 - acc: 0.5000\n",
      "Epoch 201/500\n",
      "40/40 [==============================] - 0s 44us/step - loss: 2.4287 - acc: 0.5000\n",
      "Epoch 202/500\n",
      "40/40 [==============================] - 0s 35us/step - loss: 2.4232 - acc: 0.5000\n",
      "Epoch 203/500\n",
      "40/40 [==============================] - 0s 33us/step - loss: 2.4177 - acc: 0.5000\n",
      "Epoch 204/500\n",
      "40/40 [==============================] - 0s 29us/step - loss: 2.4123 - acc: 0.5000\n",
      "Epoch 205/500\n",
      "40/40 [==============================] - 0s 32us/step - loss: 2.4069 - acc: 0.5000\n",
      "Epoch 206/500\n",
      "40/40 [==============================] - 0s 35us/step - loss: 2.4014 - acc: 0.5000\n",
      "Epoch 207/500\n",
      "40/40 [==============================] - 0s 39us/step - loss: 2.3960 - acc: 0.5000\n",
      "Epoch 208/500\n",
      "40/40 [==============================] - 0s 43us/step - loss: 2.3906 - acc: 0.5000\n",
      "Epoch 209/500\n",
      "40/40 [==============================] - 0s 44us/step - loss: 2.3853 - acc: 0.5000\n",
      "Epoch 210/500\n",
      "40/40 [==============================] - 0s 27us/step - loss: 2.3799 - acc: 0.5000\n",
      "Epoch 211/500\n",
      "40/40 [==============================] - 0s 38us/step - loss: 2.3746 - acc: 0.5000\n",
      "Epoch 212/500\n",
      "40/40 [==============================] - 0s 38us/step - loss: 2.3693 - acc: 0.5000\n",
      "Epoch 213/500\n",
      "40/40 [==============================] - 0s 36us/step - loss: 2.3640 - acc: 0.5000\n",
      "Epoch 214/500\n",
      "40/40 [==============================] - 0s 65us/step - loss: 2.3587 - acc: 0.5000\n",
      "Epoch 215/500\n",
      "40/40 [==============================] - 0s 69us/step - loss: 2.3534 - acc: 0.5000\n",
      "Epoch 216/500\n",
      "40/40 [==============================] - 0s 39us/step - loss: 2.3481 - acc: 0.5000\n",
      "Epoch 217/500\n",
      "40/40 [==============================] - 0s 29us/step - loss: 2.3429 - acc: 0.5000\n",
      "Epoch 218/500\n",
      "40/40 [==============================] - 0s 41us/step - loss: 2.3376 - acc: 0.5000\n",
      "Epoch 219/500\n",
      "40/40 [==============================] - 0s 32us/step - loss: 2.3324 - acc: 0.5000\n",
      "Epoch 220/500\n",
      "40/40 [==============================] - 0s 34us/step - loss: 2.3273 - acc: 0.5000\n",
      "Epoch 221/500\n",
      "40/40 [==============================] - 0s 56us/step - loss: 2.3221 - acc: 0.5000\n",
      "Epoch 222/500\n",
      "40/40 [==============================] - 0s 40us/step - loss: 2.3169 - acc: 0.5000\n",
      "Epoch 223/500\n",
      "40/40 [==============================] - 0s 29us/step - loss: 2.3118 - acc: 0.5000\n",
      "Epoch 224/500\n",
      "40/40 [==============================] - 0s 24us/step - loss: 2.3067 - acc: 0.5000\n",
      "Epoch 225/500\n",
      "40/40 [==============================] - 0s 58us/step - loss: 2.3015 - acc: 0.5000\n",
      "Epoch 226/500\n",
      "40/40 [==============================] - 0s 90us/step - loss: 2.2965 - acc: 0.5000\n",
      "Epoch 227/500\n",
      "40/40 [==============================] - 0s 36us/step - loss: 2.2914 - acc: 0.5000\n",
      "Epoch 228/500\n",
      "40/40 [==============================] - 0s 57us/step - loss: 2.2863 - acc: 0.5000\n",
      "Epoch 229/500\n",
      "40/40 [==============================] - 0s 35us/step - loss: 2.2813 - acc: 0.5000\n",
      "Epoch 230/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 2.2763 - acc: 0.5000\n",
      "Epoch 231/500\n",
      "40/40 [==============================] - 0s 27us/step - loss: 2.2713 - acc: 0.5000\n",
      "Epoch 232/500\n",
      "40/40 [==============================] - 0s 30us/step - loss: 2.2663 - acc: 0.5000\n",
      "Epoch 233/500\n",
      "40/40 [==============================] - 0s 85us/step - loss: 2.2613 - acc: 0.5000\n",
      "Epoch 234/500\n",
      "40/40 [==============================] - 0s 24us/step - loss: 2.2564 - acc: 0.5000\n",
      "Epoch 235/500\n",
      "40/40 [==============================] - 0s 43us/step - loss: 2.2514 - acc: 0.5000\n",
      "Epoch 236/500\n",
      "40/40 [==============================] - 0s 33us/step - loss: 2.2465 - acc: 0.5000\n",
      "Epoch 237/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 2.2416 - acc: 0.5000\n",
      "Epoch 238/500\n",
      "40/40 [==============================] - 0s 54us/step - loss: 2.2367 - acc: 0.5000\n",
      "Epoch 239/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 2.2318 - acc: 0.5000\n",
      "Epoch 240/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 2.2270 - acc: 0.5000\n",
      "Epoch 241/500\n",
      "40/40 [==============================] - 0s 39us/step - loss: 2.2222 - acc: 0.5000\n",
      "Epoch 242/500\n",
      "40/40 [==============================] - 0s 75us/step - loss: 2.2173 - acc: 0.5000\n",
      "Epoch 243/500\n",
      "40/40 [==============================] - 0s 53us/step - loss: 2.2126 - acc: 0.5000\n",
      "Epoch 244/500\n",
      "40/40 [==============================] - 0s 33us/step - loss: 2.2078 - acc: 0.5000\n",
      "Epoch 245/500\n",
      "40/40 [==============================] - 0s 27us/step - loss: 2.2030 - acc: 0.5000\n",
      "Epoch 246/500\n",
      "40/40 [==============================] - 0s 26us/step - loss: 2.1983 - acc: 0.5000\n",
      "Epoch 247/500\n",
      "40/40 [==============================] - 0s 37us/step - loss: 2.1935 - acc: 0.5000\n",
      "Epoch 248/500\n",
      "40/40 [==============================] - 0s 44us/step - loss: 2.1888 - acc: 0.5000\n",
      "Epoch 249/500\n",
      "40/40 [==============================] - 0s 50us/step - loss: 2.1841 - acc: 0.5000\n",
      "Epoch 250/500\n",
      "40/40 [==============================] - 0s 35us/step - loss: 2.1794 - acc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 251/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 2.1748 - acc: 0.5000\n",
      "Epoch 252/500\n",
      "40/40 [==============================] - 0s 31us/step - loss: 2.1701 - acc: 0.5000\n",
      "Epoch 253/500\n",
      "40/40 [==============================] - 0s 56us/step - loss: 2.1655 - acc: 0.5000\n",
      "Epoch 254/500\n",
      "40/40 [==============================] - 0s 46us/step - loss: 2.1609 - acc: 0.5000\n",
      "Epoch 255/500\n",
      "40/40 [==============================] - 0s 31us/step - loss: 2.1563 - acc: 0.5000\n",
      "Epoch 256/500\n",
      "40/40 [==============================] - 0s 49us/step - loss: 2.1517 - acc: 0.5000\n",
      "Epoch 257/500\n",
      "40/40 [==============================] - 0s 58us/step - loss: 2.1472 - acc: 0.5000\n",
      "Epoch 258/500\n",
      "40/40 [==============================] - 0s 53us/step - loss: 2.1426 - acc: 0.5000\n",
      "Epoch 259/500\n",
      "40/40 [==============================] - 0s 36us/step - loss: 2.1381 - acc: 0.5000\n",
      "Epoch 260/500\n",
      "40/40 [==============================] - 0s 36us/step - loss: 2.1336 - acc: 0.5000\n",
      "Epoch 261/500\n",
      "40/40 [==============================] - 0s 30us/step - loss: 2.1291 - acc: 0.5000\n",
      "Epoch 262/500\n",
      "40/40 [==============================] - 0s 54us/step - loss: 2.1246 - acc: 0.5000\n",
      "Epoch 263/500\n",
      "40/40 [==============================] - 0s 77us/step - loss: 2.1202 - acc: 0.5000\n",
      "Epoch 264/500\n",
      "40/40 [==============================] - 0s 29us/step - loss: 2.1158 - acc: 0.5000\n",
      "Epoch 265/500\n",
      "40/40 [==============================] - 0s 40us/step - loss: 2.1113 - acc: 0.4750\n",
      "Epoch 266/500\n",
      "40/40 [==============================] - 0s 25us/step - loss: 2.1069 - acc: 0.4750\n",
      "Epoch 267/500\n",
      "40/40 [==============================] - 0s 43us/step - loss: 2.1026 - acc: 0.4750\n",
      "Epoch 268/500\n",
      "40/40 [==============================] - 0s 47us/step - loss: 2.0982 - acc: 0.4750\n",
      "Epoch 269/500\n",
      "40/40 [==============================] - 0s 33us/step - loss: 2.0938 - acc: 0.4750\n",
      "Epoch 270/500\n",
      "40/40 [==============================] - 0s 29us/step - loss: 2.0895 - acc: 0.4750\n",
      "Epoch 271/500\n",
      "40/40 [==============================] - 0s 34us/step - loss: 2.0852 - acc: 0.4750\n",
      "Epoch 272/500\n",
      "40/40 [==============================] - 0s 29us/step - loss: 2.0809 - acc: 0.4750\n",
      "Epoch 273/500\n",
      "40/40 [==============================] - 0s 27us/step - loss: 2.0766 - acc: 0.4750\n",
      "Epoch 274/500\n",
      "40/40 [==============================] - 0s 29us/step - loss: 2.0724 - acc: 0.4750\n",
      "Epoch 275/500\n",
      "40/40 [==============================] - 0s 30us/step - loss: 2.0681 - acc: 0.4750\n",
      "Epoch 276/500\n",
      "40/40 [==============================] - 0s 30us/step - loss: 2.0639 - acc: 0.4750\n",
      "Epoch 277/500\n",
      "40/40 [==============================] - 0s 30us/step - loss: 2.0597 - acc: 0.4750\n",
      "Epoch 278/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 2.0555 - acc: 0.4750\n",
      "Epoch 279/500\n",
      "40/40 [==============================] - 0s 33us/step - loss: 2.0513 - acc: 0.4750\n",
      "Epoch 280/500\n",
      "40/40 [==============================] - 0s 21us/step - loss: 2.0471 - acc: 0.4750\n",
      "Epoch 281/500\n",
      "40/40 [==============================] - 0s 25us/step - loss: 2.0430 - acc: 0.4750\n",
      "Epoch 282/500\n",
      "40/40 [==============================] - 0s 31us/step - loss: 2.0389 - acc: 0.4750\n",
      "Epoch 283/500\n",
      "40/40 [==============================] - 0s 21us/step - loss: 2.0348 - acc: 0.4750\n",
      "Epoch 284/500\n",
      "40/40 [==============================] - 0s 24us/step - loss: 2.0307 - acc: 0.4750\n",
      "Epoch 285/500\n",
      "40/40 [==============================] - 0s 25us/step - loss: 2.0266 - acc: 0.4750\n",
      "Epoch 286/500\n",
      "40/40 [==============================] - 0s 24us/step - loss: 2.0225 - acc: 0.4750\n",
      "Epoch 287/500\n",
      "40/40 [==============================] - 0s 23us/step - loss: 2.0185 - acc: 0.4750\n",
      "Epoch 288/500\n",
      "40/40 [==============================] - 0s 24us/step - loss: 2.0145 - acc: 0.4500\n",
      "Epoch 289/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 2.0105 - acc: 0.4500\n",
      "Epoch 290/500\n",
      "40/40 [==============================] - 0s 26us/step - loss: 2.0065 - acc: 0.4500\n",
      "Epoch 291/500\n",
      "40/40 [==============================] - 0s 47us/step - loss: 2.0025 - acc: 0.4500\n",
      "Epoch 292/500\n",
      "40/40 [==============================] - 0s 24us/step - loss: 1.9986 - acc: 0.4500\n",
      "Epoch 293/500\n",
      "40/40 [==============================] - 0s 22us/step - loss: 1.9946 - acc: 0.4500\n",
      "Epoch 294/500\n",
      "40/40 [==============================] - 0s 24us/step - loss: 1.9907 - acc: 0.4500\n",
      "Epoch 295/500\n",
      "40/40 [==============================] - 0s 36us/step - loss: 1.9868 - acc: 0.4500\n",
      "Epoch 296/500\n",
      "40/40 [==============================] - 0s 22us/step - loss: 1.9829 - acc: 0.4250\n",
      "Epoch 297/500\n",
      "40/40 [==============================] - 0s 24us/step - loss: 1.9790 - acc: 0.4250\n",
      "Epoch 298/500\n",
      "40/40 [==============================] - 0s 25us/step - loss: 1.9752 - acc: 0.4250\n",
      "Epoch 299/500\n",
      "40/40 [==============================] - 0s 22us/step - loss: 1.9713 - acc: 0.4250\n",
      "Epoch 300/500\n",
      "40/40 [==============================] - 0s 26us/step - loss: 1.9675 - acc: 0.4250\n",
      "Epoch 301/500\n",
      "40/40 [==============================] - 0s 22us/step - loss: 1.9637 - acc: 0.4250\n",
      "Epoch 302/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 1.9599 - acc: 0.4250\n",
      "Epoch 303/500\n",
      "40/40 [==============================] - 0s 21us/step - loss: 1.9561 - acc: 0.4250\n",
      "Epoch 304/500\n",
      "40/40 [==============================] - 0s 23us/step - loss: 1.9524 - acc: 0.4250\n",
      "Epoch 305/500\n",
      "40/40 [==============================] - 0s 25us/step - loss: 1.9486 - acc: 0.4250\n",
      "Epoch 306/500\n",
      "40/40 [==============================] - 0s 34us/step - loss: 1.9449 - acc: 0.4250\n",
      "Epoch 307/500\n",
      "40/40 [==============================] - 0s 37us/step - loss: 1.9412 - acc: 0.4250\n",
      "Epoch 308/500\n",
      "40/40 [==============================] - 0s 30us/step - loss: 1.9375 - acc: 0.4250\n",
      "Epoch 309/500\n",
      "40/40 [==============================] - 0s 40us/step - loss: 1.9338 - acc: 0.4250\n",
      "Epoch 310/500\n",
      "40/40 [==============================] - 0s 25us/step - loss: 1.9302 - acc: 0.4250\n",
      "Epoch 311/500\n",
      "40/40 [==============================] - 0s 38us/step - loss: 1.9265 - acc: 0.4250\n",
      "Epoch 312/500\n",
      "40/40 [==============================] - 0s 29us/step - loss: 1.9229 - acc: 0.4000\n",
      "Epoch 313/500\n",
      "40/40 [==============================] - 0s 24us/step - loss: 1.9193 - acc: 0.4000\n",
      "Epoch 314/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 1.9157 - acc: 0.4000\n",
      "Epoch 315/500\n",
      "40/40 [==============================] - 0s 32us/step - loss: 1.9121 - acc: 0.4000\n",
      "Epoch 316/500\n",
      "40/40 [==============================] - 0s 23us/step - loss: 1.9086 - acc: 0.4000\n",
      "Epoch 317/500\n",
      "40/40 [==============================] - 0s 21us/step - loss: 1.9050 - acc: 0.4000\n",
      "Epoch 318/500\n",
      "40/40 [==============================] - 0s 32us/step - loss: 1.9015 - acc: 0.4000\n",
      "Epoch 319/500\n",
      "40/40 [==============================] - 0s 24us/step - loss: 1.8980 - acc: 0.4000\n",
      "Epoch 320/500\n",
      "40/40 [==============================] - 0s 37us/step - loss: 1.8945 - acc: 0.4000\n",
      "Epoch 321/500\n",
      "40/40 [==============================] - 0s 30us/step - loss: 1.8910 - acc: 0.4000\n",
      "Epoch 322/500\n",
      "40/40 [==============================] - 0s 43us/step - loss: 1.8875 - acc: 0.4000\n",
      "Epoch 323/500\n",
      "40/40 [==============================] - 0s 23us/step - loss: 1.8841 - acc: 0.3750\n",
      "Epoch 324/500\n",
      "40/40 [==============================] - 0s 22us/step - loss: 1.8806 - acc: 0.3750\n",
      "Epoch 325/500\n",
      "40/40 [==============================] - 0s 31us/step - loss: 1.8772 - acc: 0.3500\n",
      "Epoch 326/500\n",
      "40/40 [==============================] - 0s 25us/step - loss: 1.8738 - acc: 0.3250\n",
      "Epoch 327/500\n",
      "40/40 [==============================] - 0s 37us/step - loss: 1.8704 - acc: 0.3250\n",
      "Epoch 328/500\n",
      "40/40 [==============================] - 0s 46us/step - loss: 1.8670 - acc: 0.3000\n",
      "Epoch 329/500\n",
      "40/40 [==============================] - 0s 37us/step - loss: 1.8637 - acc: 0.3000\n",
      "Epoch 330/500\n",
      "40/40 [==============================] - 0s 36us/step - loss: 1.8603 - acc: 0.2750\n",
      "Epoch 331/500\n",
      "40/40 [==============================] - 0s 34us/step - loss: 1.8570 - acc: 0.2750\n",
      "Epoch 332/500\n",
      "40/40 [==============================] - 0s 27us/step - loss: 1.8537 - acc: 0.2750\n",
      "Epoch 333/500\n",
      "40/40 [==============================] - 0s 40us/step - loss: 1.8504 - acc: 0.2750\n",
      "Epoch 334/500\n",
      "40/40 [==============================] - 0s 57us/step - loss: 1.8471 - acc: 0.2750\n",
      "Epoch 335/500\n",
      "40/40 [==============================] - 0s 32us/step - loss: 1.8438 - acc: 0.2250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 336/500\n",
      "40/40 [==============================] - 0s 35us/step - loss: 1.8406 - acc: 0.2000\n",
      "Epoch 337/500\n",
      "40/40 [==============================] - 0s 41us/step - loss: 1.8373 - acc: 0.1750\n",
      "Epoch 338/500\n",
      "40/40 [==============================] - 0s 29us/step - loss: 1.8341 - acc: 0.1750\n",
      "Epoch 339/500\n",
      "40/40 [==============================] - 0s 31us/step - loss: 1.8309 - acc: 0.1750\n",
      "Epoch 340/500\n",
      "40/40 [==============================] - 0s 34us/step - loss: 1.8277 - acc: 0.1750\n",
      "Epoch 341/500\n",
      "40/40 [==============================] - 0s 35us/step - loss: 1.8245 - acc: 0.1750\n",
      "Epoch 342/500\n",
      "40/40 [==============================] - 0s 50us/step - loss: 1.8214 - acc: 0.1750\n",
      "Epoch 343/500\n",
      "40/40 [==============================] - 0s 31us/step - loss: 1.8182 - acc: 0.1750\n",
      "Epoch 344/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 1.8151 - acc: 0.1750\n",
      "Epoch 345/500\n",
      "40/40 [==============================] - 0s 41us/step - loss: 1.8120 - acc: 0.1750\n",
      "Epoch 346/500\n",
      "40/40 [==============================] - 0s 42us/step - loss: 1.8089 - acc: 0.1750\n",
      "Epoch 347/500\n",
      "40/40 [==============================] - 0s 43us/step - loss: 1.8058 - acc: 0.1750\n",
      "Epoch 348/500\n",
      "40/40 [==============================] - 0s 31us/step - loss: 1.8027 - acc: 0.1750\n",
      "Epoch 349/500\n",
      "40/40 [==============================] - 0s 44us/step - loss: 1.7996 - acc: 0.1500\n",
      "Epoch 350/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 1.7966 - acc: 0.1500\n",
      "Epoch 351/500\n",
      "40/40 [==============================] - 0s 55us/step - loss: 1.7936 - acc: 0.1250\n",
      "Epoch 352/500\n",
      "40/40 [==============================] - 0s 49us/step - loss: 1.7905 - acc: 0.1250\n",
      "Epoch 353/500\n",
      "40/40 [==============================] - 0s 26us/step - loss: 1.7875 - acc: 0.1000\n",
      "Epoch 354/500\n",
      "40/40 [==============================] - 0s 34us/step - loss: 1.7845 - acc: 0.1000\n",
      "Epoch 355/500\n",
      "40/40 [==============================] - 0s 27us/step - loss: 1.7816 - acc: 0.1000\n",
      "Epoch 356/500\n",
      "40/40 [==============================] - 0s 39us/step - loss: 1.7786 - acc: 0.1000\n",
      "Epoch 357/500\n",
      "40/40 [==============================] - 0s 75us/step - loss: 1.7757 - acc: 0.1000\n",
      "Epoch 358/500\n",
      "40/40 [==============================] - 0s 49us/step - loss: 1.7727 - acc: 0.1000\n",
      "Epoch 359/500\n",
      "40/40 [==============================] - 0s 26us/step - loss: 1.7698 - acc: 0.1000\n",
      "Epoch 360/500\n",
      "40/40 [==============================] - 0s 29us/step - loss: 1.7669 - acc: 0.1000\n",
      "Epoch 361/500\n",
      "40/40 [==============================] - 0s 32us/step - loss: 1.7640 - acc: 0.1000\n",
      "Epoch 362/500\n",
      "40/40 [==============================] - 0s 33us/step - loss: 1.7611 - acc: 0.1000\n",
      "Epoch 363/500\n",
      "40/40 [==============================] - 0s 23us/step - loss: 1.7583 - acc: 0.0750\n",
      "Epoch 364/500\n",
      "40/40 [==============================] - 0s 47us/step - loss: 1.7554 - acc: 0.0750\n",
      "Epoch 365/500\n",
      "40/40 [==============================] - 0s 39us/step - loss: 1.7526 - acc: 0.0750\n",
      "Epoch 366/500\n",
      "40/40 [==============================] - 0s 34us/step - loss: 1.7497 - acc: 0.0500\n",
      "Epoch 367/500\n",
      "40/40 [==============================] - 0s 29us/step - loss: 1.7469 - acc: 0.0500\n",
      "Epoch 368/500\n",
      "40/40 [==============================] - 0s 49us/step - loss: 1.7441 - acc: 0.0500\n",
      "Epoch 369/500\n",
      "40/40 [==============================] - 0s 59us/step - loss: 1.7413 - acc: 0.0250\n",
      "Epoch 370/500\n",
      "40/40 [==============================] - 0s 65us/step - loss: 1.7386 - acc: 0.0250\n",
      "Epoch 371/500\n",
      "40/40 [==============================] - 0s 45us/step - loss: 1.7358 - acc: 0.0250\n",
      "Epoch 372/500\n",
      "40/40 [==============================] - 0s 46us/step - loss: 1.7331 - acc: 0.0250\n",
      "Epoch 373/500\n",
      "40/40 [==============================] - 0s 23us/step - loss: 1.7303 - acc: 0.0250\n",
      "Epoch 374/500\n",
      "40/40 [==============================] - 0s 40us/step - loss: 1.7276 - acc: 0.0250\n",
      "Epoch 375/500\n",
      "40/40 [==============================] - 0s 34us/step - loss: 1.7249 - acc: 0.0250\n",
      "Epoch 376/500\n",
      "40/40 [==============================] - 0s 24us/step - loss: 1.7222 - acc: 0.0250\n",
      "Epoch 377/500\n",
      "40/40 [==============================] - 0s 38us/step - loss: 1.7195 - acc: 0.0250\n",
      "Epoch 378/500\n",
      "40/40 [==============================] - 0s 41us/step - loss: 1.7169 - acc: 0.0250\n",
      "Epoch 379/500\n",
      "40/40 [==============================] - 0s 37us/step - loss: 1.7142 - acc: 0.0250\n",
      "Epoch 380/500\n",
      "40/40 [==============================] - 0s 38us/step - loss: 1.7116 - acc: 0.0250\n",
      "Epoch 381/500\n",
      "40/40 [==============================] - 0s 37us/step - loss: 1.7090 - acc: 0.0250\n",
      "Epoch 382/500\n",
      "40/40 [==============================] - 0s 27us/step - loss: 1.7063 - acc: 0.0250\n",
      "Epoch 383/500\n",
      "40/40 [==============================] - 0s 39us/step - loss: 1.7037 - acc: 0.0250\n",
      "Epoch 384/500\n",
      "40/40 [==============================] - 0s 24us/step - loss: 1.7011 - acc: 0.0250\n",
      "Epoch 385/500\n",
      "40/40 [==============================] - 0s 38us/step - loss: 1.6986 - acc: 0.0250\n",
      "Epoch 386/500\n",
      "40/40 [==============================] - 0s 29us/step - loss: 1.6960 - acc: 0.0250\n",
      "Epoch 387/500\n",
      "40/40 [==============================] - 0s 30us/step - loss: 1.6934 - acc: 0.0250\n",
      "Epoch 388/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 1.6909 - acc: 0.0250\n",
      "Epoch 389/500\n",
      "40/40 [==============================] - 0s 49us/step - loss: 1.6884 - acc: 0.0250\n",
      "Epoch 390/500\n",
      "40/40 [==============================] - 0s 32us/step - loss: 1.6859 - acc: 0.0250\n",
      "Epoch 391/500\n",
      "40/40 [==============================] - 0s 30us/step - loss: 1.6833 - acc: 0.0250\n",
      "Epoch 392/500\n",
      "40/40 [==============================] - 0s 33us/step - loss: 1.6809 - acc: 0.0250\n",
      "Epoch 393/500\n",
      "40/40 [==============================] - 0s 46us/step - loss: 1.6784 - acc: 0.0250\n",
      "Epoch 394/500\n",
      "40/40 [==============================] - 0s 50us/step - loss: 1.6759 - acc: 0.0250\n",
      "Epoch 395/500\n",
      "40/40 [==============================] - 0s 63us/step - loss: 1.6734 - acc: 0.0250\n",
      "Epoch 396/500\n",
      "40/40 [==============================] - 0s 56us/step - loss: 1.6710 - acc: 0.0250\n",
      "Epoch 397/500\n",
      "40/40 [==============================] - 0s 65us/step - loss: 1.6686 - acc: 0.0250\n",
      "Epoch 398/500\n",
      "40/40 [==============================] - 0s 85us/step - loss: 1.6661 - acc: 0.0250\n",
      "Epoch 399/500\n",
      "40/40 [==============================] - 0s 35us/step - loss: 1.6637 - acc: 0.0250\n",
      "Epoch 400/500\n",
      "40/40 [==============================] - 0s 34us/step - loss: 1.6613 - acc: 0.0250\n",
      "Epoch 401/500\n",
      "40/40 [==============================] - 0s 38us/step - loss: 1.6589 - acc: 0.0250\n",
      "Epoch 402/500\n",
      "40/40 [==============================] - 0s 62us/step - loss: 1.6566 - acc: 0.0250\n",
      "Epoch 403/500\n",
      "40/40 [==============================] - 0s 38us/step - loss: 1.6542 - acc: 0.0250\n",
      "Epoch 404/500\n",
      "40/40 [==============================] - 0s 46us/step - loss: 1.6519 - acc: 0.0250\n",
      "Epoch 405/500\n",
      "40/40 [==============================] - 0s 34us/step - loss: 1.6495 - acc: 0.0250\n",
      "Epoch 406/500\n",
      "40/40 [==============================] - 0s 29us/step - loss: 1.6472 - acc: 0.0250\n",
      "Epoch 407/500\n",
      "40/40 [==============================] - 0s 30us/step - loss: 1.6449 - acc: 0.0250\n",
      "Epoch 408/500\n",
      "40/40 [==============================] - 0s 47us/step - loss: 1.6426 - acc: 0.0250\n",
      "Epoch 409/500\n",
      "40/40 [==============================] - 0s 37us/step - loss: 1.6403 - acc: 0.0250\n",
      "Epoch 410/500\n",
      "40/40 [==============================] - 0s 35us/step - loss: 1.6380 - acc: 0.0250\n",
      "Epoch 411/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 1.6357 - acc: 0.0250\n",
      "Epoch 412/500\n",
      "40/40 [==============================] - 0s 30us/step - loss: 1.6334 - acc: 0.0250\n",
      "Epoch 413/500\n",
      "40/40 [==============================] - 0s 35us/step - loss: 1.6312 - acc: 0.0250\n",
      "Epoch 414/500\n",
      "40/40 [==============================] - 0s 36us/step - loss: 1.6289 - acc: 0.0250\n",
      "Epoch 415/500\n",
      "40/40 [==============================] - 0s 28us/step - loss: 1.6267 - acc: 0.0250\n",
      "Epoch 416/500\n",
      "40/40 [==============================] - 0s 47us/step - loss: 1.6245 - acc: 0.0250\n",
      "Epoch 417/500\n",
      "40/40 [==============================] - 0s 50us/step - loss: 1.6223 - acc: 0.0250\n",
      "Epoch 418/500\n",
      "40/40 [==============================] - 0s 44us/step - loss: 1.6201 - acc: 0.0250\n",
      "Epoch 419/500\n",
      "40/40 [==============================] - 0s 41us/step - loss: 1.6179 - acc: 0.0250\n",
      "Epoch 420/500\n",
      "40/40 [==============================] - 0s 38us/step - loss: 1.6157 - acc: 0.0250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 421/500\n",
      "40/40 [==============================] - 0s 41us/step - loss: 1.6136 - acc: 0.0250\n",
      "Epoch 422/500\n",
      "40/40 [==============================] - 0s 24us/step - loss: 1.6114 - acc: 0.0250\n",
      "Epoch 423/500\n",
      "40/40 [==============================] - 0s 26us/step - loss: 1.6092 - acc: 0.0250\n",
      "Epoch 424/500\n",
      "40/40 [==============================] - 0s 22us/step - loss: 1.6071 - acc: 0.0250\n",
      "Epoch 425/500\n",
      "40/40 [==============================] - 0s 31us/step - loss: 1.6050 - acc: 0.0250\n",
      "Epoch 426/500\n",
      "40/40 [==============================] - 0s 24us/step - loss: 1.6029 - acc: 0.0250\n",
      "Epoch 427/500\n",
      "40/40 [==============================] - 0s 26us/step - loss: 1.6008 - acc: 0.0000e+00\n",
      "Epoch 428/500\n",
      "40/40 [==============================] - 0s 64us/step - loss: 1.5987 - acc: 0.0000e+00\n",
      "Epoch 429/500\n",
      "40/40 [==============================] - 0s 44us/step - loss: 1.5966 - acc: 0.0000e+00\n",
      "Epoch 430/500\n",
      "40/40 [==============================] - 0s 46us/step - loss: 1.5945 - acc: 0.0000e+00\n",
      "Epoch 431/500\n",
      "40/40 [==============================] - 0s 51us/step - loss: 1.5924 - acc: 0.0000e+00\n",
      "Epoch 432/500\n",
      "40/40 [==============================] - 0s 48us/step - loss: 1.5904 - acc: 0.0000e+00\n",
      "Epoch 433/500\n",
      "40/40 [==============================] - 0s 39us/step - loss: 1.5883 - acc: 0.0000e+00\n",
      "Epoch 434/500\n",
      "40/40 [==============================] - 0s 34us/step - loss: 1.5863 - acc: 0.0000e+00\n",
      "Epoch 435/500\n",
      "40/40 [==============================] - 0s 62us/step - loss: 1.5843 - acc: 0.0000e+00\n",
      "Epoch 436/500\n",
      "40/40 [==============================] - 0s 40us/step - loss: 1.5823 - acc: 0.0000e+00\n",
      "Epoch 437/500\n",
      "40/40 [==============================] - 0s 42us/step - loss: 1.5802 - acc: 0.0000e+00\n",
      "Epoch 438/500\n",
      "40/40 [==============================] - 0s 41us/step - loss: 1.5782 - acc: 0.0000e+00\n",
      "Epoch 439/500\n",
      "40/40 [==============================] - 0s 35us/step - loss: 1.5763 - acc: 0.0000e+00\n",
      "Epoch 440/500\n",
      "40/40 [==============================] - 0s 30us/step - loss: 1.5743 - acc: 0.0000e+00\n",
      "Epoch 441/500\n",
      "40/40 [==============================] - 0s 25us/step - loss: 1.5723 - acc: 0.0000e+00\n",
      "Epoch 442/500\n",
      "40/40 [==============================] - 0s 37us/step - loss: 1.5703 - acc: 0.0000e+00\n",
      "Epoch 443/500\n",
      "40/40 [==============================] - 0s 34us/step - loss: 1.5684 - acc: 0.0000e+00\n",
      "Epoch 444/500\n",
      "40/40 [==============================] - 0s 23us/step - loss: 1.5664 - acc: 0.0000e+00\n",
      "Epoch 445/500\n",
      "40/40 [==============================] - 0s 34us/step - loss: 1.5645 - acc: 0.0000e+00\n",
      "Epoch 446/500\n",
      "40/40 [==============================] - 0s 53us/step - loss: 1.5626 - acc: 0.0000e+00\n",
      "Epoch 447/500\n",
      "40/40 [==============================] - 0s 38us/step - loss: 1.5607 - acc: 0.0000e+00\n",
      "Epoch 448/500\n",
      "40/40 [==============================] - 0s 42us/step - loss: 1.5588 - acc: 0.0000e+00\n",
      "Epoch 449/500\n",
      "40/40 [==============================] - 0s 37us/step - loss: 1.5569 - acc: 0.0000e+00\n",
      "Epoch 450/500\n",
      "40/40 [==============================] - 0s 60us/step - loss: 1.5550 - acc: 0.0000e+00\n",
      "Epoch 451/500\n",
      "40/40 [==============================] - 0s 24us/step - loss: 1.5531 - acc: 0.0000e+00\n",
      "Epoch 452/500\n",
      "40/40 [==============================] - 0s 39us/step - loss: 1.5512 - acc: 0.0000e+00\n",
      "Epoch 453/500\n",
      "40/40 [==============================] - 0s 30us/step - loss: 1.5494 - acc: 0.0000e+00\n",
      "Epoch 454/500\n",
      "40/40 [==============================] - 0s 38us/step - loss: 1.5475 - acc: 0.0000e+00\n",
      "Epoch 455/500\n",
      "40/40 [==============================] - 0s 38us/step - loss: 1.5457 - acc: 0.0000e+00\n",
      "Epoch 456/500\n",
      "40/40 [==============================] - 0s 84us/step - loss: 1.5438 - acc: 0.0000e+00\n",
      "Epoch 457/500\n",
      "40/40 [==============================] - 0s 30us/step - loss: 1.5420 - acc: 0.0000e+00\n",
      "Epoch 458/500\n",
      "40/40 [==============================] - 0s 38us/step - loss: 1.5402 - acc: 0.0000e+00\n",
      "Epoch 459/500\n",
      "40/40 [==============================] - 0s 34us/step - loss: 1.5384 - acc: 0.0000e+00\n",
      "Epoch 460/500\n",
      "40/40 [==============================] - 0s 25us/step - loss: 1.5366 - acc: 0.0000e+00\n",
      "Epoch 461/500\n",
      "40/40 [==============================] - 0s 34us/step - loss: 1.5348 - acc: 0.0000e+00\n",
      "Epoch 462/500\n",
      "40/40 [==============================] - 0s 32us/step - loss: 1.5330 - acc: 0.0000e+00\n",
      "Epoch 463/500\n",
      "40/40 [==============================] - 0s 50us/step - loss: 1.5312 - acc: 0.0000e+00\n",
      "Epoch 464/500\n",
      "40/40 [==============================] - 0s 49us/step - loss: 1.5294 - acc: 0.0000e+00\n",
      "Epoch 465/500\n",
      "40/40 [==============================] - 0s 54us/step - loss: 1.5277 - acc: 0.0000e+00\n",
      "Epoch 466/500\n",
      "40/40 [==============================] - 0s 35us/step - loss: 1.5259 - acc: 0.0000e+00\n",
      "Epoch 467/500\n",
      "40/40 [==============================] - 0s 36us/step - loss: 1.5242 - acc: 0.0000e+00\n",
      "Epoch 468/500\n",
      "40/40 [==============================] - 0s 37us/step - loss: 1.5224 - acc: 0.0000e+00\n",
      "Epoch 469/500\n",
      "40/40 [==============================] - 0s 47us/step - loss: 1.5207 - acc: 0.0000e+00\n",
      "Epoch 470/500\n",
      "40/40 [==============================] - 0s 51us/step - loss: 1.5190 - acc: 0.0000e+00\n",
      "Epoch 471/500\n",
      "40/40 [==============================] - 0s 30us/step - loss: 1.5172 - acc: 0.0000e+00\n",
      "Epoch 472/500\n",
      "40/40 [==============================] - 0s 34us/step - loss: 1.5155 - acc: 0.0000e+00\n",
      "Epoch 473/500\n",
      "40/40 [==============================] - 0s 62us/step - loss: 1.5138 - acc: 0.0000e+00\n",
      "Epoch 474/500\n",
      "40/40 [==============================] - 0s 52us/step - loss: 1.5121 - acc: 0.0000e+00\n",
      "Epoch 475/500\n",
      "40/40 [==============================] - 0s 34us/step - loss: 1.5104 - acc: 0.0000e+00\n",
      "Epoch 476/500\n",
      "40/40 [==============================] - 0s 37us/step - loss: 1.5088 - acc: 0.0000e+00\n",
      "Epoch 477/500\n",
      "40/40 [==============================] - 0s 32us/step - loss: 1.5071 - acc: 0.0000e+00\n",
      "Epoch 478/500\n",
      "40/40 [==============================] - 0s 29us/step - loss: 1.5054 - acc: 0.0000e+00\n",
      "Epoch 479/500\n",
      "40/40 [==============================] - 0s 48us/step - loss: 1.5038 - acc: 0.0000e+00\n",
      "Epoch 480/500\n",
      "40/40 [==============================] - 0s 36us/step - loss: 1.5021 - acc: 0.0000e+00\n",
      "Epoch 481/500\n",
      "40/40 [==============================] - 0s 45us/step - loss: 1.5005 - acc: 0.0000e+00\n",
      "Epoch 482/500\n",
      "40/40 [==============================] - 0s 64us/step - loss: 1.4988 - acc: 0.0000e+00\n",
      "Epoch 483/500\n",
      "40/40 [==============================] - 0s 37us/step - loss: 1.4972 - acc: 0.0000e+00\n",
      "Epoch 484/500\n",
      "40/40 [==============================] - 0s 39us/step - loss: 1.4956 - acc: 0.0000e+00\n",
      "Epoch 485/500\n",
      "40/40 [==============================] - 0s 40us/step - loss: 1.4940 - acc: 0.0000e+00\n",
      "Epoch 486/500\n",
      "40/40 [==============================] - 0s 36us/step - loss: 1.4924 - acc: 0.0000e+00\n",
      "Epoch 487/500\n",
      "40/40 [==============================] - 0s 31us/step - loss: 1.4908 - acc: 0.0000e+00\n",
      "Epoch 488/500\n",
      "40/40 [==============================] - 0s 34us/step - loss: 1.4892 - acc: 0.0000e+00\n",
      "Epoch 489/500\n",
      "40/40 [==============================] - 0s 36us/step - loss: 1.4876 - acc: 0.0000e+00\n",
      "Epoch 490/500\n",
      "40/40 [==============================] - 0s 36us/step - loss: 1.4860 - acc: 0.0000e+00\n",
      "Epoch 491/500\n",
      "40/40 [==============================] - 0s 60us/step - loss: 1.4844 - acc: 0.0000e+00\n",
      "Epoch 492/500\n",
      "40/40 [==============================] - 0s 58us/step - loss: 1.4828 - acc: 0.0000e+00\n",
      "Epoch 493/500\n",
      "40/40 [==============================] - 0s 59us/step - loss: 1.4813 - acc: 0.0000e+00\n",
      "Epoch 494/500\n",
      "40/40 [==============================] - 0s 65us/step - loss: 1.4797 - acc: 0.0000e+00\n",
      "Epoch 495/500\n",
      "40/40 [==============================] - 0s 111us/step - loss: 1.4782 - acc: 0.0000e+00\n",
      "Epoch 496/500\n",
      "40/40 [==============================] - 0s 64us/step - loss: 1.4766 - acc: 0.0000e+00\n",
      "Epoch 497/500\n",
      "40/40 [==============================] - 0s 40us/step - loss: 1.4751 - acc: 0.0000e+00\n",
      "Epoch 498/500\n",
      "40/40 [==============================] - 0s 36us/step - loss: 1.4736 - acc: 0.0000e+00\n",
      "Epoch 499/500\n",
      "40/40 [==============================] - 0s 41us/step - loss: 1.4720 - acc: 0.0000e+00\n",
      "Epoch 500/500\n",
      "40/40 [==============================] - 0s 33us/step - loss: 1.4705 - acc: 0.0000e+00\n",
      "60/60 [==============================] - ETA:  - 0s 415us/step\n"
     ]
    }
   ],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"uci_iris\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "url = \"../data/iris.csv\" if path.exists(\"../data/iris.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
    "class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
    "data.iloc[:,-1] = index\n",
    "data = data.loc[data[4] != 2]\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "# params = {\n",
    "#     'epochs': 170\n",
    "# }\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Keras\r\n",
      "Version: 2.2.4\r\n",
      "Summary: Deep Learning for humans\r\n",
      "Home-page: https://github.com/keras-team/keras\r\n",
      "Author: Francois Chollet\r\n",
      "Author-email: francois.chollet@gmail.com\r\n",
      "License: MIT\r\n",
      "Location: /home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages\r\n",
      "Requires: six, keras-applications, numpy, keras-preprocessing, scipy, pyyaml, h5py\r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "source": [
    "!pip show keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18 22]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "a, b = np.unique(y_train, return_counts=True)\n",
    "# len(y_test)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 5.509 - 0s 12ms/step - loss: 5.1482\n",
      "Epoch 2/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.240 - 0s 0us/step - loss: 5.1335\n",
      "Epoch 3/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.517 - 0s 391us/step - loss: 5.1188\n",
      "Epoch 4/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 5.011 - 0s 390us/step - loss: 5.1038\n",
      "Epoch 5/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.508 - 0s 391us/step - loss: 5.0873\n",
      "Epoch 6/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 5.215 - 0s 0us/step - loss: 5.0720\n",
      "Epoch 7/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 5.087 - 0s 390us/step - loss: 5.0557\n",
      "Epoch 8/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.836 - 0s 390us/step - loss: 5.0393\n",
      "Epoch 9/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 5.396 - 0s 391us/step - loss: 5.0238\n",
      "Epoch 10/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 5.395 - 0s 391us/step - loss: 5.0078\n",
      "Epoch 11/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 5.055 - 0s 391us/step - loss: 4.9915\n",
      "Epoch 12/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.953 - 0s 0us/step - loss: 4.9744\n",
      "Epoch 13/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 5.635 - 0s 391us/step - loss: 4.9605\n",
      "Epoch 14/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.623 - 0s 390us/step - loss: 4.9432\n",
      "Epoch 15/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.790 - 0s 391us/step - loss: 4.9275\n",
      "Epoch 16/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.494 - 0s 391us/step - loss: 4.9111\n",
      "Epoch 17/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.714 - 0s 390us/step - loss: 4.8952\n",
      "Epoch 18/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.580 - 0s 0us/step - loss: 4.8786\n",
      "Epoch 19/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.689 - 0s 391us/step - loss: 4.8623\n",
      "Epoch 20/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.878 - 0s 391us/step - loss: 4.8461\n",
      "Epoch 21/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.709 - 0s 0us/step - loss: 4.8294\n",
      "Epoch 22/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.673 - 0s 391us/step - loss: 4.8130\n",
      "Epoch 23/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.621 - 0s 390us/step - loss: 4.7966\n",
      "Epoch 24/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.869 - 0s 391us/step - loss: 4.7806\n",
      "Epoch 25/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.858 - 0s 0us/step - loss: 4.7642\n",
      "Epoch 26/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.628 - 0s 391us/step - loss: 4.7477\n",
      "Epoch 27/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.529 - 0s 391us/step - loss: 4.7315\n",
      "Epoch 28/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.308 - 0s 390us/step - loss: 4.7150\n",
      "Epoch 29/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.863 - 0s 0us/step - loss: 4.6993\n",
      "Epoch 30/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.463 - 0s 391us/step - loss: 4.6824\n",
      "Epoch 31/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.488 - 0s 391us/step - loss: 4.6661\n",
      "Epoch 32/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.270 - 0s 390us/step - loss: 4.6494\n",
      "Epoch 33/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.786 - 0s 391us/step - loss: 4.6336\n",
      "Epoch 34/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.702 - 0s 390us/step - loss: 4.6170\n",
      "Epoch 35/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.659 - 0s 781us/step - loss: 4.6007\n",
      "Epoch 36/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.110 - 0s 390us/step - loss: 4.5838\n",
      "Epoch 37/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.053 - 0s 0us/step - loss: 4.5675\n",
      "Epoch 38/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.404 - 0s 0us/step - loss: 4.5514\n",
      "Epoch 39/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.908 - 0s 391us/step - loss: 4.5354\n",
      "Epoch 40/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.359 - 0s 391us/step - loss: 4.5182\n",
      "Epoch 41/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.530 - 0s 391us/step - loss: 4.5022\n",
      "Epoch 42/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.781 - 0s 391us/step - loss: 4.4864\n",
      "Epoch 43/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 5.048 - 0s 390us/step - loss: 4.4707\n",
      "Epoch 44/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.526 - 0s 390us/step - loss: 4.4544\n",
      "Epoch 45/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.533 - 0s 0us/step - loss: 4.4391\n",
      "Epoch 46/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.277 - 0s 390us/step - loss: 4.4234\n",
      "Epoch 47/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.263 - 0s 391us/step - loss: 4.4079\n",
      "Epoch 48/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.745 - 0s 0us/step - loss: 4.3915\n",
      "Epoch 49/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.943 - 0s 390us/step - loss: 4.3771\n",
      "Epoch 50/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.038 - 0s 0us/step - loss: 4.3597\n",
      "Epoch 51/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.719 - 0s 391us/step - loss: 4.3433\n",
      "Epoch 52/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.439 - 0s 781us/step - loss: 4.3280\n",
      "Epoch 53/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.210 - 0s 391us/step - loss: 4.3111\n",
      "Epoch 54/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.314 - 0s 391us/step - loss: 4.2949\n",
      "Epoch 55/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.834 - 0s 391us/step - loss: 4.2779\n",
      "Epoch 56/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.171 - 0s 523us/step - loss: 4.2619\n",
      "Epoch 57/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.022 - 0s 0us/step - loss: 4.2452\n",
      "Epoch 58/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 5.090 - 0s 391us/step - loss: 4.2303\n",
      "Epoch 59/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.372 - 0s 390us/step - loss: 4.2130\n",
      "Epoch 60/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.004 - 0s 391us/step - loss: 4.1970\n",
      "Epoch 61/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.203 - 0s 392us/step - loss: 4.1817\n",
      "Epoch 62/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.513 - 0s 390us/step - loss: 4.1664\n",
      "Epoch 63/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.094 - 0s 390us/step - loss: 4.1501\n",
      "Epoch 64/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.275 - 0s 391us/step - loss: 4.1349\n",
      "Epoch 65/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.851 - 0s 0us/step - loss: 4.1188\n",
      "Epoch 66/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.861 - 0s 781us/step - loss: 4.1047\n",
      "Epoch 67/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.231 - 0s 391us/step - loss: 4.0882\n",
      "Epoch 68/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.723 - 0s 391us/step - loss: 4.0724\n",
      "Epoch 69/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.415 - 0s 0us/step - loss: 4.0566\n",
      "Epoch 70/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.104 - 0s 0us/step - loss: 4.0417\n",
      "Epoch 71/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.804 - 0s 391us/step - loss: 4.0250\n",
      "Epoch 72/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.847 - 0s 391us/step - loss: 4.0088\n",
      "Epoch 73/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.039 - 0s 390us/step - loss: 3.9928\n",
      "Epoch 74/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.277 - 0s 390us/step - loss: 3.9769\n",
      "Epoch 75/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.317 - 0s 0us/step - loss: 3.9608\n",
      "Epoch 76/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.784 - 0s 390us/step - loss: 3.9443\n",
      "Epoch 77/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.753 - 0s 390us/step - loss: 3.9287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.743 - 0s 391us/step - loss: 3.9128\n",
      "Epoch 79/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.752 - 0s 390us/step - loss: 3.8968\n",
      "Epoch 80/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.988 - 0s 391us/step - loss: 3.8810\n",
      "Epoch 81/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.013 - 0s 391us/step - loss: 3.8649\n",
      "Epoch 82/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.802 - 0s 390us/step - loss: 3.8486\n",
      "Epoch 83/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.492 - 0s 390us/step - loss: 3.8322\n",
      "Epoch 84/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.399 - 0s 391us/step - loss: 3.8160\n",
      "Epoch 85/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.101 - 0s 391us/step - loss: 3.8007\n",
      "Epoch 86/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.168 - 0s 391us/step - loss: 3.7843\n",
      "Epoch 87/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.587 - 0s 391us/step - loss: 3.7674\n",
      "Epoch 88/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.915 - 0s 0us/step - loss: 3.7522\n",
      "Epoch 89/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.781 - 0s 0us/step - loss: 3.7361\n",
      "Epoch 90/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.762 - 0s 0us/step - loss: 3.7204\n",
      "Epoch 91/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.485 - 0s 0us/step - loss: 3.7043\n",
      "Epoch 92/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.315 - 0s 391us/step - loss: 3.6881\n",
      "Epoch 93/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.937 - 0s 390us/step - loss: 3.6731\n",
      "Epoch 94/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.400 - 0s 0us/step - loss: 3.6560\n",
      "Epoch 95/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.745 - 0s 0us/step - loss: 3.6405\n",
      "Epoch 96/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.762 - 0s 390us/step - loss: 3.6244\n",
      "Epoch 97/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.713 - 0s 390us/step - loss: 3.6084\n",
      "Epoch 98/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.460 - 0s 391us/step - loss: 3.5922\n",
      "Epoch 99/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.474 - 0s 391us/step - loss: 3.5764\n",
      "Epoch 100/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.676 - 0s 391us/step - loss: 3.5608\n",
      "Epoch 101/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.412 - 0s 391us/step - loss: 3.5444\n",
      "Epoch 102/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.191 - 0s 391us/step - loss: 3.5281\n",
      "Epoch 103/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.802 - 0s 391us/step - loss: 3.5130\n",
      "Epoch 104/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.355 - 0s 0us/step - loss: 3.4960\n",
      "Epoch 105/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.545 - 0s 0us/step - loss: 3.4804\n",
      "Epoch 106/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.301 - 0s 0us/step - loss: 3.4639\n",
      "Epoch 107/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.530 - 0s 390us/step - loss: 3.4483\n",
      "Epoch 108/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.769 - 0s 391us/step - loss: 3.4327\n",
      "Epoch 109/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.130 - 0s 391us/step - loss: 3.4158\n",
      "Epoch 110/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.464 - 0s 391us/step - loss: 3.4007\n",
      "Epoch 111/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.222 - 0s 0us/step - loss: 3.3844\n",
      "Epoch 112/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.709 - 0s 0us/step - loss: 3.3694\n",
      "Epoch 113/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.640 - 0s 391us/step - loss: 3.3534\n",
      "Epoch 114/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.449 - 0s 391us/step - loss: 3.3377\n",
      "Epoch 115/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.202 - 0s 390us/step - loss: 3.3220\n",
      "Epoch 116/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.037 - 0s 0us/step - loss: 3.3064\n",
      "Epoch 117/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.792 - 0s 0us/step - loss: 3.2902\n",
      "Epoch 118/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.338 - 0s 0us/step - loss: 3.2751\n",
      "Epoch 119/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.517 - 0s 391us/step - loss: 3.2591\n",
      "Epoch 120/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.152 - 0s 391us/step - loss: 3.2422\n",
      "Epoch 121/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.443 - 0s 0us/step - loss: 3.2270\n",
      "Epoch 122/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.158 - 0s 0us/step - loss: 3.2106\n",
      "Epoch 123/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.128 - 0s 391us/step - loss: 3.1949\n",
      "Epoch 124/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.026 - 0s 391us/step - loss: 3.1790\n",
      "Epoch 125/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.434 - 0s 0us/step - loss: 3.1639\n",
      "Epoch 126/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.406 - 0s 0us/step - loss: 3.1480\n",
      "Epoch 127/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.360 - 0s 391us/step - loss: 3.1325\n",
      "Epoch 128/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.351 - 0s 0us/step - loss: 3.1174\n",
      "Epoch 129/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.308 - 0s 0us/step - loss: 3.1024\n",
      "Epoch 130/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.160 - 0s 391us/step - loss: 3.0874\n",
      "Epoch 131/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.276 - 0s 391us/step - loss: 3.0729\n",
      "Epoch 132/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.797 - 0s 0us/step - loss: 3.0573\n",
      "Epoch 133/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.941 - 0s 0us/step - loss: 3.0426\n",
      "Epoch 134/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.097 - 0s 391us/step - loss: 3.0276\n",
      "Epoch 135/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.317 - 0s 391us/step - loss: 3.0126\n",
      "Epoch 136/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.871 - 0s 0us/step - loss: 2.9965\n",
      "Epoch 137/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.615 - 0s 0us/step - loss: 2.9808\n",
      "Epoch 138/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.624 - 0s 391us/step - loss: 2.9652\n",
      "Epoch 139/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.206 - 0s 391us/step - loss: 2.9504\n",
      "Epoch 140/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.321 - 0s 0us/step - loss: 2.9345\n",
      "Epoch 141/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.950 - 0s 0us/step - loss: 2.9182\n",
      "Epoch 142/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.817 - 0s 0us/step - loss: 2.9027\n",
      "Epoch 143/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.303 - 0s 0us/step - loss: 2.8864\n",
      "Epoch 144/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.756 - 0s 0us/step - loss: 2.8715\n",
      "Epoch 145/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.132 - 0s 391us/step - loss: 2.8561\n",
      "Epoch 146/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.777 - 0s 390us/step - loss: 2.8392\n",
      "Epoch 147/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.689 - 0s 391us/step - loss: 2.8233\n",
      "Epoch 148/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.030 - 0s 0us/step - loss: 2.8082\n",
      "Epoch 149/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.840 - 0s 391us/step - loss: 2.7921\n",
      "Epoch 150/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.820 - 0s 391us/step - loss: 2.7766\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imly import dope\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1,\n",
    "                input_dim=4,\n",
    "                activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy')\n",
    "\n",
    "# m = dope(LogisticRegression())\n",
    "# m.fit(x_train.values, y_train.values)\n",
    "# m.predict(x_test)\n",
    "model.fit(x_train.values, y_train.values, epochs=150)\n",
    "test = model.predict(x_test)\n",
    "test.argmax(axis=-1)\n",
    "# value, count = np.unique(test, return_counts=True)\n",
    "# count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ../data/uci_iris_logistic_regression.pdf to Amazon S3 bucket mlsquare-datasets\n",
      "..."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://mlsquare-pdf.s3.ap-south-1.amazonaws.com:443/uci_iris_logistic_regression.pdf'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto\n",
    "import sys\n",
    "from boto.s3.key import Key\n",
    "# from boto.s3.key import Key\n",
    "fig_path = '../data/uci_iris_logistic_regression.pdf'\n",
    "fig_name = 'uci_iris_logistic_regression.pdf'\n",
    "bucket_name = 'mlsquare-datasets'\n",
    "credentials_json = json.load(open('../data/aws_credentials.json'))\n",
    "AWS_ACCESS_KEY_ID = credentials_json['AWS_ACCESS_KEY_ID']\n",
    "AWS_SECRET_ACCESS_KEY = credentials_json['AWS_SECRET_ACCESS_KEY']\n",
    "REGION_HOST = 's3.ap-south-1.amazonaws.com'\n",
    "\n",
    "# bucket_name = AWS_ACCESS_KEY_ID.lower() + '-dump'\n",
    "conn = boto.connect_s3(AWS_ACCESS_KEY_ID,\n",
    "                       AWS_SECRET_ACCESS_KEY, host=REGION_HOST)\n",
    "bucket = conn.get_bucket('mlsquare-pdf', validate=False)\n",
    "\n",
    "# bucket = conn.create_bucket(bucket_name,\n",
    "#     location=boto.s3.connection.Location.DEFAULT)\n",
    "\n",
    "print('Uploading %s to Amazon S3 bucket %s' % (fig_path, bucket_name))\n",
    "\n",
    "def percent_cb(complete, total):\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "k = Key(bucket)\n",
    "k.key = fig_name\n",
    "k.set_contents_from_filename(fig_path,\n",
    "                             cb=percent_cb, num_cb=10)  # upload file\n",
    "url = k.generate_url(expires_in=0, query_auth=False)\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #4\n",
    "\n",
    "#### UCI Adult salary dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n",
      "Epoch 1/1\n",
      "13024/13024 [==============================] - 0s 23us/step - loss: 14.3505 - acc: 0.0999\n",
      "19537/19537 [==============================] - 0s 8us/step\n"
     ]
    }
   ],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from hyperopt import hp\n",
    "\n",
    "dataset_name = \"uci_adult_salary\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "\n",
    "names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "         'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', \n",
    "         'hours-per-week', 'native-country', 'target']\n",
    "url = \"../data/adult.data.csv\" if path.exists(\"../data/adult.data.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, names=names)\n",
    "\n",
    "\n",
    "data = data[data[\"workclass\"] != \"?\"]\n",
    "data = data[data[\"occupation\"] != \"?\"]\n",
    "data = data[data[\"native-country\"] != \"?\"]\n",
    "\n",
    "# Convert categorical fields #\n",
    "categorical_col = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                   'relationship', 'race', 'sex', 'native-country', 'target']\n",
    "\n",
    "for col in categorical_col:\n",
    "    b, c = np.unique(data[col], return_inverse=True)\n",
    "    data[col] = c\n",
    "\n",
    "feature_list = names[:14]\n",
    "# Test train split #\n",
    "X = data.loc[:, feature_list]\n",
    "Y = data[['target']]\n",
    "\n",
    "space = {\n",
    "    'epochs': hp.choice(\"epochs\",[100,200])\n",
    "}\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60)\n",
    "\n",
    "# Split the dataset into test and train datasets\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #5\n",
    "\n",
    "#### UCI Ad dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, object were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, object were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "2019-02-26 15:43:57,993\tINFO tune.py:135 -- Tip: to resume incomplete experiments, pass resume='prompt' or resume=True to run_experiments()\n",
      "2019-02-26 15:43:57,994\tINFO tune.py:145 -- Starting a new experiment.\n",
      "2019-02-26 15:43:58,053\tERROR worker.py:1632 -- Warning: The actor WrappedFunc has size 11765523 when pickled. It will be stored in Redis, which could cause memory issues. This may mean that its definition uses a large array or other object.\n",
      "2019-02-26 15:43:58,100\tERROR worker.py:1632 -- Warning: The actor WrappedFunc has size 11765523 when pickled. It will be stored in Redis, which could cause memory issues. This may mean that its definition uses a large array or other object.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.2 GB\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "PENDING trials:\n",
      " - train_model_1_epochs=200:\tPENDING\n",
      "RUNNING trials:\n",
      " - train_model_0_epochs=100:\tRUNNING\n",
      "\n",
      "Result for train_model_0_epochs=100:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''sigmoid'', ''optimizer'':\n",
      "    ''adam'', ''losses'': ''binary_crossentropy'', ''epochs'': 100}.h5'\n",
      "  date: 2019-02-26_15-44-00\n",
      "  done: false\n",
      "  experiment_id: 0171d16c8e3a4d77b7b57534af433ead\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.8123011666795482\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 11985\n",
      "  time_since_restore: 1.0011260509490967\n",
      "  time_this_iter_s: 1.0011260509490967\n",
      "  time_total_s: 1.0011260509490967\n",
      "  timestamp: 1551176040\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "Result for train_model_1_epochs=200:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''sigmoid'', ''optimizer'':\n",
      "    ''adam'', ''losses'': ''binary_crossentropy'', ''epochs'': 200}.h5'\n",
      "  date: 2019-02-26_15-44-00\n",
      "  done: false\n",
      "  experiment_id: bf2b1a16fd6244e0bd53ee3f2c5781b7\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.8165429482277985\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 11987\n",
      "  time_since_restore: 1.0011229515075684\n",
      "  time_this_iter_s: 1.0011229515075684\n",
      "  time_total_s: 1.0011229515075684\n",
      "  timestamp: 1551176040\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "Result for train_model_0_epochs=100:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''sigmoid'', ''optimizer'':\n",
      "    ''adam'', ''losses'': ''binary_crossentropy'', ''epochs'': 100}.h5'\n",
      "  date: 2019-02-26_15-44-01\n",
      "  done: true\n",
      "  experiment_id: 0171d16c8e3a4d77b7b57534af433ead\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 2\n",
      "  mean_accuracy: 0.8123011666795482\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 11985\n",
      "  time_since_restore: 2.002617359161377\n",
      "  time_this_iter_s: 1.0014913082122803\n",
      "  time_total_s: 2.002617359161377\n",
      "  timestamp: 1551176041\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  \n",
      "Result for train_model_1_epochs=200:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''sigmoid'', ''optimizer'':\n",
      "    ''adam'', ''losses'': ''binary_crossentropy'', ''epochs'': 200}.h5'\n",
      "  date: 2019-02-26_15-44-01\n",
      "  done: true\n",
      "  experiment_id: bf2b1a16fd6244e0bd53ee3f2c5781b7\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 2\n",
      "  mean_accuracy: 0.8165429482277985\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 11987\n",
      "  time_since_restore: 2.0023839473724365\n",
      "  time_this_iter_s: 1.0012609958648682\n",
      "  time_total_s: 2.0023839473724365\n",
      "  timestamp: 1551176041\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.9/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "TERMINATED trials:\n",
      " - train_model_0_epochs=100:\tTERMINATED [pid=11985], 2 s, 2 iter, 0.812 acc\n",
      " - train_model_1_epochs=200:\tTERMINATED [pid=11987], 2 s, 2 iter, 0.817 acc\n",
      "\n",
      "Creating model...\n",
      "Loading from /home/shakkeel/ray_results/experiment_name/train_model_1_epochs=200_2019-02-26_15-43-5858kseym3/weights_tune_{'units': 1, 'activation': 'sigmoid', 'optimizer': 'adam', 'losses': 'binary_crossentropy', 'epochs': 200}.h5\n",
      "1416/1416 [==============================] - 0s 51us/step\n"
     ]
    }
   ],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from ray import tune\n",
    "from hyperopt import hp\n",
    "\n",
    "dataset_name = \"uci_ad\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "url = \"../data/ad.data.csv\" if path.exists(\"../data/ad.data.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "\n",
    "# Check for columns that contain missing values #\n",
    "\n",
    "data = data.applymap(lambda val: np.nan if str(val).strip() == '?' else val)\n",
    "data = data.dropna()\n",
    "\n",
    "\n",
    "# Label encoding #\n",
    "\n",
    "lb = LabelEncoder()\n",
    "Y = lb.fit_transform(data.iloc[:, -1])\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "\n",
    "# Normalize the X values #\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "X = pd.DataFrame(X)\n",
    "Y = pd.DataFrame(Y)\n",
    "\n",
    "params = {\n",
    "    \"epochs\": tune.grid_search([100, 200])\n",
    "}\n",
    "\n",
    "space = {\n",
    "    'epochs': hp.choice(\"epochs\", [100, 200])\n",
    "}\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #6\n",
    "\n",
    "#### UCI Mushroom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset info #\n",
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from hyperopt import hp\n",
    "from ray import tune\n",
    "\n",
    "dataset_name = \"uci_mushroom\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields with missing values\n",
      "stalk-root\n",
      "2480\n",
      "30.53%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = ['classes', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment',\n",
    "        'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n",
    "        'stalk-surface-below-ring', 'stalk-color-above-ring', 'stalk-color-below-ring',\n",
    "        'veil-type', 'veil-color', 'ring-number', 'ring-type', 'spore-print-color',\n",
    "        'population', 'habitat']\n",
    "url = \"../data/mushroom.data.csv\" if path.exists(\"../data/mushroom.data.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, names=names, index_col=False)\n",
    "\n",
    "# Check for columns that contain missing values #\n",
    "\n",
    "print(\"Fields with missing values\")\n",
    "col_names = data.columns\n",
    "num_data = data.shape[0]\n",
    "for c in col_names:\n",
    "    num_non = data[c].isin([\"?\"]).sum()\n",
    "    if num_non > 0:\n",
    "        print (c)\n",
    "        print (num_non)\n",
    "        print (\"{0:.2f}%\".format(float(num_non) / num_data * 100))\n",
    "        print (\"\\n\")\n",
    "\n",
    "data = data[data[\"stalk-root\"] != \"?\"]\n",
    "\n",
    "# Convert categorical fields #\n",
    "\n",
    "for col in names:\n",
    "    b, c = np.unique(data[col], return_inverse=True)\n",
    "    data[col] = c\n",
    "\n",
    "# Split the dataset into test and train datasets #\n",
    "feature_list = names[1:23]\n",
    "X = data.loc[:, feature_list]\n",
    "Y = data[['classes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "2019-02-26 16:02:49,126\tINFO tune.py:135 -- Tip: to resume incomplete experiments, pass resume='prompt' or resume=True to run_experiments()\n",
      "2019-02-26 16:02:49,127\tINFO tune.py:145 -- Starting a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.2 GB\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "PENDING trials:\n",
      " - train_model_1_epochs=200:\tPENDING\n",
      "RUNNING trials:\n",
      " - train_model_0_epochs=100:\tRUNNING\n",
      "\n",
      "Result for train_model_0_epochs=100:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''sigmoid'', ''optimizer'':\n",
      "    ''adam'', ''losses'': ''binary_crossentropy'', ''epochs'': 100}.h5'\n",
      "  date: 2019-02-26_16-02-51\n",
      "  done: false\n",
      "  experiment_id: b16163bb59874f86bfd4eb1b31b2e7a6\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.5618077095335545\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 13118\n",
      "  time_since_restore: 1.0011248588562012\n",
      "  time_this_iter_s: 1.0011248588562012\n",
      "  time_total_s: 1.0011248588562012\n",
      "  timestamp: 1551177171\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "Result for train_model_1_epochs=200:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''sigmoid'', ''optimizer'':\n",
      "    ''adam'', ''losses'': ''binary_crossentropy'', ''epochs'': 200}.h5'\n",
      "  date: 2019-02-26_16-02-51\n",
      "  done: false\n",
      "  experiment_id: 4d05e3627f3b4d97a7b4fb483f93bcc7\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.5618077095335545\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 13116\n",
      "  time_since_restore: 1.0011086463928223\n",
      "  time_this_iter_s: 1.0011086463928223\n",
      "  time_total_s: 1.0011086463928223\n",
      "  timestamp: 1551177171\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "Result for train_model_0_epochs=100:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''sigmoid'', ''optimizer'':\n",
      "    ''adam'', ''losses'': ''binary_crossentropy'', ''epochs'': 100}.h5'\n",
      "  date: 2019-02-26_16-02-52\n",
      "  done: true\n",
      "  experiment_id: b16163bb59874f86bfd4eb1b31b2e7a6\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 2\n",
      "  mean_accuracy: 0.5618077095335545\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 13118\n",
      "  time_since_restore: 2.0026259422302246\n",
      "  time_this_iter_s: 1.0015010833740234\n",
      "  time_total_s: 2.0026259422302246\n",
      "  timestamp: 1551177172\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  \n",
      "Result for train_model_1_epochs=200:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''sigmoid'', ''optimizer'':\n",
      "    ''adam'', ''losses'': ''binary_crossentropy'', ''epochs'': 200}.h5'\n",
      "  date: 2019-02-26_16-02-52\n",
      "  done: true\n",
      "  experiment_id: 4d05e3627f3b4d97a7b4fb483f93bcc7\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 2\n",
      "  mean_accuracy: 0.5618077095335545\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 13116\n",
      "  time_since_restore: 2.0023560523986816\n",
      "  time_this_iter_s: 1.0012474060058594\n",
      "  time_total_s: 2.0023560523986816\n",
      "  timestamp: 1551177172\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.8/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "TERMINATED trials:\n",
      " - train_model_0_epochs=100:\tTERMINATED [pid=13118], 2 s, 2 iter, 0.562 acc\n",
      " - train_model_1_epochs=200:\tTERMINATED [pid=13116], 2 s, 2 iter, 0.562 acc\n",
      "\n",
      "Creating model...\n",
      "Loading from /home/shakkeel/ray_results/experiment_name/train_model_0_epochs=100_2019-02-26_16-02-49g6yixt6d/weights_tune_{'units': 1, 'activation': 'sigmoid', 'optimizer': 'adam', 'losses': 'binary_crossentropy', 'epochs': 100}.h5\n",
      "3387/3387 [==============================] - 0s 68us/step\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"epochs\": tune.grid_search([100, 200])\n",
    "}\n",
    "\n",
    "space = {\n",
    "    'epochs': hp.choice(\"epochs\", [30, 10])\n",
    "}\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #7\n",
    "\n",
    "#### Covertype dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from ray import tune\n",
    "from hyperopt import hp\n",
    "\n",
    "dataset_name = \"covertype\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/covtype.data.csv\", delimiter=\",\", header=None, index_col=False)\n",
    "\n",
    "data = data[data[54].isin([1,2])]\n",
    "\n",
    "Y = data.iloc[:, -1]\n",
    "X = data.iloc[:,:-1]\n",
    "\n",
    "# Normalize the X values #\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# sc = StandardScaler()\n",
    "# X = sc.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "2019-02-26 16:30:47,685\tINFO node.py:278 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-02-26_16-30-47_14505/logs.\n",
      "2019-02-26 16:30:47,816\tINFO services.py:396 -- Waiting for redis server at 127.0.0.1:41803 to respond...\n",
      "2019-02-26 16:30:47,947\tINFO services.py:396 -- Waiting for redis server at 127.0.0.1:55615 to respond...\n",
      "2019-02-26 16:30:47,949\tINFO services.py:798 -- Starting Redis shard with 20.0 GB max memory.\n",
      "2019-02-26 16:30:48,036\tINFO services.py:1360 -- Starting the Plasma object store with 1.0 GB memory using /dev/shm.\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8889/notebooks/ray_ui.ipynb?token=68d5054d6328dc9bacde3f2d7d21a167692a71c9d31bea63\n",
      "======================================================================\n",
      "\n",
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-26 16:30:52,411\tINFO tune.py:135 -- Tip: to resume incomplete experiments, pass resume='prompt' or resume=True to run_experiments()\n",
      "2019-02-26 16:30:52,412\tINFO tune.py:145 -- Starting a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.7/8.2 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-26 16:30:52,977\tERROR worker.py:1632 -- Warning: The actor WrappedFunc has size 87149043 when pickled. It will be stored in Redis, which could cause memory issues. This may mean that its definition uses a large array or other object.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 7.0/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "PENDING trials:\n",
      " - train_model_1_epochs=200:\tPENDING\n",
      "RUNNING trials:\n",
      " - train_model_0_epochs=100:\tRUNNING\n",
      "\n",
      "Result for train_model_0_epochs=100:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''sigmoid'', ''optimizer'':\n",
      "    ''adam'', ''losses'': ''binary_crossentropy'', ''epochs'': 100, ''batch_size'':\n",
      "    100}.h5'\n",
      "  date: 2019-02-26_16-30-59\n",
      "  done: false\n",
      "  experiment_id: 4af12ae281a442c7b80fb7eb2f6921e2\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.4135749484994143\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 14541\n",
      "  time_since_restore: 5.0045411586761475\n",
      "  time_this_iter_s: 5.0045411586761475\n",
      "  time_total_s: 5.0045411586761475\n",
      "  timestamp: 1551178859\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 7.3/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "PENDING trials:\n",
      " - train_model_1_epochs=200:\tPENDING\n",
      "RUNNING trials:\n",
      " - train_model_0_epochs=100:\tRUNNING [pid=14541], 5 s, 1 iter, 0.414 acc\n",
      "\n",
      "Result for train_model_0_epochs=100:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''sigmoid'', ''optimizer'':\n",
      "    ''adam'', ''losses'': ''binary_crossentropy'', ''epochs'': 100, ''batch_size'':\n",
      "    100}.h5'\n",
      "  date: 2019-02-26_16-31-00\n",
      "  done: true\n",
      "  experiment_id: 4af12ae281a442c7b80fb7eb2f6921e2\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 2\n",
      "  mean_accuracy: 0.4135749484994143\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 14541\n",
      "  time_since_restore: 6.00605320930481\n",
      "  time_this_iter_s: 1.001512050628662\n",
      "  time_total_s: 6.00605320930481\n",
      "  timestamp: 1551178860\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-26 16:31:00,610\tERROR worker.py:1632 -- Warning: The actor WrappedFunc has size 87149043 when pickled. It will be stored in Redis, which could cause memory issues. This may mean that its definition uses a large array or other object.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_model_1_epochs=200:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''sigmoid'', ''optimizer'':\n",
      "    ''adam'', ''losses'': ''binary_crossentropy'', ''epochs'': 200, ''batch_size'':\n",
      "    100}.h5'\n",
      "  date: 2019-02-26_16-31-06\n",
      "  done: false\n",
      "  experiment_id: 367af0f5cd9b4c54b76315964f0d1af0\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.4135749484994143\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 14542\n",
      "  time_since_restore: 5.004612684249878\n",
      "  time_this_iter_s: 5.004612684249878\n",
      "  time_total_s: 5.004612684249878\n",
      "  timestamp: 1551178866\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 7.2/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "RUNNING trials:\n",
      " - train_model_1_epochs=200:\tRUNNING [pid=14542], 5 s, 1 iter, 0.414 acc\n",
      "TERMINATED trials:\n",
      " - train_model_0_epochs=100:\tTERMINATED [pid=14541], 6 s, 2 iter, 0.414 acc\n",
      "\n",
      "Result for train_model_1_epochs=200:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''sigmoid'', ''optimizer'':\n",
      "    ''adam'', ''losses'': ''binary_crossentropy'', ''epochs'': 200, ''batch_size'':\n",
      "    100}.h5'\n",
      "  date: 2019-02-26_16-31-07\n",
      "  done: true\n",
      "  experiment_id: 367af0f5cd9b4c54b76315964f0d1af0\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 2\n",
      "  mean_accuracy: 0.4135749484994143\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 14542\n",
      "  time_since_restore: 6.00609827041626\n",
      "  time_this_iter_s: 1.0014855861663818\n",
      "  time_total_s: 6.00609827041626\n",
      "  timestamp: 1551178867\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 7.2/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "TERMINATED trials:\n",
      " - train_model_0_epochs=100:\tTERMINATED [pid=14541], 6 s, 2 iter, 0.414 acc\n",
      " - train_model_1_epochs=200:\tTERMINATED [pid=14542], 6 s, 2 iter, 0.414 acc\n",
      "\n",
      "Creating model...\n",
      "Loading from /home/shakkeel/ray_results/experiment_name/train_model_0_epochs=100_2019-02-26_16-30-52q3xgy681/weights_tune_{'units': 1, 'activation': 'sigmoid', 'optimizer': 'adam', 'losses': 'binary_crossentropy', 'epochs': 100, 'batch_size': 100}.h5\n",
      "297085/297085 [==============================] - 2s 5us/step\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"epochs\": tune.grid_search([100, 200]),\n",
    "    \"batch_size\": 100\n",
    "}\n",
    "\n",
    "space = {\n",
    "    'epochs': hp.choice(\"epochs\", [100, 200]),\n",
    "    \"batch_size\": 100\n",
    "}\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #8\n",
    "\n",
    "#### TestData1 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"test_data_1\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/testData1.csv\", delimiter=\",\", header=0, index_col=0)\n",
    "\n",
    "\n",
    "Y = data.iloc[:, -1]\n",
    "X = data.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "600/600 [==============================] - ETA:  - 0s 105us/step\n",
      "Confusion matrix, without normalization\n",
      "Uploading ../data/test_data_1_logistic_regression.pdf to Amazon S3 bucket mlsquare-datasets\n",
      "..."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\":10\n",
    "}\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #9\n",
    "\n",
    "#### TestData2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"test_data_2\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/testData2.csv\", delimiter=\",\", header=0, index_col=0)\n",
    "\n",
    "\n",
    "Y = data.iloc[:, -1]\n",
    "X = data.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique,count = np.unique(Y,return_counts=True)\n",
    "class1=count[0]/X.shape[0]*100\n",
    "class2=count[1]/X.shape[0]*100\n",
    "class_distribution = round(class1, 2)\n",
    "unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "2019-02-26 16:55:11,877\tINFO tune.py:135 -- Tip: to resume incomplete experiments, pass resume='prompt' or resume=True to run_experiments()\n",
      "2019-02-26 16:55:11,877\tINFO tune.py:145 -- Starting a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.2 GB\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "RUNNING trials:\n",
      " - train_model_0:\tRUNNING\n",
      "\n",
      "Result for train_model_0:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''sigmoid'', ''optimizer'':\n",
      "    ''adam'', ''losses'': ''binary_crossentropy'', ''epochs'': 1000, ''batch_size'':\n",
      "    100}.h5'\n",
      "  date: 2019-02-26_16-55-13\n",
      "  done: false\n",
      "  experiment_id: 8de9fff6f02842d1a96422200e31d48a\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.496875\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 14540\n",
      "  time_since_restore: 1.0011181831359863\n",
      "  time_this_iter_s: 1.0011181831359863\n",
      "  time_total_s: 1.0011181831359863\n",
      "  timestamp: 1551180313\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "Result for train_model_0:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''sigmoid'', ''optimizer'':\n",
      "    ''adam'', ''losses'': ''binary_crossentropy'', ''epochs'': 1000, ''batch_size'':\n",
      "    100}.h5'\n",
      "  date: 2019-02-26_16-55-14\n",
      "  done: true\n",
      "  experiment_id: 8de9fff6f02842d1a96422200e31d48a\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 2\n",
      "  mean_accuracy: 0.496875\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 14540\n",
      "  time_since_restore: 2.0028293132781982\n",
      "  time_this_iter_s: 1.001711130142212\n",
      "  time_total_s: 2.0028293132781982\n",
      "  timestamp: 1551180314\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "TERMINATED trials:\n",
      " - train_model_0:\tTERMINATED [pid=14540], 2 s, 2 iter, 0.497 acc\n",
      "\n",
      "Creating model...\n",
      "Loading from /home/shakkeel/ray_results/experiment_name/train_model_0_2019-02-26_16-55-11mj4ugxfg/weights_tune_{'units': 1, 'activation': 'sigmoid', 'optimizer': 'adam', 'losses': 'binary_crossentropy', 'epochs': 1000, 'batch_size': 100}.h5\n",
      "480/480 [==============================] - 0s 64us/step\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"epochs\": 1000,\n",
    "    \"batch_size\":100\n",
    "}\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #10\n",
    "\n",
    "#### UCI Airfoil dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dataset_name = \"uci_airfoil\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/uci_airfoil_self_noise.csv\", delimiter=\",\", header=0, index_col=0)\n",
    "sc = StandardScaler()\n",
    "data = sc.fit_transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "Y = data.iloc[:, -1]\n",
    "X = data.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "601/601 [==============================] - 0s 163us/step - loss: 3.0601 - acc: 0.0000e+00\n",
      "902/902 [==============================] - 0s 38us/step\n"
     ]
    }
   ],
   "source": [
    "automation_script.run_imly(dataset_info, 'linear_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #11\n",
    "\n",
    "#### UCI Auto-mpg dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64, object were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, float64, object were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dataset_name = \"uci_auto_mpg\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/uci_auto_mpg.csv\", delimiter=\",\", header=0, index_col='car name')\n",
    "data = data[data.horsepower != '?']\n",
    "sc = StandardScaler()\n",
    "data = sc.fit_transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "Y = data.iloc[:,1]\n",
    "X = data.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "156/156 [==============================] - 0s 684us/step - loss: 0.8558 - acc: 0.0000e+00\n",
      "236/236 [==============================] - 0s 126us/step\n"
     ]
    }
   ],
   "source": [
    "automation_script.run_imly(dataset_info, 'linear_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "array is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2f5bc21b2f8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: array is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.zeros((int(1e12),int(1e12)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #12\n",
    "\n",
    "#### Testdata 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dataset_name = \"test_data_3\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/testData3.csv\", delimiter=\",\", header=0, index_col=0)\n",
    "\n",
    "sc = StandardScaler()\n",
    "data = sc.fit_transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "Y = data.iloc[:, -1]\n",
    "X = data.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "400/400 [==============================] - 0s 353us/step - loss: 2.8405 - acc: 0.0000e+00\n",
      "600/600 [==============================] - 0s 90us/step\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"epochs\": 1000,\n",
    "    \"batch_size\":100\n",
    "}\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'linear_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #13\n",
    "\n",
    "#### Testdata 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dataset_name = \"test_data_4\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/testData4.csv\", delimiter=\",\", header=0, index_col=0)\n",
    "\n",
    "sc = StandardScaler()\n",
    "data = sc.fit_transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "Y = data.iloc[:, -1]\n",
    "X = data.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:27<00:00, 27.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "600/600 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 843us/step\n",
      "Uploading ../data/test_data_4_linear_regression.pdf to Amazon S3 bucket mlsquare-pdf\n",
      "...."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {\n",
    "    \"epochs\": 1000,\n",
    "    \"batch_size\":100\n",
    "}\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'linear_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #14\n",
    "\n",
    "#### Iris LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "2019-02-27 17:25:36,504\tWARNING worker.py:1354 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n",
      "2019-02-27 17:25:36,527\tINFO node.py:278 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-02-27_17-25-36_1352/logs.\n",
      "2019-02-27 17:25:36,679\tINFO services.py:396 -- Waiting for redis server at 127.0.0.1:41754 to respond...\n",
      "2019-02-27 17:25:36,791\tINFO services.py:396 -- Waiting for redis server at 127.0.0.1:34342 to respond...\n",
      "2019-02-27 17:25:36,793\tINFO services.py:798 -- Starting Redis shard with 20.0 GB max memory.\n",
      "2019-02-27 17:25:36,837\tINFO services.py:1360 -- Starting the Plasma object store with 1.0 GB memory using /dev/shm.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8890/notebooks/ray_ui.ipynb?token=5c45a972d5bb7c223bae8eb31e6b9126083531db1e838c91\n",
      "======================================================================\n",
      "\n",
      "Keras classifier chosen\n",
      "Epoch 1/1\n",
      "40/40 [==============================] - 0s 135us/step - loss: 0.4438 - acc: 0.5500\n",
      "60/60 [==============================] - 0s 23us/step\n"
     ]
    }
   ],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"uci_iris_lda\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "url = \"../data/iris.csv\" if path.exists(\"../data/iris.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
    "class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
    "data.iloc[:,-1] = index\n",
    "data = data.loc[data[4] != 2]\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "params = {\n",
    "    'epochs': 170\n",
    "}\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'linear_discrimant_analysis', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #15\n",
    "\n",
    "#### Adult salary LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "2019-02-26 17:28:42,012\tWARNING worker.py:1354 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n",
      "2019-02-26 17:28:42,014\tINFO node.py:278 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-02-26_17-28-42_16711/logs.\n",
      "2019-02-26 17:28:42,155\tINFO services.py:396 -- Waiting for redis server at 127.0.0.1:10057 to respond...\n",
      "2019-02-26 17:28:42,293\tINFO services.py:396 -- Waiting for redis server at 127.0.0.1:32600 to respond...\n",
      "2019-02-26 17:28:42,302\tINFO services.py:798 -- Starting Redis shard with 20.0 GB max memory.\n",
      "2019-02-26 17:28:42,368\tINFO services.py:1360 -- Starting the Plasma object store with 1.0 GB memory using /dev/shm.\n",
      "2019-02-26 17:28:42,523\tINFO tune.py:135 -- Tip: to resume incomplete experiments, pass resume='prompt' or resume=True to run_experiments()\n",
      "2019-02-26 17:28:42,524\tINFO tune.py:145 -- Starting a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8890/notebooks/ray_ui.ipynb?token=6c6f8f461e6b30cd0a9965c3f12c297797cc09cd8174bcad\n",
      "======================================================================\n",
      "\n",
      "Keras classifier chosen\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.2 GB\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "RUNNING trials:\n",
      " - train_model_0:\tRUNNING\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-26 17:28:54,305\tERROR trial_runner.py:413 -- Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shakkeel/anaconda3/envs/theano/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 378, in _process_events\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/shakkeel/anaconda3/envs/theano/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py\", line 228, in fetch_result\n",
      "    result = ray.get(trial_future[0])\n",
      "  File \"/home/shakkeel/anaconda3/envs/theano/lib/python3.6/site-packages/ray/worker.py\", line 2132, in get\n",
      "    raise value\n",
      "ray.worker.RayTaskError: \u001b[36mray_worker\u001b[39m (pid=16772, host=shakkeel-TUF-GAMING-FX504GD-FX80GD)\n",
      "  File \"/home/shakkeel/anaconda3/envs/theano/lib/python3.6/site-packages/ray/tune/trainable.py\", line 151, in train\n",
      "    result = self._train()\n",
      "  File \"/home/shakkeel/anaconda3/envs/theano/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 128, in _train\n",
      "    result = self._status_reporter._get_and_clear_status()\n",
      "  File \"/home/shakkeel/anaconda3/envs/theano/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 50, in _get_and_clear_status\n",
      "    raise TuneError(\"Error running trial: \" + str(self._error))\n",
      "ray.tune.error.TuneError: Error running trial: InvalidValueError\n",
      "        type(variable) = TensorType(float32, matrix)\n",
      "        variable       = Elemwise{true_div,no_inplace}.0\n",
      "        type(value)    = <class 'numpy.ndarray'>\n",
      "        dtype(value)   = float32\n",
      "        shape(value)   = (1, 1)\n",
      "        value          = [[nan]]\n",
      "        min(value)     = nan\n",
      "        max(value)     = nan\n",
      "        isfinite       = False\n",
      "        client_node    = None\n",
      "        hint           = perform output\n",
      "        specific_hint  = non-finite elements not allowed\n",
      "        context        = ...\n",
      "  Elemwise{true_div,no_inplace} [id A] ''   \n",
      "   |Dot22 [id B] ''   \n",
      "   | |InplaceDimShuffle{1,0} [id C] ''   \n",
      "   | | |Elemwise{Sub}[(0, 0)] [id D] ''   \n",
      "   | |   |AdvancedSubtensor1 [id E] ''   \n",
      "   | |   | |<TensorType(float32, matrix)> [id F]\n",
      "   | |   | |Subtensor{int64} [id G] ''   \n",
      "   | |   |   |Nonzero [id H] ''   \n",
      "   | |   |   | |<TensorType(bool, vector)> [id I]\n",
      "   | |   |   |Constant{0} [id J]\n",
      "   | |   |Elemwise{TrueDiv}[(0, 0)] [id K] ''   \n",
      "   | |     |InplaceDimShuffle{x,0} [id L] ''   \n",
      "   | |     | |Sum{axis=[0], acc_dtype=float64} [id M] ''   \n",
      "   | |     |   |AdvancedSubtensor1 [id E] ''   \n",
      "   | |     |Elemwise{Cast{float32}} [id N] ''   \n",
      "   | |       |InplaceDimShuffle{x,x} [id O] ''   \n",
      "   | |         |Shape_i{1} [id P] ''   \n",
      "   | |           |Nonzero [id H] ''   \n",
      "   | |Elemwise{Sub}[(0, 0)] [id D] ''   \n",
      "   |Elemwise{Add}[(0, 1)] [id Q] ''   \n",
      "     |TensorConstant{(1, 1) of -1.0} [id R]\n",
      "     |Elemwise{Cast{float32}} [id N] ''   \n",
      "\n",
      "        \n",
      "Apply node that caused the error: for{cpu,scan_fn}(Shape_i{0}.0, Elemwise{eq,no_inplace}.0, Shape_i{0}.0, Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)].0)\n",
      "Toposort index: 70\n",
      "Inputs types: [TensorType(int64, scalar), TensorType(bool, matrix), TensorType(int64, scalar), TensorType(float32, matrix)]\n",
      "Inputs shapes: [(), (2, 32), (), (32, 1)]\n",
      "Inputs strides: [(), (32, 1), (), (4, 4)]\n",
      "Inputs values: [array(2), 'not shown', array(2), 'not shown']\n",
      "Outputs clients: [[Sum{axis=[0], acc_dtype=float64}(for{cpu,scan_fn}.0)]]\n",
      "\n",
      "HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\n",
      "HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.5/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "ERROR trials:\n",
      " - train_model_0:\tERROR, 1 failures: /home/shakkeel/ray_results/experiment_name/train_model_0_2019-02-26_17-28-4200u6xlme/error_2019-02-26_17-28-54.txt\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.5/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "ERROR trials:\n",
      " - train_model_0:\tERROR, 1 failures: /home/shakkeel/ray_results/experiment_name/train_model_0_2019-02-26_17-28-4200u6xlme/error_2019-02-26_17-28-54.txt\n",
      "\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [train_model_0])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c3c07585f780>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m }\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mautomation_script\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_imly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'linear_discrimant_analysis'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/mlsquare/cook-imly/imly/automation_script.py\u001b[0m in \u001b[0;36mrun_imly\u001b[0;34m(dataset_info, model_name, X, Y, test_size, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'space'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0mkeras_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/mlsquare/cook-imly/imly/wrappers/sklearn/keras_classifier.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x_train, y_train, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 self.model = get_best_model(x_train, y_train,\n\u001b[1;32m     54\u001b[0m                                             \u001b[0mprimal_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprimal_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                             params=self.params, space=hyperopt_space)\n\u001b[0m\u001b[1;32m     56\u001b[0m                 self.model.fit(x_train, y_train, epochs=200,\n\u001b[1;32m     57\u001b[0m                                batch_size=30, verbose=0)\n",
      "\u001b[0;32m~/Desktop/mlsquare/cook-imly/imly/optimizers/tune/tune.py\u001b[0m in \u001b[0;36mget_best_model\u001b[0;34m(x_train, y_train, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfiguration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"mean_accuracy\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/theano/lib/python3.6/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun_experiments\u001b[0;34m(experiments, search_alg, scheduler, with_server, server_port, verbose, resume, queue_trials, trial_executor, raise_on_failed_trial)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [train_model_0])"
     ]
    }
   ],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.utils.validation import column_or_1d\n",
    "\n",
    "dataset_name = \"uci_adult_salary_lda\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "\n",
    "names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "         'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', \n",
    "         'hours-per-week', 'native-country', 'target']\n",
    "url = \"../data/adult.data.csv\"\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, names=names)\n",
    "\n",
    "\n",
    "data = data[data[\"workclass\"] != \"?\"]\n",
    "data = data[data[\"occupation\"] != \"?\"]\n",
    "data = data[data[\"native-country\"] != \"?\"]\n",
    "\n",
    "# Convert categorical fields #\n",
    "categorical_col = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                   'relationship', 'race', 'sex', 'native-country', 'target']\n",
    "\n",
    "for col in categorical_col:\n",
    "    b, c = np.unique(data[col], return_inverse=True)\n",
    "    data[col] = c\n",
    "\n",
    "feature_list = names[:14]\n",
    "# Test train split #\n",
    "X = data.loc[:, feature_list]\n",
    "Y = data[['target']]\n",
    "# Y = column_or_1d(Y, warn=True)\n",
    "\n",
    "params = {\n",
    "    'batch_size': 1000,\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'linear_discrimant_analysis', X, Y, 0.60, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing tune by ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.tune as tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 1.6259 - acc: 0.5500\n",
      "60/60 [==============================] - 0s 630us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.8434850215911864, 0.4666666626930237]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose iris with logistic regression as a test dataset #\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url = \"../data/iris.csv\"\n",
    "data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
    "class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
    "data.iloc[:,-1] = index\n",
    "data = data.loc[data[4] != 2]\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "\n",
    "def make_model():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1,\n",
    "                    input_dim=4,\n",
    "                    activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_iris(args):\n",
    "    model = make_model()\n",
    "    model.fit(x_train,y_train)\n",
    "    model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-16 19:59:52,761\tWARNING worker.py:1354 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n",
      "2019-02-16 19:59:52,763\tINFO node.py:278 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-02-16_19-59-52_11337/logs.\n",
      "2019-02-16 19:59:52,876\tINFO services.py:396 -- Waiting for redis server at 127.0.0.1:44284 to respond...\n",
      "2019-02-16 19:59:52,989\tINFO services.py:396 -- Waiting for redis server at 127.0.0.1:38375 to respond...\n",
      "2019-02-16 19:59:52,997\tINFO services.py:798 -- Starting Redis shard with 10.0 GB max memory.\n",
      "2019-02-16 19:59:53,049\tINFO services.py:1360 -- Starting the Plasma object store with 3.2850935800000003 GB memory using /dev/shm.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8888/notebooks/ray_ui.ipynb?token=28905882e7b4006a0c688e21e69424958e0f2e952cc53040\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': None,\n",
       " 'redis_address': '192.168.1.4:44284',\n",
       " 'object_store_address': '/tmp/ray/session_2019-02-16_19-59-52_11337/sockets/plasma_store',\n",
       " 'webui_url': 'http://localhost:8888/notebooks/ray_ui.ipynb?token=28905882e7b4006a0c688e21e69424958e0f2e952cc53040',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2019-02-16_19-59-52_11337/sockets/raylet'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the signature of train function to accomodate Tune #\n",
    "def train_iris_tune(config, reporter):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        config (dict): Parameters provided from the search algorithm\n",
    "            or variant generation.\n",
    "        reporter (Reporter): Handle to report intermediate metrics to Tune.\n",
    "    \"\"\"\n",
    "    model = make_model()\n",
    "    model.fit(x_train,y_train)\n",
    "    accuracy = model.evaluate(x_test, y_test)[1]\n",
    "    reporter(mean_accuracy=accuracy, metric2=1, metric3=0.3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'test_reporter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a0aacb736dd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtest_reporter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mtest_reporter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iris_tune\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'test_reporter'"
     ]
    }
   ],
   "source": [
    "from ray import test_reporter\n",
    "assert test_reporter(train_iris_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring a Tune experiment #\n",
    "# 1) Search space\n",
    "# 2) Stopping criteria\n",
    "\n",
    "configuration = tune.Experiment(\n",
    "    \"experiment_name\",\n",
    "    run=train_iris_tune,\n",
    "    resources_per_trial={\"cpu\": 4},\n",
    "    stop={\"mean_accuracy\": 95},  # TODO: Part 1\n",
    "    config={\n",
    "        \"optimizer\": tune.grid_search(['adam', 'nadam'])\n",
    "    }  # TODO: Part 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-17 11:28:02,985\tINFO tune.py:135 -- Tip: to resume incomplete experiments, pass resume='prompt' or resume=True to run_experiments()\n",
      "2019-02-17 11:28:02,986\tINFO tune.py:145 -- Starting a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/8.2 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/8 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/8.2 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "PENDING trials:\n",
      " - train_iris_tune_1_optimizer=nadam:\tPENDING\n",
      "RUNNING trials:\n",
      " - train_iris_tune_0_optimizer=adam:\tRUNNING\n",
      "\n",
      "Result for train_iris_tune_1_optimizer=nadam:\n",
      "  date: 2019-02-17_11-28-05\n",
      "  done: false\n",
      "  experiment_id: 49523f8d5a164a08a23bfbd2b5e5800c\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.4666666626930237\n",
      "  metric2: 1\n",
      "  metric3: 0.3\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 23323\n",
      "  time_since_restore: 1.0010333061218262\n",
      "  time_this_iter_s: 1.0010333061218262\n",
      "  time_total_s: 1.0010333061218262\n",
      "  timestamp: 1550383085\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "Result for train_iris_tune_0_optimizer=adam:\n",
      "  date: 2019-02-17_11-28-05\n",
      "  done: false\n",
      "  experiment_id: cf285405c95e47e19c57db48b29f9ea7\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.4666666626930237\n",
      "  metric2: 1\n",
      "  metric3: 0.3\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 23321\n",
      "  time_since_restore: 1.0004656314849854\n",
      "  time_this_iter_s: 1.0004656314849854\n",
      "  time_total_s: 1.0004656314849854\n",
      "  timestamp: 1550383085\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "Result for train_iris_tune_1_optimizer=nadam:\n",
      "  date: 2019-02-17_11-28-06\n",
      "  done: true\n",
      "  experiment_id: 49523f8d5a164a08a23bfbd2b5e5800c\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 2\n",
      "  mean_accuracy: 0.4666666626930237\n",
      "  metric2: 1\n",
      "  metric3: 0.3\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 23323\n",
      "  time_since_restore: 2.0022518634796143\n",
      "  time_this_iter_s: 1.001218557357788\n",
      "  time_total_s: 2.0022518634796143\n",
      "  timestamp: 1550383086\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  \n",
      "Result for train_iris_tune_0_optimizer=adam:\n",
      "  date: 2019-02-17_11-28-06\n",
      "  done: true\n",
      "  experiment_id: cf285405c95e47e19c57db48b29f9ea7\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 2\n",
      "  mean_accuracy: 0.4666666626930237\n",
      "  metric2: 1\n",
      "  metric3: 0.3\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 23321\n",
      "  time_since_restore: 2.001732110977173\n",
      "  time_this_iter_s: 1.0012664794921875\n",
      "  time_total_s: 2.001732110977173\n",
      "  timestamp: 1550383086\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/8.2 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "TERMINATED trials:\n",
      " - train_iris_tune_0_optimizer=adam:\tTERMINATED [pid=23321], 2 s, 2 iter, 0.467 acc\n",
      " - train_iris_tune_1_optimizer=nadam:\tTERMINATED [pid=23323], 2 s, 2 iter, 0.467 acc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the experiment #\n",
    "trials = tune.run_experiments(configuration, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_best_result'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-3f9cda0cf31a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtune\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_best_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The best result is\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_best_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mean_accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_best_result'"
     ]
    }
   ],
   "source": [
    "from ray.tune.util import \n",
    "# print(\"The best result is\", get_best_result(trials, metric=\"mean_accuracy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               False\n",
       "workclass         False\n",
       "fnlwgt            False\n",
       "education         False\n",
       "education-num     False\n",
       "marital-status    False\n",
       "occupation        False\n",
       "relationship      False\n",
       "race              False\n",
       "sex               False\n",
       "capital-gain      False\n",
       "capital-loss      False\n",
       "hours-per-week    False\n",
       "native-country    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.isinf(X).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #16\n",
    "\n",
    "#### Abalone LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n",
      "Epoch 1/100\n",
      "529/529 [==============================] - 1s 1ms/step - loss: 0.0051 - acc: 0.4726\n",
      "Epoch 2/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0051 - acc: 0.4726\n",
      "Epoch 3/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0051 - acc: 0.4726\n",
      "Epoch 4/100\n",
      "529/529 [==============================] - 0s 8us/step - loss: 0.0051 - acc: 0.4726\n",
      "Epoch 5/100\n",
      "529/529 [==============================] - 0s 11us/step - loss: 0.0050 - acc: 0.4726\n",
      "Epoch 6/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0050 - acc: 0.4726\n",
      "Epoch 7/100\n",
      "529/529 [==============================] - 0s 11us/step - loss: 0.0050 - acc: 0.4726\n",
      "Epoch 8/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0050 - acc: 0.4726\n",
      "Epoch 9/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0050 - acc: 0.4726\n",
      "Epoch 10/100\n",
      "529/529 [==============================] - 0s 11us/step - loss: 0.0050 - acc: 0.4726\n",
      "Epoch 11/100\n",
      "529/529 [==============================] - 0s 8us/step - loss: 0.0050 - acc: 0.4726\n",
      "Epoch 12/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0050 - acc: 0.4726\n",
      "Epoch 13/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0049 - acc: 0.4726\n",
      "Epoch 14/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0049 - acc: 0.4726\n",
      "Epoch 15/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0049 - acc: 0.4726\n",
      "Epoch 16/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0049 - acc: 0.4726\n",
      "Epoch 17/100\n",
      "529/529 [==============================] - 0s 9us/step - loss: 0.0049 - acc: 0.4726\n",
      "Epoch 18/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0049 - acc: 0.4726\n",
      "Epoch 19/100\n",
      "529/529 [==============================] - 0s 8us/step - loss: 0.0049 - acc: 0.4726\n",
      "Epoch 20/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0049 - acc: 0.4726\n",
      "Epoch 21/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0048 - acc: 0.4726\n",
      "Epoch 22/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0048 - acc: 0.4726\n",
      "Epoch 23/100\n",
      "529/529 [==============================] - 0s 9us/step - loss: 0.0048 - acc: 0.4726\n",
      "Epoch 24/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0048 - acc: 0.4726\n",
      "Epoch 25/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0048 - acc: 0.4726\n",
      "Epoch 26/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0048 - acc: 0.4726\n",
      "Epoch 27/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0047 - acc: 0.4726\n",
      "Epoch 28/100\n",
      "529/529 [==============================] - 0s 13us/step - loss: 0.0047 - acc: 0.4726\n",
      "Epoch 29/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0047 - acc: 0.4726\n",
      "Epoch 30/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0047 - acc: 0.4726\n",
      "Epoch 31/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0047 - acc: 0.4726\n",
      "Epoch 32/100\n",
      "529/529 [==============================] - 0s 13us/step - loss: 0.0047 - acc: 0.4726\n",
      "Epoch 33/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0046 - acc: 0.4726\n",
      "Epoch 34/100\n",
      "529/529 [==============================] - 0s 15us/step - loss: 0.0046 - acc: 0.4726\n",
      "Epoch 35/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0046 - acc: 0.4726\n",
      "Epoch 36/100\n",
      "529/529 [==============================] - 0s 9us/step - loss: 0.0046 - acc: 0.4726\n",
      "Epoch 37/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0046 - acc: 0.4726\n",
      "Epoch 38/100\n",
      "529/529 [==============================] - 0s 9us/step - loss: 0.0045 - acc: 0.4726\n",
      "Epoch 39/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0045 - acc: 0.4726\n",
      "Epoch 40/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0045 - acc: 0.4726\n",
      "Epoch 41/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0045 - acc: 0.4726\n",
      "Epoch 42/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0045 - acc: 0.4726\n",
      "Epoch 43/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0044 - acc: 0.4726\n",
      "Epoch 44/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0044 - acc: 0.4726\n",
      "Epoch 45/100\n",
      "529/529 [==============================] - 0s 11us/step - loss: 0.0044 - acc: 0.4726\n",
      "Epoch 46/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0044 - acc: 0.4726\n",
      "Epoch 47/100\n",
      "529/529 [==============================] - 0s 17us/step - loss: 0.0043 - acc: 0.4726\n",
      "Epoch 48/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0043 - acc: 0.4726\n",
      "Epoch 49/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0043 - acc: 0.4726\n",
      "Epoch 50/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0043 - acc: 0.4726\n",
      "Epoch 51/100\n",
      "529/529 [==============================] - 0s 15us/step - loss: 0.0042 - acc: 0.4726\n",
      "Epoch 52/100\n",
      "529/529 [==============================] - 0s 9us/step - loss: 0.0042 - acc: 0.4726\n",
      "Epoch 53/100\n",
      "529/529 [==============================] - 0s 9us/step - loss: 0.0042 - acc: 0.4726\n",
      "Epoch 54/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0042 - acc: 0.4726\n",
      "Epoch 55/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0041 - acc: 0.4726\n",
      "Epoch 56/100\n",
      "529/529 [==============================] - 0s 13us/step - loss: 0.0041 - acc: 0.4726\n",
      "Epoch 57/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0041 - acc: 0.4726\n",
      "Epoch 58/100\n",
      "529/529 [==============================] - 0s 15us/step - loss: 0.0040 - acc: 0.4726\n",
      "Epoch 59/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0040 - acc: 0.4726\n",
      "Epoch 60/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0040 - acc: 0.4726\n",
      "Epoch 61/100\n",
      "529/529 [==============================] - 0s 11us/step - loss: 0.0039 - acc: 0.4726\n",
      "Epoch 62/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0039 - acc: 0.4726\n",
      "Epoch 63/100\n",
      "529/529 [==============================] - 0s 11us/step - loss: 0.0039 - acc: 0.4726\n",
      "Epoch 64/100\n",
      "529/529 [==============================] - 0s 9us/step - loss: 0.0039 - acc: 0.4726\n",
      "Epoch 65/100\n",
      "529/529 [==============================] - 0s 8us/step - loss: 0.0038 - acc: 0.4726\n",
      "Epoch 66/100\n",
      "529/529 [==============================] - 0s 11us/step - loss: 0.0038 - acc: 0.4726\n",
      "Epoch 67/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0037 - acc: 0.4726\n",
      "Epoch 68/100\n",
      "529/529 [==============================] - 0s 11us/step - loss: 0.0037 - acc: 0.4726\n",
      "Epoch 69/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0037 - acc: 0.4726\n",
      "Epoch 70/100\n",
      "529/529 [==============================] - 0s 8us/step - loss: 0.0036 - acc: 0.4726\n",
      "Epoch 71/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0036 - acc: 0.4726\n",
      "Epoch 72/100\n",
      "529/529 [==============================] - 0s 17us/step - loss: 0.0036 - acc: 0.4726\n",
      "Epoch 73/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0035 - acc: 0.4726\n",
      "Epoch 74/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0035 - acc: 0.4726\n",
      "Epoch 75/100\n",
      "529/529 [==============================] - 0s 13us/step - loss: 0.0034 - acc: 0.4726\n",
      "Epoch 76/100\n",
      "529/529 [==============================] - 0s 8us/step - loss: 0.0034 - acc: 0.4726\n",
      "Epoch 77/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0034 - acc: 0.4726\n",
      "Epoch 78/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0033 - acc: 0.4726\n",
      "Epoch 79/100\n",
      "529/529 [==============================] - 0s 17us/step - loss: 0.0033 - acc: 0.4726\n",
      "Epoch 80/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0032 - acc: 0.4726\n",
      "Epoch 81/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0032 - acc: 0.4726\n",
      "Epoch 82/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0031 - acc: 0.4726\n",
      "Epoch 83/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0031 - acc: 0.4726\n",
      "Epoch 84/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0030 - acc: 0.4726\n",
      "Epoch 85/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0030 - acc: 0.4726\n",
      "Epoch 86/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0030 - acc: 0.4726\n",
      "Epoch 87/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0029 - acc: 0.4726\n",
      "Epoch 88/100\n",
      "529/529 [==============================] - 0s 9us/step - loss: 0.0029 - acc: 0.4726\n",
      "Epoch 89/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0028 - acc: 0.4726\n",
      "Epoch 90/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0027 - acc: 0.4726\n",
      "Epoch 91/100\n",
      "529/529 [==============================] - 0s 8us/step - loss: 0.0027 - acc: 0.4726\n",
      "Epoch 92/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0026 - acc: 0.4726\n",
      "Epoch 93/100\n",
      "529/529 [==============================] - 0s 13us/step - loss: 0.0026 - acc: 0.4726\n",
      "Epoch 94/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0025 - acc: 0.4726\n",
      "Epoch 95/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0025 - acc: 0.4726\n",
      "Epoch 96/100\n",
      "529/529 [==============================] - 0s 8us/step - loss: 0.0024 - acc: 0.4726\n",
      "Epoch 97/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0024 - acc: 0.4726\n",
      "Epoch 98/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0023 - acc: 0.4726\n",
      "Epoch 99/100\n",
      "529/529 [==============================] - 0s 8us/step - loss: 0.0022 - acc: 0.4726\n",
      "Epoch 100/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0022 - acc: 0.4726\n",
      "794/794 [==============================] - ETA:  - 0s 14us/step\n"
     ]
    }
   ],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "dataset_info = automation_script.get_dataset_info(\"uci_abalone_lda\")\n",
    "\n",
    "names = [\"sex\", \"length\", \"diameter\", \"height\", \"whole weight\",\n",
    "        \"shucked weight\", \"viscera weight\", \"shell weight\", \"rings\"]\n",
    "url = \"../data/abalone.data.csv\"\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, names=names, index_col=False)\n",
    "data.head()\n",
    "\n",
    "# Check for columns that contain missing values #\n",
    "col_names = data.columns\n",
    "\n",
    "num_data = data.shape[0]\n",
    "\n",
    "categorical_col = ['sex']\n",
    "for col in categorical_col:\n",
    "    b, c = np.unique(data[col], return_inverse=True)\n",
    "    data[col] = c\n",
    "\n",
    "    \n",
    "# Filter dataset to contain 'rings' 9 and 10 #\n",
    "data = data[data['rings'].isin([9,10])]\n",
    "data['rings'] = data['rings'].map({9: 0, 10: 1})\n",
    "\n",
    "\n",
    "feature_list = names[:7]\n",
    "X = data.loc[:, feature_list]\n",
    "Y = data[['rings']]\n",
    "\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'linear_discrimant_analysis', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #17\n",
    "\n",
    "#### Mushroom LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields with missing values\n",
      "stalk-root\n",
      "2480\n",
      "30.53%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.0564 - acc: 0.5432\n",
      "Epoch 2/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0572 - acc: 0.5445\n",
      "Epoch 3/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.0581 - acc: 0.5467\n",
      "Epoch 4/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.0590 - acc: 0.5476\n",
      "Epoch 5/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.0599 - acc: 0.5498\n",
      "Epoch 6/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0608 - acc: 0.5516\n",
      "Epoch 7/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0617 - acc: 0.5516\n",
      "Epoch 8/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0627 - acc: 0.5521\n",
      "Epoch 9/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0636 - acc: 0.5529\n",
      "Epoch 10/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.0646 - acc: 0.5525\n",
      "Epoch 11/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.0656 - acc: 0.5543\n",
      "Epoch 12/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.0666 - acc: 0.5552\n",
      "Epoch 13/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0676 - acc: 0.5556\n",
      "Epoch 14/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0687 - acc: 0.5565\n",
      "Epoch 15/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0697 - acc: 0.5569\n",
      "Epoch 16/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0708 - acc: 0.5583\n",
      "Epoch 17/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0719 - acc: 0.5583\n",
      "Epoch 18/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.0730 - acc: 0.5591\n",
      "Epoch 19/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0741 - acc: 0.5605\n",
      "Epoch 20/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0752 - acc: 0.5614\n",
      "Epoch 21/100\n",
      "2257/2257 [==============================] - 0s 5us/step - loss: -0.0764 - acc: 0.5631\n",
      "Epoch 22/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0775 - acc: 0.5662\n",
      "Epoch 23/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0787 - acc: 0.5667\n",
      "Epoch 24/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0800 - acc: 0.5702\n",
      "Epoch 25/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.0812 - acc: 0.5698\n",
      "Epoch 26/100\n",
      "2257/2257 [==============================] - 0s 5us/step - loss: -0.0825 - acc: 0.5711\n",
      "Epoch 27/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0837 - acc: 0.5742\n",
      "Epoch 28/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0850 - acc: 0.5742\n",
      "Epoch 29/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0864 - acc: 0.5755\n",
      "Epoch 30/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0877 - acc: 0.5778\n",
      "Epoch 31/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.0891 - acc: 0.5778\n",
      "Epoch 32/100\n",
      "2257/2257 [==============================] - 0s 5us/step - loss: -0.0904 - acc: 0.5804\n",
      "Epoch 33/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0918 - acc: 0.5813\n",
      "Epoch 34/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.0933 - acc: 0.5813\n",
      "Epoch 35/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.0947 - acc: 0.5826\n",
      "Epoch 36/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0962 - acc: 0.5840\n",
      "Epoch 37/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0977 - acc: 0.5857\n",
      "Epoch 38/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.0992 - acc: 0.5866\n",
      "Epoch 39/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.1007 - acc: 0.5888\n",
      "Epoch 40/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1022 - acc: 0.5911\n",
      "Epoch 41/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.1038 - acc: 0.5919\n",
      "Epoch 42/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.1054 - acc: 0.5942\n",
      "Epoch 43/100\n",
      "2257/2257 [==============================] - 0s 6us/step - loss: -0.1070 - acc: 0.5937\n",
      "Epoch 44/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1086 - acc: 0.5942\n",
      "Epoch 45/100\n",
      "2257/2257 [==============================] - 0s 6us/step - loss: -0.1103 - acc: 0.5955\n",
      "Epoch 46/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.1119 - acc: 0.5959\n",
      "Epoch 47/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1136 - acc: 0.5981\n",
      "Epoch 48/100\n",
      "2257/2257 [==============================] - 0s 5us/step - loss: -0.1153 - acc: 0.5964\n",
      "Epoch 49/100\n",
      "2257/2257 [==============================] - 0s 5us/step - loss: -0.1170 - acc: 0.5977\n",
      "Epoch 50/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1187 - acc: 0.5995\n",
      "Epoch 51/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1205 - acc: 0.6008\n",
      "Epoch 52/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1223 - acc: 0.5999\n",
      "Epoch 53/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.1241 - acc: 0.5990\n",
      "Epoch 54/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1259 - acc: 0.6008\n",
      "Epoch 55/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1277 - acc: 0.6008\n",
      "Epoch 56/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1295 - acc: 0.6008\n",
      "Epoch 57/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.1314 - acc: 0.6026\n",
      "Epoch 58/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.1333 - acc: 0.6043\n",
      "Epoch 59/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1352 - acc: 0.6061\n",
      "Epoch 60/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.1371 - acc: 0.6061\n",
      "Epoch 61/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.1390 - acc: 0.6079\n",
      "Epoch 62/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1409 - acc: 0.6079\n",
      "Epoch 63/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.1429 - acc: 0.6088\n",
      "Epoch 64/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.1449 - acc: 0.6097\n",
      "Epoch 65/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.1469 - acc: 0.6097\n",
      "Epoch 66/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.1489 - acc: 0.6114\n",
      "Epoch 67/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1509 - acc: 0.6123\n",
      "Epoch 68/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.1529 - acc: 0.6128\n",
      "Epoch 69/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1549 - acc: 0.6128\n",
      "Epoch 70/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.1570 - acc: 0.6145\n",
      "Epoch 71/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1591 - acc: 0.6154\n",
      "Epoch 72/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1611 - acc: 0.6154\n",
      "Epoch 73/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1632 - acc: 0.6167\n",
      "Epoch 74/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1653 - acc: 0.6167\n",
      "Epoch 75/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1674 - acc: 0.6176\n",
      "Epoch 76/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1696 - acc: 0.6185\n",
      "Epoch 77/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1717 - acc: 0.6194\n",
      "Epoch 78/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.1738 - acc: 0.6207\n",
      "Epoch 79/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1760 - acc: 0.6221\n",
      "Epoch 80/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1781 - acc: 0.6234\n",
      "Epoch 81/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1803 - acc: 0.6238\n",
      "Epoch 82/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1825 - acc: 0.6243\n",
      "Epoch 83/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1847 - acc: 0.6256\n",
      "Epoch 84/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1868 - acc: 0.6269\n",
      "Epoch 85/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1890 - acc: 0.6269\n",
      "Epoch 86/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1912 - acc: 0.6287\n",
      "Epoch 87/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.1934 - acc: 0.6296\n",
      "Epoch 88/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1957 - acc: 0.6296\n",
      "Epoch 89/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1979 - acc: 0.6305\n",
      "Epoch 90/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.2001 - acc: 0.6318\n",
      "Epoch 91/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.2023 - acc: 0.6323\n",
      "Epoch 92/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.2046 - acc: 0.6331\n",
      "Epoch 93/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.2068 - acc: 0.6336\n",
      "Epoch 94/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.2090 - acc: 0.6336\n",
      "Epoch 95/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.2113 - acc: 0.6340\n",
      "Epoch 96/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.2135 - acc: 0.6336\n",
      "Epoch 97/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.2158 - acc: 0.6349\n",
      "Epoch 98/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.2180 - acc: 0.6358\n",
      "Epoch 99/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.2202 - acc: 0.6380\n",
      "Epoch 100/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.2225 - acc: 0.6389\n",
      "3387/3387 [==============================] - ETA:  - 0s 13us/step\n"
     ]
    }
   ],
   "source": [
    "# Load dataset info #\n",
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"uci_mushroom_lda\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "names = ['classes', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment',\n",
    "        'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n",
    "        'stalk-surface-below-ring', 'stalk-color-above-ring', 'stalk-color-below-ring',\n",
    "        'veil-type', 'veil-color', 'ring-number', 'ring-type', 'spore-print-color',\n",
    "        'population', 'habitat']\n",
    "url = \"../data/mushroom.data.csv\"\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, names=names, index_col=False)\n",
    "\n",
    "# Check for columns that contain missing values #\n",
    "\n",
    "print(\"Fields with missing values\")\n",
    "col_names = data.columns\n",
    "num_data = data.shape[0]\n",
    "for c in col_names:\n",
    "    num_non = data[c].isin([\"?\"]).sum()\n",
    "    if num_non > 0:\n",
    "        print (c)\n",
    "        print (num_non)\n",
    "        print (\"{0:.2f}%\".format(float(num_non) / num_data * 100))\n",
    "        print (\"\\n\")\n",
    "\n",
    "data = data[data[\"stalk-root\"] != \"?\"]\n",
    "\n",
    "# Convert categorical fields #\n",
    "\n",
    "for col in names:\n",
    "    b, c = np.unique(data[col], return_inverse=True)\n",
    "    data[col] = c\n",
    "\n",
    "# Split the dataset into test and train datasets #\n",
    "feature_list = names[1:23]\n",
    "X = data.loc[:, feature_list]\n",
    "Y = data[['classes']]\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'linear_discrimant_analysis', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #18\n",
    "\n",
    "#### Ad dataset LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, object were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, object were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'sklearn.linear_model' has no attribute 'LinearDiscriminantAnalysis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-adec3f214c98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mautomation_script\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_imly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'linear_discrimant_analysis'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/mlsquare/cook-imly/imly/automation_script.py\u001b[0m in \u001b[0;36mrun_imly\u001b[0;34m(dataset_info, model_name, X, Y, test_size, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sklearn.linear_model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfromlist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;31m# module = __import__('sklearn.discriminant_analysis', fromlist=[name]) # Find a fix!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mimported_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimported_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0mmodel_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'sklearn.linear_model' has no attribute 'LinearDiscriminantAnalysis'"
     ]
    }
   ],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dataset_name = \"uci_ad_lda\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "url = \"../data/ad.data.csv\"\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "\n",
    "# Check for columns that contain missing values #\n",
    "\n",
    "data = data.applymap(lambda val: np.nan if str(val).strip() == '?' else val)\n",
    "data = data.dropna()\n",
    "\n",
    "\n",
    "# Label encoding #\n",
    "\n",
    "lb = LabelEncoder()\n",
    "Y = lb.fit_transform(data.iloc[:, -1])\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "\n",
    "# Normalize the X values #\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "X = pd.DataFrame(X)\n",
    "Y = pd.DataFrame(Y)\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'linear_discrimant_analysis', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #19\n",
    "\n",
    "#### Abalone multiclass version dataset (multi class logReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "2019-03-06 13:43:54,687\tINFO tune.py:135 -- Tip: to resume incomplete experiments, pass resume='prompt' or resume=True to run_experiments()\n",
      "2019-03-06 13:43:54,688\tINFO tune.py:145 -- Starting a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n",
      "LogisticRegressionMultiClass  --- from keras_classifier.py\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/8.2 GB\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "PENDING trials:\n",
      " - train_model_1_epochs=200:\tPENDING\n",
      "RUNNING trials:\n",
      " - train_model_0_epochs=100:\tRUNNING\n",
      "\n",
      "Result for train_model_0_epochs=100:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''sigmoid'', ''optimizer'':\n",
      "    ''adam'', ''losses'': ''binary_crossentropy'', ''epochs'': 100}.h5'\n",
      "  date: 2019-03-06_13-43-59\n",
      "  done: false\n",
      "  experiment_id: ddbe569152c44fa38c8037816cc9b72e\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.5847669618306972\n",
      "  node_ip: 192.168.1.22\n",
      "  pid: 15780\n",
      "  time_since_restore: 1.000140905380249\n",
      "  time_this_iter_s: 1.000140905380249\n",
      "  time_total_s: 1.000140905380249\n",
      "  timestamp: 1551860039\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "Result for train_model_0_epochs=100:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''sigmoid'', ''optimizer'':\n",
      "    ''adam'', ''losses'': ''binary_crossentropy'', ''epochs'': 100}.h5'\n",
      "  date: 2019-03-06_13-44-00\n",
      "  done: true\n",
      "  experiment_id: ddbe569152c44fa38c8037816cc9b72e\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 2\n",
      "  mean_accuracy: 0.5847669618306972\n",
      "  node_ip: 192.168.1.22\n",
      "  pid: 15780\n",
      "  time_since_restore: 2.000673770904541\n",
      "  time_this_iter_s: 1.000532865524292\n",
      "  time_total_s: 2.000673770904541\n",
      "  timestamp: 1551860040\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "PENDING trials:\n",
      " - train_model_1_epochs=200:\tPENDING\n",
      "TERMINATED trials:\n",
      " - train_model_0_epochs=100:\tTERMINATED [pid=15780], 2 s, 2 iter, 0.585 acc\n",
      "\n",
      "Result for train_model_1_epochs=200:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''sigmoid'', ''optimizer'':\n",
      "    ''adam'', ''losses'': ''binary_crossentropy'', ''epochs'': 200}.h5'\n",
      "  date: 2019-03-06_13-44-03\n",
      "  done: false\n",
      "  experiment_id: ce6bd4e1d34b4fcd8204a0b8b8b7dbea\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.7136377856599913\n",
      "  node_ip: 192.168.1.22\n",
      "  pid: 15782\n",
      "  time_since_restore: 1.0006623268127441\n",
      "  time_this_iter_s: 1.0006623268127441\n",
      "  time_total_s: 1.0006623268127441\n",
      "  timestamp: 1551860043\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "Result for train_model_1_epochs=200:\n",
      "  checkpoint: 'weights_tune_{''units'': 1, ''activation'': ''sigmoid'', ''optimizer'':\n",
      "    ''adam'', ''losses'': ''binary_crossentropy'', ''epochs'': 200}.h5'\n",
      "  date: 2019-03-06_13-44-04\n",
      "  done: true\n",
      "  experiment_id: ce6bd4e1d34b4fcd8204a0b8b8b7dbea\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 2\n",
      "  mean_accuracy: 0.7136377856599913\n",
      "  node_ip: 192.168.1.22\n",
      "  pid: 15782\n",
      "  time_since_restore: 2.0018603801727295\n",
      "  time_this_iter_s: 1.0011980533599854\n",
      "  time_total_s: 2.0018603801727295\n",
      "  timestamp: 1551860044\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "TERMINATED trials:\n",
      " - train_model_0_epochs=100:\tTERMINATED [pid=15780], 2 s, 2 iter, 0.585 acc\n",
      " - train_model_1_epochs=200:\tTERMINATED [pid=15782], 2 s, 2 iter, 0.714 acc\n",
      "\n",
      "Creating model...\n",
      "Loading from /home/shakkeel/ray_results/experiment_name/train_model_1_epochs=200_2019-03-06_13-44-00_auyadne/weights_tune_{'units': 1, 'activation': 'sigmoid', 'optimizer': 'adam', 'losses': 'binary_crossentropy', 'epochs': 200}.h5\n",
      "2504/2504 [==============================] - 0s 37us/step\n",
      "[0.5797714421543451, 0.713953333159986]\n"
     ]
    }
   ],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "dataset_info = automation_script.get_dataset_info(\"uci_abalone_multi_class\")\n",
    "\n",
    "names = [\"sex\", \"length\", \"diameter\", \"height\", \"whole weight\",\n",
    "        \"shucked weight\", \"viscera weight\", \"shell weight\", \"rings\"]\n",
    "url = \"../data/abalone.data.csv\"\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, names=names, index_col=False)\n",
    "data.head()\n",
    "\n",
    "# Check for columns that contain missing values #\n",
    "col_names = data.columns\n",
    "\n",
    "num_data = data.shape[0]\n",
    "\n",
    "categorical_col = ['sex']\n",
    "for col in categorical_col:\n",
    "    b, c = np.unique(data[col], return_inverse=True)\n",
    "    data[col] = c\n",
    "\n",
    "    \n",
    "# Filter dataset to contain 'rings' 9 and 10 #\n",
    "# data = data[data['rings'].isin([9,10])]\n",
    "# data['rings'] = data['rings'].map({9: 0, 10: 1})\n",
    "\n",
    "# Excluding labels 1, 2, 25, 26 and 29 since these labels contain just one sample\n",
    "# and this causes issue while creating a stratified Y.\n",
    "for x in [1,2,25,26,29]:\n",
    "    data = data[data['rings'] != x]\n",
    "\n",
    "\n",
    "feature_list = names[:7]\n",
    "X = data.loc[:, feature_list]\n",
    "Y = data[['rings']]\n",
    "\n",
    "# Y = to_categorical(Y)\n",
    "\n",
    "params = {\n",
    "    'epochs': {'grid_search': [100, 200]}\n",
    "}\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #20\n",
    "\n",
    "####  Covertype(multi class logReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from ray import tune\n",
    "from hyperopt import hp\n",
    "\n",
    "dataset_name = \"covertype_multiclass\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/covtype.data.csv\", delimiter=\",\", header=None, index_col=False)\n",
    "\n",
    "# data = data[data[54].isin([1,2])]\n",
    "\n",
    "Y = data.iloc[:, -1]\n",
    "X = data.iloc[:,:-1]\n",
    "\n",
    "# Normalize the X values #\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# sc = StandardScaler()\n",
    "# X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionMultiClass  --- from keras_classifier.py\n",
      "Epoch 1/1\n",
      "232404/232404 [==============================] - 9s 38us/step - loss: 2.9839 - acc: 0.8124\n",
      "348608/348608 [==============================] - 7s 21us/step\n",
      "[2.1819754718191278, 0.8621706281186288]\n"
     ]
    }
   ],
   "source": [
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #21\n",
    "\n",
    "#### Page blocks dataset (multi class logReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from ray import tune\n",
    "from hyperopt import hp\n",
    "\n",
    "dataset_name = \"uci_page_blocks\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/uci_page_blocks\", delimiter=\"\\s+\", header=None, index_col=False)\n",
    "\n",
    "# data = data[data[54].isin([1,2])]\n",
    "\n",
    "Y = data.iloc[:, -1]\n",
    "X = data.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n",
      "LogisticRegressionMultiClass  --- from keras_classifier.py\n",
      "Epoch 1/1\n",
      "2189/2189 [==============================] - 0s 190us/step - loss: 5.0581 - acc: 0.6576\n",
      "3284/3284 [==============================] - 0s 35us/step\n",
      "[4.308372911669305, 0.6934835646532921]\n"
     ]
    }
   ],
   "source": [
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(path_or_buf='/home/shakkeel/Desktop/mlsquare/cook-imly/data/uci_page_blocks.csv', header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "ohe = OneHotEncoder()\n",
    "encoded_y_train = ohe.fit_transform(y_train)\n",
    "\n",
    "sklearn_model = LogisticRegression()\n",
    "sklearn_model.fit(x_train, y_train)\n",
    "y_pred = sklearn_model.predict(x_test)\n",
    "\n",
    "encoded_y_pred = ohe.transform(y_pred.reshape(-1, 1))\n",
    "keras_encoded_y_pred = to_categorical(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2504, 12)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_encoded_y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1668/1668 [==============================] - 0s 92us/step - loss: 3.1298 - acc: 0.0719\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f264c511208>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='error')\n",
    "\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "\n",
    "def make_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(y_train.shape[1], input_dim=x_train.shape[1], activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "dummy_model = make_model()\n",
    "dummy_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "send stratified split\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unknown categorical feature present [27 27] during transform.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-bc9f808076e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"send stratified split\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# dummy_model.evaluate(x_test, y_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-bc9f808076e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"send stratified split\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    607\u001b[0m             return _transform_selected(X, self._legacy_transform, self.dtype,\n\u001b[1;32m    608\u001b[0m                                        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_categorical_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                                        copy=True)\n\u001b[0m\u001b[1;32m    610\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/preprocessing/base.py\u001b[0m in \u001b[0;36m_transform_selected\u001b[0;34m(X, transform, dtype, selected, copy, retain_order)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mselected\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"all\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_legacy_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'error'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m                 raise ValueError(\"unknown categorical feature present %s \"\n\u001b[0;32m--> 547\u001b[0;31m                                  \"during transform.\" % X.ravel()[~mask])\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0mcolumn_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unknown categorical feature present [27 27] during transform."
     ]
    }
   ],
   "source": [
    "try:\n",
    "    y_test = encoder.transform(y_test)\n",
    "except ValueError as e:\n",
    "    print(\"send stratified split\")\n",
    "    raise e\n",
    "# dummy_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 30)                240       \n",
      "=================================================================\n",
      "Total params: 240\n",
      "Trainable params: 240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from keras.utils import plot_model\n",
    "# plot_model(dummy_model, to_file='model.png')\n",
    "dummy_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2507/2507 [==============================] - 0s 20us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.2268173135415843, 0.22895891507355684]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 30)                240       \n",
      "=================================================================\n",
      "Total params: 240\n",
      "Trainable params: 240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dummy_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3b4f6a5685ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimly\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprimal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mimly_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimly_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "from imly import dope\n",
    "from keras.utils import to_categorical\n",
    "primal = LogisticRegression(fit_intercept=False)\n",
    "imly_model = dope(primal)\n",
    "imly_model.fit(x_train, y_train, params=params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = imly_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03909054646988432"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.metrics.accuracy_score(y_test, y_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_1 to have shape (12,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-17c6b490d7a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimly_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_1 to have shape (12,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "score = imly_model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, count = np.unique(y_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "sklearn_model = LogisticRegression()\n",
    "sklearn_model.fit(x_train, y_train)\n",
    "sklearn_pred = sklearn_model.predict(x_train)\n",
    "test_split = keras.utils.np_utils.to_categorical(sklearn_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1670, 12)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_split.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_1 to have shape (12,) but got array with shape (30,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5b6d3cd89ffd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mencoded_y_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimly_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_y_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_1 to have shape (12,) but got array with shape (30,)"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "encoded_y_test = to_categorical(y_test)\n",
    "imly_model.model.evaluate(x_test, encoded_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-27d3425f7a85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import column_or_1d\n",
    "Y = column_or_1d(Y)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# m = LogisticRegression(solver='lbfgs', multi_class='ovr')\n",
    "m = LogisticRegression()\n",
    "\n",
    "m.fit(x_train, y_train)\n",
    "score = m.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "       19, 20, 21, 22, 23, 24, 25, 26, 27, 29])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test iris multiclass logreg #\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "url = \"../data/iris.csv\"\n",
    "data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
    "class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
    "data.iloc[:,-1] = index\n",
    "data = data.loc[data[4] != 2]\n",
    "X2 = data.iloc[:,:-1]\n",
    "Y2 = data.iloc[:,-1]\n",
    "\n",
    "m2 = LogisticRegression()\n",
    "m2.fit(X2, Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LogisticRegressionMultiClass'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.__class__.__name__ + 'MultiClass'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test bed ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Hyperas(Logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from imly import dope\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import boto\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import sys\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from boto.s3.key import Key\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import LabelEncoder\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.utils.validation import column_or_1d\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import LabelEncoder\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import copy\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import datasets\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import experiment_automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LinearRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import mean_squared_error\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import experiment_automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import re\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from automation_script import get_dataset_info\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from imly import dope\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from utils.correlations import concordance_correlation_coefficient as ccc\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LinearRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from imly import dope\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from utils.correlations import concordance_correlation_coefficient as ccc\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import boto\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import sys\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from boto.s3.key import Key\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.datasets import make_regression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import theano.tensor as T\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import theano\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from theano.compile.ops import as_op\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adam\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l2\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'optimizer': hp.choice('optimizer', ['adam', 'nadam']),\n",
      "        'batch_size': hp.choice('batch_size', [10, 30]),\n",
      "        'epochs': hp.choice('epochs', [100, 170]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: '''\n",
      "  3: Data providing function:\n",
      "  4: \n",
      "  5: Make sure to have every relevant import statement included here and return data as\n",
      "  6: used in model function below. This function is separated from model() so that hyperopt\n",
      "  7: won't reload data for each evaluation run.\n",
      "  8: '''\n",
      "  9: url = \"../data/iris.csv\"\n",
      " 10: data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
      " 11: class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
      " 12: data.iloc[:,-1] = index\n",
      " 13: data = data.loc[data[4] != 2]\n",
      " 14: X = data.iloc[:,:-1]\n",
      " 15: Y = data.iloc[:,-1]\n",
      " 16: x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
      " 17: \n",
      " 18: \n",
      " 19: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     '''\n",
      "   4:     Model providing function:\n",
      "   5: \n",
      "   6:     Create Keras model with double curly brackets dropped-in as needed.\n",
      "   7:     Return value has to be a valid python dictionary with two customary keys:\n",
      "   8:         - loss: Specify a numeric evaluation metric to be minimized\n",
      "   9:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
      "  10:     The last one is optional, though recommended, namely:\n",
      "  11:         - model: specify the model just created so that we can later use it again.\n",
      "  12:     '''\n",
      "  13: \n",
      "  14:     model = Sequential()\n",
      "  15:     model.add(Dense(1, input_dim=4, activation='sigmoid'))\n",
      "  16: \n",
      "  17:     model.compile(loss='binary_crossentropy', optimizer=space['optimizer'],\n",
      "  18:                  metrics=['accuracy'])\n",
      "  19: \n",
      "  20:     model.fit(x_train, y_train,\n",
      "  21:               batch_size=space['batch_size'],\n",
      "  22:               epochs=space['epochs'],\n",
      "  23:               verbose=2,\n",
      "  24:               validation_data=(x_test, y_test))\n",
      "  25:     score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
      "  26:     print('Test accuracy:', acc)\n",
      "  27:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  28: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40 samples, validate on 60 samples\n",
      "Epoch 1/170\n",
      " - 0s - loss: 0.8220 - acc: 0.5500 - val_loss: 0.8823 - val_acc: 0.4667\n",
      "Epoch 2/170\n",
      " - 0s - loss: 0.8120 - acc: 0.5500 - val_loss: 0.8704 - val_acc: 0.4667\n",
      "Epoch 3/170\n",
      " - 0s - loss: 0.8036 - acc: 0.5500 - val_loss: 0.8589 - val_acc: 0.4667\n",
      "Epoch 4/170\n",
      " - 0s - loss: 0.7932 - acc: 0.5500 - val_loss: 0.8484 - val_acc: 0.4667\n",
      "Epoch 5/170\n",
      " - 0s - loss: 0.7865 - acc: 0.5500 - val_loss: 0.8375 - val_acc: 0.4667\n",
      "Epoch 6/170\n",
      " - 0s - loss: 0.7778 - acc: 0.5500 - val_loss: 0.8276 - val_acc: 0.4667\n",
      "Epoch 7/170\n",
      " - 0s - loss: 0.7692 - acc: 0.5500 - val_loss: 0.8183 - val_acc: 0.4667\n",
      "Epoch 8/170\n",
      " - 0s - loss: 0.7628 - acc: 0.5500 - val_loss: 0.8088 - val_acc: 0.4667\n",
      "Epoch 9/170\n",
      " - 0s - loss: 0.7558 - acc: 0.5500 - val_loss: 0.7994 - val_acc: 0.4667\n",
      "Epoch 10/170\n",
      " - 0s - loss: 0.7486 - acc: 0.5500 - val_loss: 0.7904 - val_acc: 0.4667\n",
      "Epoch 11/170\n",
      " - 0s - loss: 0.7421 - acc: 0.5500 - val_loss: 0.7815 - val_acc: 0.4667\n",
      "Epoch 12/170\n",
      " - 0s - loss: 0.7358 - acc: 0.5500 - val_loss: 0.7729 - val_acc: 0.4667\n",
      "Epoch 13/170\n",
      " - 0s - loss: 0.7287 - acc: 0.5500 - val_loss: 0.7648 - val_acc: 0.4667\n",
      "Epoch 14/170\n",
      " - 0s - loss: 0.7218 - acc: 0.5500 - val_loss: 0.7575 - val_acc: 0.4667\n",
      "Epoch 15/170\n",
      " - 0s - loss: 0.7161 - acc: 0.5500 - val_loss: 0.7503 - val_acc: 0.4667\n",
      "Epoch 16/170\n",
      " - 0s - loss: 0.7110 - acc: 0.5500 - val_loss: 0.7427 - val_acc: 0.4667\n",
      "Epoch 17/170\n",
      " - 0s - loss: 0.7061 - acc: 0.5500 - val_loss: 0.7350 - val_acc: 0.4667\n",
      "Epoch 18/170\n",
      " - 0s - loss: 0.6991 - acc: 0.5500 - val_loss: 0.7284 - val_acc: 0.4667\n",
      "Epoch 19/170\n",
      " - 0s - loss: 0.6941 - acc: 0.5500 - val_loss: 0.7217 - val_acc: 0.4667\n",
      "Epoch 20/170\n",
      " - 0s - loss: 0.6882 - acc: 0.5500 - val_loss: 0.7157 - val_acc: 0.4667\n",
      "Epoch 21/170\n",
      " - 0s - loss: 0.6837 - acc: 0.5500 - val_loss: 0.7094 - val_acc: 0.4667\n",
      "Epoch 22/170\n",
      " - 0s - loss: 0.6786 - acc: 0.5500 - val_loss: 0.7031 - val_acc: 0.4667\n",
      "Epoch 23/170\n",
      " - 0s - loss: 0.6736 - acc: 0.5500 - val_loss: 0.6971 - val_acc: 0.4667\n",
      "Epoch 24/170\n",
      " - 0s - loss: 0.6687 - acc: 0.5500 - val_loss: 0.6913 - val_acc: 0.4667\n",
      "Epoch 25/170\n",
      " - 0s - loss: 0.6638 - acc: 0.5500 - val_loss: 0.6857 - val_acc: 0.4667\n",
      "Epoch 26/170\n",
      " - 0s - loss: 0.6591 - acc: 0.5500 - val_loss: 0.6802 - val_acc: 0.4667\n",
      "Epoch 27/170\n",
      " - 0s - loss: 0.6545 - acc: 0.5500 - val_loss: 0.6748 - val_acc: 0.4667\n",
      "Epoch 28/170\n",
      " - 0s - loss: 0.6508 - acc: 0.5500 - val_loss: 0.6690 - val_acc: 0.4667\n",
      "Epoch 29/170\n",
      " - 0s - loss: 0.6462 - acc: 0.5500 - val_loss: 0.6633 - val_acc: 0.4667\n",
      "Epoch 30/170\n",
      " - 0s - loss: 0.6415 - acc: 0.5500 - val_loss: 0.6581 - val_acc: 0.4667\n",
      "Epoch 31/170\n",
      " - 0s - loss: 0.6367 - acc: 0.5500 - val_loss: 0.6533 - val_acc: 0.4667\n",
      "Epoch 32/170\n",
      " - 0s - loss: 0.6325 - acc: 0.5500 - val_loss: 0.6484 - val_acc: 0.4667\n",
      "Epoch 33/170\n",
      " - 0s - loss: 0.6286 - acc: 0.5500 - val_loss: 0.6434 - val_acc: 0.4667\n",
      "Epoch 34/170\n",
      " - 0s - loss: 0.6244 - acc: 0.5500 - val_loss: 0.6385 - val_acc: 0.4667\n",
      "Epoch 35/170\n",
      " - 0s - loss: 0.6199 - acc: 0.5750 - val_loss: 0.6342 - val_acc: 0.4667\n",
      "Epoch 36/170\n",
      " - 0s - loss: 0.6162 - acc: 0.5750 - val_loss: 0.6295 - val_acc: 0.4667\n",
      "Epoch 37/170\n",
      " - 0s - loss: 0.6121 - acc: 0.5750 - val_loss: 0.6248 - val_acc: 0.4667\n",
      "Epoch 38/170\n",
      " - 0s - loss: 0.6080 - acc: 0.5750 - val_loss: 0.6205 - val_acc: 0.4667\n",
      "Epoch 39/170\n",
      " - 0s - loss: 0.6042 - acc: 0.5750 - val_loss: 0.6161 - val_acc: 0.4833\n",
      "Epoch 40/170\n",
      " - 0s - loss: 0.6004 - acc: 0.6000 - val_loss: 0.6116 - val_acc: 0.5000\n",
      "Epoch 41/170\n",
      " - 0s - loss: 0.5967 - acc: 0.6000 - val_loss: 0.6071 - val_acc: 0.5000\n",
      "Epoch 42/170\n",
      " - 0s - loss: 0.5926 - acc: 0.6000 - val_loss: 0.6029 - val_acc: 0.5000\n",
      "Epoch 43/170\n",
      " - 0s - loss: 0.5895 - acc: 0.6000 - val_loss: 0.5983 - val_acc: 0.5167\n",
      "Epoch 44/170\n",
      " - 0s - loss: 0.5852 - acc: 0.6250 - val_loss: 0.5942 - val_acc: 0.5167\n",
      "Epoch 45/170\n",
      " - 0s - loss: 0.5819 - acc: 0.6250 - val_loss: 0.5899 - val_acc: 0.5333\n",
      "Epoch 46/170\n",
      " - 0s - loss: 0.5778 - acc: 0.6500 - val_loss: 0.5861 - val_acc: 0.5333\n",
      "Epoch 47/170\n",
      " - 0s - loss: 0.5741 - acc: 0.6500 - val_loss: 0.5825 - val_acc: 0.5333\n",
      "Epoch 48/170\n",
      " - 0s - loss: 0.5710 - acc: 0.6500 - val_loss: 0.5784 - val_acc: 0.5333\n",
      "Epoch 49/170\n",
      " - 0s - loss: 0.5672 - acc: 0.6500 - val_loss: 0.5748 - val_acc: 0.5333\n",
      "Epoch 50/170\n",
      " - 0s - loss: 0.5636 - acc: 0.6500 - val_loss: 0.5713 - val_acc: 0.5667\n",
      "Epoch 51/170\n",
      " - 0s - loss: 0.5603 - acc: 0.6500 - val_loss: 0.5676 - val_acc: 0.5667\n",
      "Epoch 52/170\n",
      " - 0s - loss: 0.5569 - acc: 0.6500 - val_loss: 0.5639 - val_acc: 0.5667\n",
      "Epoch 53/170\n",
      " - 0s - loss: 0.5538 - acc: 0.6500 - val_loss: 0.5601 - val_acc: 0.5833\n",
      "Epoch 54/170\n",
      " - 0s - loss: 0.5503 - acc: 0.6750 - val_loss: 0.5564 - val_acc: 0.5833\n",
      "Epoch 55/170\n",
      " - 0s - loss: 0.5468 - acc: 0.6750 - val_loss: 0.5531 - val_acc: 0.6167\n",
      "Epoch 56/170\n",
      " - 0s - loss: 0.5438 - acc: 0.6750 - val_loss: 0.5494 - val_acc: 0.6167\n",
      "Epoch 57/170\n",
      " - 0s - loss: 0.5404 - acc: 0.6750 - val_loss: 0.5460 - val_acc: 0.6167\n",
      "Epoch 58/170\n",
      " - 0s - loss: 0.5371 - acc: 0.6750 - val_loss: 0.5427 - val_acc: 0.6167\n",
      "Epoch 59/170\n",
      " - 0s - loss: 0.5341 - acc: 0.6750 - val_loss: 0.5391 - val_acc: 0.6333\n",
      "Epoch 60/170\n",
      " - 0s - loss: 0.5308 - acc: 0.6750 - val_loss: 0.5357 - val_acc: 0.6333\n",
      "Epoch 61/170\n",
      " - 0s - loss: 0.5277 - acc: 0.6750 - val_loss: 0.5325 - val_acc: 0.6333\n",
      "Epoch 62/170\n",
      " - 0s - loss: 0.5246 - acc: 0.7000 - val_loss: 0.5293 - val_acc: 0.6333\n",
      "Epoch 63/170\n",
      " - 0s - loss: 0.5216 - acc: 0.7000 - val_loss: 0.5259 - val_acc: 0.6500\n",
      "Epoch 64/170\n",
      " - 0s - loss: 0.5184 - acc: 0.7000 - val_loss: 0.5229 - val_acc: 0.6667\n",
      "Epoch 65/170\n",
      " - 0s - loss: 0.5154 - acc: 0.7000 - val_loss: 0.5196 - val_acc: 0.7000\n",
      "Epoch 66/170\n",
      " - 0s - loss: 0.5123 - acc: 0.7000 - val_loss: 0.5164 - val_acc: 0.7000\n",
      "Epoch 67/170\n",
      " - 0s - loss: 0.5094 - acc: 0.7250 - val_loss: 0.5132 - val_acc: 0.7000\n",
      "Epoch 68/170\n",
      " - 0s - loss: 0.5066 - acc: 0.7250 - val_loss: 0.5100 - val_acc: 0.7167\n",
      "Epoch 69/170\n",
      " - 0s - loss: 0.5035 - acc: 0.7250 - val_loss: 0.5068 - val_acc: 0.7167\n",
      "Epoch 70/170\n",
      " - 0s - loss: 0.5006 - acc: 0.7250 - val_loss: 0.5039 - val_acc: 0.7333\n",
      "Epoch 71/170\n",
      " - 0s - loss: 0.4979 - acc: 0.7500 - val_loss: 0.5006 - val_acc: 0.8000\n",
      "Epoch 72/170\n",
      " - 0s - loss: 0.4949 - acc: 0.7500 - val_loss: 0.4977 - val_acc: 0.8167\n",
      "Epoch 73/170\n",
      " - 0s - loss: 0.4921 - acc: 0.7500 - val_loss: 0.4946 - val_acc: 0.8167\n",
      "Epoch 74/170\n",
      " - 0s - loss: 0.4892 - acc: 0.7750 - val_loss: 0.4918 - val_acc: 0.8167\n",
      "Epoch 75/170\n",
      " - 0s - loss: 0.4865 - acc: 0.7750 - val_loss: 0.4888 - val_acc: 0.8333\n",
      "Epoch 76/170\n",
      " - 0s - loss: 0.4837 - acc: 0.7750 - val_loss: 0.4859 - val_acc: 0.8333\n",
      "Epoch 77/170\n",
      " - 0s - loss: 0.4813 - acc: 0.7750 - val_loss: 0.4828 - val_acc: 0.8500\n",
      "Epoch 78/170\n",
      " - 0s - loss: 0.4783 - acc: 0.8000 - val_loss: 0.4800 - val_acc: 0.8500\n",
      "Epoch 79/170\n",
      " - 0s - loss: 0.4755 - acc: 0.8000 - val_loss: 0.4775 - val_acc: 0.8500\n",
      "Epoch 80/170\n",
      " - 0s - loss: 0.4729 - acc: 0.8000 - val_loss: 0.4748 - val_acc: 0.8500\n",
      "Epoch 81/170\n",
      " - 0s - loss: 0.4703 - acc: 0.8000 - val_loss: 0.4720 - val_acc: 0.8667\n",
      "Epoch 82/170\n",
      " - 0s - loss: 0.4676 - acc: 0.8250 - val_loss: 0.4694 - val_acc: 0.8667\n",
      "Epoch 83/170\n",
      " - 0s - loss: 0.4652 - acc: 0.8250 - val_loss: 0.4666 - val_acc: 0.8667\n",
      "Epoch 84/170\n",
      " - 0s - loss: 0.4625 - acc: 0.8250 - val_loss: 0.4643 - val_acc: 0.8667\n",
      "Epoch 85/170\n",
      " - 0s - loss: 0.4599 - acc: 0.8250 - val_loss: 0.4618 - val_acc: 0.8667\n",
      "Epoch 86/170\n",
      " - 0s - loss: 0.4574 - acc: 0.8250 - val_loss: 0.4590 - val_acc: 0.8667\n",
      "Epoch 87/170\n",
      " - 0s - loss: 0.4550 - acc: 0.8250 - val_loss: 0.4563 - val_acc: 0.9000\n",
      "Epoch 88/170\n",
      " - 0s - loss: 0.4524 - acc: 0.8250 - val_loss: 0.4537 - val_acc: 0.9000\n",
      "Epoch 89/170\n",
      " - 0s - loss: 0.4499 - acc: 0.8250 - val_loss: 0.4512 - val_acc: 0.9000\n",
      "Epoch 90/170\n",
      " - 0s - loss: 0.4475 - acc: 0.8250 - val_loss: 0.4488 - val_acc: 0.9000\n",
      "Epoch 91/170\n",
      " - 0s - loss: 0.4453 - acc: 0.8250 - val_loss: 0.4460 - val_acc: 0.9000\n",
      "Epoch 92/170\n",
      " - 0s - loss: 0.4428 - acc: 0.8250 - val_loss: 0.4434 - val_acc: 0.9000\n",
      "Epoch 93/170\n",
      " - 0s - loss: 0.4402 - acc: 0.8250 - val_loss: 0.4411 - val_acc: 0.9000\n",
      "Epoch 94/170\n",
      " - 0s - loss: 0.4379 - acc: 0.8500 - val_loss: 0.4386 - val_acc: 0.9333\n",
      "Epoch 95/170\n",
      " - 0s - loss: 0.4356 - acc: 0.8750 - val_loss: 0.4362 - val_acc: 0.9500\n",
      "Epoch 96/170\n",
      " - 0s - loss: 0.4332 - acc: 0.8750 - val_loss: 0.4340 - val_acc: 0.9500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/170\n",
      " - 0s - loss: 0.4310 - acc: 0.8750 - val_loss: 0.4319 - val_acc: 0.9500\n",
      "Epoch 98/170\n",
      " - 0s - loss: 0.4288 - acc: 0.8750 - val_loss: 0.4295 - val_acc: 0.9500\n",
      "Epoch 99/170\n",
      " - 0s - loss: 0.4265 - acc: 0.9000 - val_loss: 0.4271 - val_acc: 0.9500\n",
      "Epoch 100/170\n",
      " - 0s - loss: 0.4243 - acc: 0.9250 - val_loss: 0.4247 - val_acc: 0.9500\n",
      "Epoch 101/170\n",
      " - 0s - loss: 0.4219 - acc: 0.9500 - val_loss: 0.4225 - val_acc: 0.9500\n",
      "Epoch 102/170\n",
      " - 0s - loss: 0.4198 - acc: 0.9500 - val_loss: 0.4202 - val_acc: 0.9500\n",
      "Epoch 103/170\n",
      " - 0s - loss: 0.4176 - acc: 0.9500 - val_loss: 0.4179 - val_acc: 0.9500\n",
      "Epoch 104/170\n",
      " - 0s - loss: 0.4154 - acc: 0.9500 - val_loss: 0.4157 - val_acc: 0.9500\n",
      "Epoch 105/170\n",
      " - 0s - loss: 0.4132 - acc: 0.9500 - val_loss: 0.4136 - val_acc: 0.9667\n",
      "Epoch 106/170\n",
      " - 0s - loss: 0.4111 - acc: 0.9500 - val_loss: 0.4115 - val_acc: 0.9667\n",
      "Epoch 107/170\n",
      " - 0s - loss: 0.4089 - acc: 0.9500 - val_loss: 0.4094 - val_acc: 0.9667\n",
      "Epoch 108/170\n",
      " - 0s - loss: 0.4069 - acc: 0.9500 - val_loss: 0.4074 - val_acc: 0.9667\n",
      "Epoch 109/170\n",
      " - 0s - loss: 0.4048 - acc: 0.9750 - val_loss: 0.4054 - val_acc: 0.9667\n",
      "Epoch 110/170\n",
      " - 0s - loss: 0.4027 - acc: 0.9750 - val_loss: 0.4032 - val_acc: 0.9667\n",
      "Epoch 111/170\n",
      " - 0s - loss: 0.4006 - acc: 0.9750 - val_loss: 0.4012 - val_acc: 0.9667\n",
      "Epoch 112/170\n",
      " - 0s - loss: 0.3986 - acc: 0.9750 - val_loss: 0.3991 - val_acc: 0.9833\n",
      "Epoch 113/170\n",
      " - 0s - loss: 0.3965 - acc: 0.9750 - val_loss: 0.3969 - val_acc: 0.9833\n",
      "Epoch 114/170\n",
      " - 0s - loss: 0.3945 - acc: 0.9750 - val_loss: 0.3949 - val_acc: 0.9833\n",
      "Epoch 115/170\n",
      " - 0s - loss: 0.3925 - acc: 0.9750 - val_loss: 0.3928 - val_acc: 0.9833\n",
      "Epoch 116/170\n",
      " - 0s - loss: 0.3905 - acc: 0.9750 - val_loss: 0.3907 - val_acc: 0.9833\n",
      "Epoch 117/170\n",
      " - 0s - loss: 0.3886 - acc: 0.9750 - val_loss: 0.3885 - val_acc: 0.9833\n",
      "Epoch 118/170\n",
      " - 0s - loss: 0.3865 - acc: 0.9750 - val_loss: 0.3865 - val_acc: 0.9833\n",
      "Epoch 119/170\n",
      " - 0s - loss: 0.3846 - acc: 0.9750 - val_loss: 0.3845 - val_acc: 0.9833\n",
      "Epoch 120/170\n",
      " - 0s - loss: 0.3826 - acc: 0.9750 - val_loss: 0.3827 - val_acc: 0.9833\n",
      "Epoch 121/170\n",
      " - 0s - loss: 0.3807 - acc: 0.9750 - val_loss: 0.3807 - val_acc: 0.9833\n",
      "Epoch 122/170\n",
      " - 0s - loss: 0.3788 - acc: 0.9750 - val_loss: 0.3787 - val_acc: 0.9833\n",
      "Epoch 123/170\n",
      " - 0s - loss: 0.3769 - acc: 1.0000 - val_loss: 0.3768 - val_acc: 0.9833\n",
      "Epoch 124/170\n",
      " - 0s - loss: 0.3752 - acc: 1.0000 - val_loss: 0.3747 - val_acc: 0.9833\n",
      "Epoch 125/170\n",
      " - 0s - loss: 0.3731 - acc: 1.0000 - val_loss: 0.3729 - val_acc: 0.9833\n",
      "Epoch 126/170\n",
      " - 0s - loss: 0.3713 - acc: 1.0000 - val_loss: 0.3709 - val_acc: 0.9833\n",
      "Epoch 127/170\n",
      " - 0s - loss: 0.3694 - acc: 1.0000 - val_loss: 0.3691 - val_acc: 0.9833\n",
      "Epoch 128/170\n",
      " - 0s - loss: 0.3676 - acc: 1.0000 - val_loss: 0.3672 - val_acc: 0.9833\n",
      "Epoch 129/170\n",
      " - 0s - loss: 0.3658 - acc: 1.0000 - val_loss: 0.3653 - val_acc: 0.9833\n",
      "Epoch 130/170\n",
      " - 0s - loss: 0.3639 - acc: 1.0000 - val_loss: 0.3637 - val_acc: 0.9833\n",
      "Epoch 131/170\n",
      " - 0s - loss: 0.3623 - acc: 1.0000 - val_loss: 0.3617 - val_acc: 0.9833\n",
      "Epoch 132/170\n",
      " - 0s - loss: 0.3604 - acc: 1.0000 - val_loss: 0.3602 - val_acc: 0.9833\n",
      "Epoch 133/170\n",
      " - 0s - loss: 0.3586 - acc: 1.0000 - val_loss: 0.3583 - val_acc: 0.9833\n",
      "Epoch 134/170\n",
      " - 0s - loss: 0.3570 - acc: 1.0000 - val_loss: 0.3563 - val_acc: 0.9833\n",
      "Epoch 135/170\n",
      " - 0s - loss: 0.3551 - acc: 1.0000 - val_loss: 0.3546 - val_acc: 0.9833\n",
      "Epoch 136/170\n",
      " - 0s - loss: 0.3533 - acc: 1.0000 - val_loss: 0.3529 - val_acc: 0.9833\n",
      "Epoch 137/170\n",
      " - 0s - loss: 0.3516 - acc: 1.0000 - val_loss: 0.3513 - val_acc: 0.9833\n",
      "Epoch 138/170\n",
      " - 0s - loss: 0.3499 - acc: 1.0000 - val_loss: 0.3497 - val_acc: 0.9833\n",
      "Epoch 139/170\n",
      " - 0s - loss: 0.3483 - acc: 1.0000 - val_loss: 0.3479 - val_acc: 0.9833\n",
      "Epoch 140/170\n",
      " - 0s - loss: 0.3465 - acc: 1.0000 - val_loss: 0.3461 - val_acc: 0.9833\n",
      "Epoch 141/170\n",
      " - 0s - loss: 0.3449 - acc: 1.0000 - val_loss: 0.3446 - val_acc: 0.9833\n",
      "Epoch 142/170\n",
      " - 0s - loss: 0.3432 - acc: 1.0000 - val_loss: 0.3430 - val_acc: 0.9833\n",
      "Epoch 143/170\n",
      " - 0s - loss: 0.3415 - acc: 1.0000 - val_loss: 0.3414 - val_acc: 0.9833\n",
      "Epoch 144/170\n",
      " - 0s - loss: 0.3399 - acc: 1.0000 - val_loss: 0.3397 - val_acc: 0.9833\n",
      "Epoch 145/170\n",
      " - 0s - loss: 0.3383 - acc: 1.0000 - val_loss: 0.3380 - val_acc: 0.9833\n",
      "Epoch 146/170\n",
      " - 0s - loss: 0.3366 - acc: 1.0000 - val_loss: 0.3364 - val_acc: 0.9833\n",
      "Epoch 147/170\n",
      " - 0s - loss: 0.3352 - acc: 1.0000 - val_loss: 0.3346 - val_acc: 0.9833\n",
      "Epoch 148/170\n",
      " - 0s - loss: 0.3335 - acc: 1.0000 - val_loss: 0.3331 - val_acc: 0.9833\n",
      "Epoch 149/170\n",
      " - 0s - loss: 0.3318 - acc: 1.0000 - val_loss: 0.3315 - val_acc: 0.9833\n",
      "Epoch 150/170\n",
      " - 0s - loss: 0.3303 - acc: 1.0000 - val_loss: 0.3298 - val_acc: 0.9833\n",
      "Epoch 151/170\n",
      " - 0s - loss: 0.3287 - acc: 1.0000 - val_loss: 0.3282 - val_acc: 0.9833\n",
      "Epoch 152/170\n",
      " - 0s - loss: 0.3272 - acc: 1.0000 - val_loss: 0.3266 - val_acc: 0.9833\n",
      "Epoch 153/170\n",
      " - 0s - loss: 0.3256 - acc: 1.0000 - val_loss: 0.3250 - val_acc: 0.9833\n",
      "Epoch 154/170\n",
      " - 0s - loss: 0.3240 - acc: 1.0000 - val_loss: 0.3235 - val_acc: 0.9833\n",
      "Epoch 155/170\n",
      " - 0s - loss: 0.3225 - acc: 1.0000 - val_loss: 0.3220 - val_acc: 0.9833\n",
      "Epoch 156/170\n",
      " - 0s - loss: 0.3210 - acc: 1.0000 - val_loss: 0.3207 - val_acc: 0.9833\n",
      "Epoch 157/170\n",
      " - 0s - loss: 0.3195 - acc: 1.0000 - val_loss: 0.3192 - val_acc: 0.9833\n",
      "Epoch 158/170\n",
      " - 0s - loss: 0.3180 - acc: 1.0000 - val_loss: 0.3176 - val_acc: 0.9833\n",
      "Epoch 159/170\n",
      " - 0s - loss: 0.3166 - acc: 1.0000 - val_loss: 0.3161 - val_acc: 0.9833\n",
      "Epoch 160/170\n",
      " - 0s - loss: 0.3151 - acc: 1.0000 - val_loss: 0.3145 - val_acc: 0.9833\n",
      "Epoch 161/170\n",
      " - 0s - loss: 0.3136 - acc: 1.0000 - val_loss: 0.3131 - val_acc: 0.9833\n",
      "Epoch 162/170\n",
      " - 0s - loss: 0.3121 - acc: 1.0000 - val_loss: 0.3117 - val_acc: 0.9833\n",
      "Epoch 163/170\n",
      " - 0s - loss: 0.3108 - acc: 1.0000 - val_loss: 0.3101 - val_acc: 0.9833\n",
      "Epoch 164/170\n",
      " - 0s - loss: 0.3093 - acc: 1.0000 - val_loss: 0.3088 - val_acc: 0.9833\n",
      "Epoch 165/170\n",
      " - 0s - loss: 0.3078 - acc: 1.0000 - val_loss: 0.3074 - val_acc: 0.9833\n",
      "Epoch 166/170\n",
      " - 0s - loss: 0.3064 - acc: 1.0000 - val_loss: 0.3059 - val_acc: 0.9833\n",
      "Epoch 167/170\n",
      " - 0s - loss: 0.3051 - acc: 1.0000 - val_loss: 0.3045 - val_acc: 0.9833\n",
      "Epoch 168/170\n",
      " - 0s - loss: 0.3036 - acc: 1.0000 - val_loss: 0.3031 - val_acc: 0.9833\n",
      "Epoch 169/170\n",
      " - 0s - loss: 0.3022 - acc: 1.0000 - val_loss: 0.3017 - val_acc: 0.9833\n",
      "Epoch 170/170\n",
      " - 0s - loss: 0.3008 - acc: 1.0000 - val_loss: 0.3003 - val_acc: 0.9833\n",
      "Test accuracy: 0.9833333412806193\n",
      "Train on 40 samples, validate on 60 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 0.6025 - acc: 0.5500 - val_loss: 0.5472 - val_acc: 0.5333\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.5933 - acc: 0.5500 - val_loss: 0.5400 - val_acc: 0.5333\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.5852 - acc: 0.5500 - val_loss: 0.5326 - val_acc: 0.5333\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.5768 - acc: 0.5500 - val_loss: 0.5252 - val_acc: 0.5333\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.5678 - acc: 0.5500 - val_loss: 0.5178 - val_acc: 0.5333\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.5597 - acc: 0.5500 - val_loss: 0.5105 - val_acc: 0.5333\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.5506 - acc: 0.5500 - val_loss: 0.5033 - val_acc: 0.5333\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.5427 - acc: 0.5750 - val_loss: 0.4962 - val_acc: 0.5833\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.5344 - acc: 0.5750 - val_loss: 0.4892 - val_acc: 0.5833\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.5265 - acc: 0.5750 - val_loss: 0.4824 - val_acc: 0.6000\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.5184 - acc: 0.5750 - val_loss: 0.4758 - val_acc: 0.6000\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.5101 - acc: 0.5750 - val_loss: 0.4693 - val_acc: 0.6333\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.5031 - acc: 0.5750 - val_loss: 0.4628 - val_acc: 0.6500\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.4960 - acc: 0.6000 - val_loss: 0.4566 - val_acc: 0.6500\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.4880 - acc: 0.6750 - val_loss: 0.4507 - val_acc: 0.6500\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.4813 - acc: 0.7000 - val_loss: 0.4449 - val_acc: 0.7000\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.4752 - acc: 0.7000 - val_loss: 0.4392 - val_acc: 0.7000\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.4677 - acc: 0.7000 - val_loss: 0.4339 - val_acc: 0.7000\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.4619 - acc: 0.7000 - val_loss: 0.4287 - val_acc: 0.7167\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.4552 - acc: 0.7250 - val_loss: 0.4236 - val_acc: 0.7167\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.4497 - acc: 0.7250 - val_loss: 0.4186 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100\n",
      " - 0s - loss: 0.4437 - acc: 0.7250 - val_loss: 0.4138 - val_acc: 0.7500\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.4379 - acc: 0.7250 - val_loss: 0.4092 - val_acc: 0.7500\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.4325 - acc: 0.7250 - val_loss: 0.4047 - val_acc: 0.7500\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.4273 - acc: 0.7250 - val_loss: 0.4005 - val_acc: 0.8167\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.4222 - acc: 0.7250 - val_loss: 0.3964 - val_acc: 0.8167\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.4166 - acc: 0.7250 - val_loss: 0.3924 - val_acc: 0.8167\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.4127 - acc: 0.7500 - val_loss: 0.3885 - val_acc: 0.8167\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.4075 - acc: 0.7500 - val_loss: 0.3848 - val_acc: 0.8167\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.4030 - acc: 0.7500 - val_loss: 0.3812 - val_acc: 0.8500\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.3987 - acc: 0.8250 - val_loss: 0.3776 - val_acc: 0.8667\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.3944 - acc: 0.8750 - val_loss: 0.3742 - val_acc: 0.9000\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.3903 - acc: 0.8750 - val_loss: 0.3710 - val_acc: 0.9000\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.3863 - acc: 0.8750 - val_loss: 0.3679 - val_acc: 0.9333\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.3824 - acc: 0.8750 - val_loss: 0.3649 - val_acc: 0.9500\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.3784 - acc: 0.9250 - val_loss: 0.3620 - val_acc: 0.9833\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.3749 - acc: 0.9250 - val_loss: 0.3591 - val_acc: 0.9833\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.3716 - acc: 0.9250 - val_loss: 0.3563 - val_acc: 0.9833\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.3682 - acc: 0.9500 - val_loss: 0.3537 - val_acc: 0.9833\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.3646 - acc: 0.9500 - val_loss: 0.3512 - val_acc: 0.9833\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.3614 - acc: 0.9500 - val_loss: 0.3488 - val_acc: 0.9833\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.3586 - acc: 0.9500 - val_loss: 0.3465 - val_acc: 0.9833\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.3558 - acc: 0.9500 - val_loss: 0.3443 - val_acc: 0.9833\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.3527 - acc: 0.9500 - val_loss: 0.3423 - val_acc: 0.9833\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.3500 - acc: 0.9500 - val_loss: 0.3402 - val_acc: 0.9833\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.3473 - acc: 0.9500 - val_loss: 0.3383 - val_acc: 0.9833\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.3447 - acc: 0.9500 - val_loss: 0.3363 - val_acc: 0.9833\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.3427 - acc: 0.9500 - val_loss: 0.3344 - val_acc: 0.9833\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.3396 - acc: 0.9750 - val_loss: 0.3327 - val_acc: 0.9833\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.3376 - acc: 0.9750 - val_loss: 0.3310 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.3353 - acc: 0.9750 - val_loss: 0.3294 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.3330 - acc: 0.9750 - val_loss: 0.3278 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.3312 - acc: 0.9750 - val_loss: 0.3263 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.3289 - acc: 1.0000 - val_loss: 0.3249 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.3271 - acc: 1.0000 - val_loss: 0.3236 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.3251 - acc: 1.0000 - val_loss: 0.3223 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.3234 - acc: 1.0000 - val_loss: 0.3210 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.3216 - acc: 1.0000 - val_loss: 0.3198 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.3201 - acc: 1.0000 - val_loss: 0.3186 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.3184 - acc: 1.0000 - val_loss: 0.3175 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.3167 - acc: 1.0000 - val_loss: 0.3165 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.3155 - acc: 1.0000 - val_loss: 0.3155 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.3137 - acc: 1.0000 - val_loss: 0.3146 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.3125 - acc: 1.0000 - val_loss: 0.3136 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.3112 - acc: 1.0000 - val_loss: 0.3127 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.3098 - acc: 1.0000 - val_loss: 0.3119 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.3085 - acc: 1.0000 - val_loss: 0.3110 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.3073 - acc: 1.0000 - val_loss: 0.3102 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.3061 - acc: 1.0000 - val_loss: 0.3094 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.3048 - acc: 1.0000 - val_loss: 0.3086 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.3037 - acc: 1.0000 - val_loss: 0.3079 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.3025 - acc: 1.0000 - val_loss: 0.3072 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.3015 - acc: 1.0000 - val_loss: 0.3065 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.3003 - acc: 1.0000 - val_loss: 0.3059 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.2994 - acc: 1.0000 - val_loss: 0.3052 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.2985 - acc: 1.0000 - val_loss: 0.3047 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.2976 - acc: 1.0000 - val_loss: 0.3041 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.2967 - acc: 1.0000 - val_loss: 0.3036 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.2957 - acc: 1.0000 - val_loss: 0.3031 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.2951 - acc: 1.0000 - val_loss: 0.3026 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.2944 - acc: 1.0000 - val_loss: 0.3021 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.2935 - acc: 1.0000 - val_loss: 0.3016 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.2929 - acc: 1.0000 - val_loss: 0.3012 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.2921 - acc: 1.0000 - val_loss: 0.3007 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.2915 - acc: 1.0000 - val_loss: 0.3003 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.2908 - acc: 1.0000 - val_loss: 0.2998 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.2901 - acc: 1.0000 - val_loss: 0.2994 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.2895 - acc: 1.0000 - val_loss: 0.2990 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.2889 - acc: 1.0000 - val_loss: 0.2986 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.2884 - acc: 1.0000 - val_loss: 0.2982 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.2877 - acc: 1.0000 - val_loss: 0.2978 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.2871 - acc: 1.0000 - val_loss: 0.2974 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.2866 - acc: 1.0000 - val_loss: 0.2970 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.2860 - acc: 1.0000 - val_loss: 0.2966 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.2855 - acc: 1.0000 - val_loss: 0.2962 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.2849 - acc: 1.0000 - val_loss: 0.2958 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.2844 - acc: 1.0000 - val_loss: 0.2954 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.2839 - acc: 1.0000 - val_loss: 0.2951 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.2833 - acc: 1.0000 - val_loss: 0.2947 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.2828 - acc: 1.0000 - val_loss: 0.2943 - val_acc: 1.0000\n",
      "Test accuracy: 1.0\n",
      "Train on 40 samples, validate on 60 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 1.3511 - acc: 0.0000e+00 - val_loss: 1.3797 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      " - 0s - loss: 1.3415 - acc: 0.0000e+00 - val_loss: 1.3728 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      " - 0s - loss: 1.3321 - acc: 0.0000e+00 - val_loss: 1.3651 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.3223 - acc: 0.0250 - val_loss: 1.3571 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      " - 0s - loss: 1.3115 - acc: 0.0250 - val_loss: 1.3488 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      " - 0s - loss: 1.3020 - acc: 0.0250 - val_loss: 1.3407 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      " - 0s - loss: 1.2903 - acc: 0.0250 - val_loss: 1.3323 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      " - 0s - loss: 1.2808 - acc: 0.0250 - val_loss: 1.3238 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      " - 0s - loss: 1.2702 - acc: 0.0250 - val_loss: 1.3153 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      " - 0s - loss: 1.2614 - acc: 0.0250 - val_loss: 1.3070 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      " - 0s - loss: 1.2522 - acc: 0.0750 - val_loss: 1.2989 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      " - 0s - loss: 1.2427 - acc: 0.0500 - val_loss: 1.2902 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      " - 0s - loss: 1.2316 - acc: 0.0750 - val_loss: 1.2818 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      " - 0s - loss: 1.2253 - acc: 0.0750 - val_loss: 1.2739 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      " - 0s - loss: 1.2142 - acc: 0.0750 - val_loss: 1.2657 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      " - 0s - loss: 1.2061 - acc: 0.0750 - val_loss: 1.2573 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      " - 0s - loss: 1.1967 - acc: 0.0750 - val_loss: 1.2488 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      " - 0s - loss: 1.1883 - acc: 0.0750 - val_loss: 1.2404 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      " - 0s - loss: 1.1788 - acc: 0.1000 - val_loss: 1.2319 - val_acc: 0.0167\n",
      "Epoch 20/100\n",
      " - 0s - loss: 1.1717 - acc: 0.0750 - val_loss: 1.2234 - val_acc: 0.0167\n",
      "Epoch 21/100\n",
      " - 0s - loss: 1.1620 - acc: 0.1000 - val_loss: 1.2151 - val_acc: 0.0333\n",
      "Epoch 22/100\n",
      " - 0s - loss: 1.1535 - acc: 0.1000 - val_loss: 1.2066 - val_acc: 0.0500\n",
      "Epoch 23/100\n",
      " - 0s - loss: 1.1465 - acc: 0.1250 - val_loss: 1.1988 - val_acc: 0.0667\n",
      "Epoch 24/100\n",
      " - 0s - loss: 1.1362 - acc: 0.1250 - val_loss: 1.1905 - val_acc: 0.0833\n",
      "Epoch 25/100\n",
      " - 0s - loss: 1.1277 - acc: 0.1250 - val_loss: 1.1823 - val_acc: 0.1000\n",
      "Epoch 26/100\n",
      " - 0s - loss: 1.1202 - acc: 0.1250 - val_loss: 1.1739 - val_acc: 0.1333\n",
      "Epoch 27/100\n",
      " - 0s - loss: 1.1120 - acc: 0.1250 - val_loss: 1.1655 - val_acc: 0.1333\n",
      "Epoch 28/100\n",
      " - 0s - loss: 1.1048 - acc: 0.1250 - val_loss: 1.1569 - val_acc: 0.1333\n",
      "Epoch 29/100\n",
      " - 0s - loss: 1.0952 - acc: 0.1250 - val_loss: 1.1485 - val_acc: 0.1333\n",
      "Epoch 30/100\n",
      " - 0s - loss: 1.0867 - acc: 0.1250 - val_loss: 1.1402 - val_acc: 0.1333\n",
      "Epoch 31/100\n",
      " - 0s - loss: 1.0803 - acc: 0.1750 - val_loss: 1.1323 - val_acc: 0.1667\n",
      "Epoch 32/100\n",
      " - 0s - loss: 1.0716 - acc: 0.2000 - val_loss: 1.1243 - val_acc: 0.1667\n",
      "Epoch 33/100\n",
      " - 0s - loss: 1.0628 - acc: 0.2000 - val_loss: 1.1158 - val_acc: 0.1667\n",
      "Epoch 34/100\n",
      " - 0s - loss: 1.0558 - acc: 0.2000 - val_loss: 1.1073 - val_acc: 0.1833\n",
      "Epoch 35/100\n",
      " - 0s - loss: 1.0486 - acc: 0.2250 - val_loss: 1.0992 - val_acc: 0.2500\n",
      "Epoch 36/100\n",
      " - 0s - loss: 1.0402 - acc: 0.2250 - val_loss: 1.0909 - val_acc: 0.2667\n",
      "Epoch 37/100\n",
      " - 0s - loss: 1.0335 - acc: 0.2250 - val_loss: 1.0830 - val_acc: 0.2667\n",
      "Epoch 38/100\n",
      " - 0s - loss: 1.0239 - acc: 0.2750 - val_loss: 1.0751 - val_acc: 0.2667\n",
      "Epoch 39/100\n",
      " - 0s - loss: 1.0177 - acc: 0.2500 - val_loss: 1.0670 - val_acc: 0.2667\n",
      "Epoch 40/100\n",
      " - 0s - loss: 1.0093 - acc: 0.3250 - val_loss: 1.0593 - val_acc: 0.3167\n",
      "Epoch 41/100\n",
      " - 0s - loss: 1.0015 - acc: 0.3500 - val_loss: 1.0515 - val_acc: 0.3333\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.9939 - acc: 0.3500 - val_loss: 1.0437 - val_acc: 0.3500\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.9875 - acc: 0.3500 - val_loss: 1.0357 - val_acc: 0.3667\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.9783 - acc: 0.3750 - val_loss: 1.0281 - val_acc: 0.3833\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.9713 - acc: 0.3750 - val_loss: 1.0202 - val_acc: 0.4000\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.9650 - acc: 0.4250 - val_loss: 1.0127 - val_acc: 0.4000\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.9567 - acc: 0.4500 - val_loss: 1.0052 - val_acc: 0.4000\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.9504 - acc: 0.4500 - val_loss: 0.9977 - val_acc: 0.4000\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.9422 - acc: 0.4500 - val_loss: 0.9900 - val_acc: 0.4000\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.9351 - acc: 0.4750 - val_loss: 0.9824 - val_acc: 0.4000\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.9279 - acc: 0.4750 - val_loss: 0.9749 - val_acc: 0.4167\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.9212 - acc: 0.4750 - val_loss: 0.9673 - val_acc: 0.4167\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.9142 - acc: 0.4750 - val_loss: 0.9598 - val_acc: 0.4167\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.9070 - acc: 0.4750 - val_loss: 0.9522 - val_acc: 0.4167\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.8993 - acc: 0.4750 - val_loss: 0.9450 - val_acc: 0.4167\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.8924 - acc: 0.5250 - val_loss: 0.9378 - val_acc: 0.4167\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.8865 - acc: 0.5250 - val_loss: 0.9305 - val_acc: 0.4333\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.8787 - acc: 0.5250 - val_loss: 0.9232 - val_acc: 0.4333\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.8721 - acc: 0.5250 - val_loss: 0.9158 - val_acc: 0.4333\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.8658 - acc: 0.5250 - val_loss: 0.9086 - val_acc: 0.4333\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.8598 - acc: 0.5250 - val_loss: 0.9016 - val_acc: 0.4500\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.8520 - acc: 0.5250 - val_loss: 0.8945 - val_acc: 0.4500\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.8464 - acc: 0.5250 - val_loss: 0.8878 - val_acc: 0.4500\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.8401 - acc: 0.5500 - val_loss: 0.8808 - val_acc: 0.4667\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.8324 - acc: 0.5500 - val_loss: 0.8737 - val_acc: 0.4667\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.8254 - acc: 0.5500 - val_loss: 0.8669 - val_acc: 0.4667\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.8198 - acc: 0.5500 - val_loss: 0.8599 - val_acc: 0.4667\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.8124 - acc: 0.5500 - val_loss: 0.8532 - val_acc: 0.4667\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.8065 - acc: 0.5500 - val_loss: 0.8464 - val_acc: 0.4667\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.7997 - acc: 0.5500 - val_loss: 0.8399 - val_acc: 0.4667\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.7936 - acc: 0.5500 - val_loss: 0.8334 - val_acc: 0.4667\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.7873 - acc: 0.5500 - val_loss: 0.8268 - val_acc: 0.4667\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.7822 - acc: 0.5500 - val_loss: 0.8204 - val_acc: 0.4667\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.7751 - acc: 0.5500 - val_loss: 0.8138 - val_acc: 0.4667\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.7686 - acc: 0.5500 - val_loss: 0.8072 - val_acc: 0.4667\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.7634 - acc: 0.5500 - val_loss: 0.8006 - val_acc: 0.4667\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.7574 - acc: 0.5500 - val_loss: 0.7942 - val_acc: 0.4667\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.7508 - acc: 0.5500 - val_loss: 0.7879 - val_acc: 0.4667\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.7449 - acc: 0.5500 - val_loss: 0.7816 - val_acc: 0.4667\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.7389 - acc: 0.5500 - val_loss: 0.7754 - val_acc: 0.4667\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.7343 - acc: 0.5500 - val_loss: 0.7695 - val_acc: 0.4667\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.7273 - acc: 0.5500 - val_loss: 0.7631 - val_acc: 0.4667\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.7229 - acc: 0.5500 - val_loss: 0.7572 - val_acc: 0.4667\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.7160 - acc: 0.5500 - val_loss: 0.7511 - val_acc: 0.4667\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.7099 - acc: 0.5500 - val_loss: 0.7451 - val_acc: 0.4667\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.7055 - acc: 0.5500 - val_loss: 0.7389 - val_acc: 0.4667\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.6987 - acc: 0.5500 - val_loss: 0.7331 - val_acc: 0.4667\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.6940 - acc: 0.5500 - val_loss: 0.7272 - val_acc: 0.4667\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.6885 - acc: 0.5500 - val_loss: 0.7214 - val_acc: 0.4667\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.6831 - acc: 0.5500 - val_loss: 0.7157 - val_acc: 0.4667\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.6774 - acc: 0.5500 - val_loss: 0.7100 - val_acc: 0.4667\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.6723 - acc: 0.5500 - val_loss: 0.7044 - val_acc: 0.4667\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.6673 - acc: 0.5500 - val_loss: 0.6989 - val_acc: 0.4667\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.6610 - acc: 0.5500 - val_loss: 0.6936 - val_acc: 0.4667\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.6562 - acc: 0.5500 - val_loss: 0.6882 - val_acc: 0.4667\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.6507 - acc: 0.5500 - val_loss: 0.6829 - val_acc: 0.4667\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.6457 - acc: 0.5500 - val_loss: 0.6775 - val_acc: 0.4667\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.6412 - acc: 0.5500 - val_loss: 0.6723 - val_acc: 0.4667\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.6355 - acc: 0.5500 - val_loss: 0.6670 - val_acc: 0.4667\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.6313 - acc: 0.5500 - val_loss: 0.6618 - val_acc: 0.4667\n",
      "Test accuracy: 0.4666666626930237\n",
      "Train on 40 samples, validate on 60 samples\n",
      "Epoch 1/170\n",
      " - 0s - loss: 2.5387 - acc: 0.5500 - val_loss: 2.9085 - val_acc: 0.4667\n",
      "Epoch 2/170\n",
      " - 0s - loss: 2.5212 - acc: 0.5500 - val_loss: 2.8939 - val_acc: 0.4667\n",
      "Epoch 3/170\n",
      " - 0s - loss: 2.5081 - acc: 0.5500 - val_loss: 2.8771 - val_acc: 0.4667\n",
      "Epoch 4/170\n",
      " - 0s - loss: 2.4935 - acc: 0.5500 - val_loss: 2.8592 - val_acc: 0.4667\n",
      "Epoch 5/170\n",
      " - 0s - loss: 2.4774 - acc: 0.5500 - val_loss: 2.8392 - val_acc: 0.4667\n",
      "Epoch 6/170\n",
      " - 0s - loss: 2.4605 - acc: 0.5500 - val_loss: 2.8196 - val_acc: 0.4667\n",
      "Epoch 7/170\n",
      " - 0s - loss: 2.4444 - acc: 0.5500 - val_loss: 2.8016 - val_acc: 0.4667\n",
      "Epoch 8/170\n",
      " - 0s - loss: 2.4270 - acc: 0.5500 - val_loss: 2.7791 - val_acc: 0.4667\n",
      "Epoch 9/170\n",
      " - 0s - loss: 2.4093 - acc: 0.5500 - val_loss: 2.7602 - val_acc: 0.4667\n",
      "Epoch 10/170\n",
      " - 0s - loss: 2.3920 - acc: 0.5500 - val_loss: 2.7392 - val_acc: 0.4667\n",
      "Epoch 11/170\n",
      " - 0s - loss: 2.3736 - acc: 0.5500 - val_loss: 2.7176 - val_acc: 0.4667\n",
      "Epoch 12/170\n",
      " - 0s - loss: 2.3560 - acc: 0.5500 - val_loss: 2.6978 - val_acc: 0.4667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/170\n",
      " - 0s - loss: 2.3368 - acc: 0.5500 - val_loss: 2.6734 - val_acc: 0.4667\n",
      "Epoch 14/170\n",
      " - 0s - loss: 2.3162 - acc: 0.5500 - val_loss: 2.6503 - val_acc: 0.4667\n",
      "Epoch 15/170\n",
      " - 0s - loss: 2.2964 - acc: 0.5500 - val_loss: 2.6275 - val_acc: 0.4667\n",
      "Epoch 16/170\n",
      " - 0s - loss: 2.2765 - acc: 0.5500 - val_loss: 2.6043 - val_acc: 0.4667\n",
      "Epoch 17/170\n",
      " - 0s - loss: 2.2569 - acc: 0.5500 - val_loss: 2.5820 - val_acc: 0.4667\n",
      "Epoch 18/170\n",
      " - 0s - loss: 2.2378 - acc: 0.5500 - val_loss: 2.5600 - val_acc: 0.4667\n",
      "Epoch 19/170\n",
      " - 0s - loss: 2.2197 - acc: 0.5500 - val_loss: 2.5398 - val_acc: 0.4667\n",
      "Epoch 20/170\n",
      " - 0s - loss: 2.2008 - acc: 0.5500 - val_loss: 2.5170 - val_acc: 0.4667\n",
      "Epoch 21/170\n",
      " - 0s - loss: 2.1810 - acc: 0.5500 - val_loss: 2.4939 - val_acc: 0.4667\n",
      "Epoch 22/170\n",
      " - 0s - loss: 2.1607 - acc: 0.5500 - val_loss: 2.4699 - val_acc: 0.4667\n",
      "Epoch 23/170\n",
      " - 0s - loss: 2.1403 - acc: 0.5500 - val_loss: 2.4468 - val_acc: 0.4667\n",
      "Epoch 24/170\n",
      " - 0s - loss: 2.1208 - acc: 0.5500 - val_loss: 2.4247 - val_acc: 0.4667\n",
      "Epoch 25/170\n",
      " - 0s - loss: 2.1011 - acc: 0.5500 - val_loss: 2.4015 - val_acc: 0.4667\n",
      "Epoch 26/170\n",
      " - 0s - loss: 2.0807 - acc: 0.5500 - val_loss: 2.3773 - val_acc: 0.4667\n",
      "Epoch 27/170\n",
      " - 0s - loss: 2.0606 - acc: 0.5500 - val_loss: 2.3550 - val_acc: 0.4667\n",
      "Epoch 28/170\n",
      " - 0s - loss: 2.0415 - acc: 0.5500 - val_loss: 2.3330 - val_acc: 0.4667\n",
      "Epoch 29/170\n",
      " - 0s - loss: 2.0224 - acc: 0.5500 - val_loss: 2.3110 - val_acc: 0.4667\n",
      "Epoch 30/170\n",
      " - 0s - loss: 2.0034 - acc: 0.5500 - val_loss: 2.2891 - val_acc: 0.4667\n",
      "Epoch 31/170\n",
      " - 0s - loss: 1.9843 - acc: 0.5500 - val_loss: 2.2670 - val_acc: 0.4667\n",
      "Epoch 32/170\n",
      " - 0s - loss: 1.9654 - acc: 0.5500 - val_loss: 2.2452 - val_acc: 0.4667\n",
      "Epoch 33/170\n",
      " - 0s - loss: 1.9470 - acc: 0.5500 - val_loss: 2.2243 - val_acc: 0.4667\n",
      "Epoch 34/170\n",
      " - 0s - loss: 1.9290 - acc: 0.5500 - val_loss: 2.2037 - val_acc: 0.4667\n",
      "Epoch 35/170\n",
      " - 0s - loss: 1.9113 - acc: 0.5500 - val_loss: 2.1833 - val_acc: 0.4667\n",
      "Epoch 36/170\n",
      " - 0s - loss: 1.8935 - acc: 0.5500 - val_loss: 2.1628 - val_acc: 0.4667\n",
      "Epoch 37/170\n",
      " - 0s - loss: 1.8753 - acc: 0.5500 - val_loss: 2.1413 - val_acc: 0.4667\n",
      "Epoch 38/170\n",
      " - 0s - loss: 1.8568 - acc: 0.5500 - val_loss: 2.1200 - val_acc: 0.4667\n",
      "Epoch 39/170\n",
      " - 0s - loss: 1.8380 - acc: 0.5500 - val_loss: 2.0977 - val_acc: 0.4667\n",
      "Epoch 40/170\n",
      " - 0s - loss: 1.8192 - acc: 0.5500 - val_loss: 2.0762 - val_acc: 0.4667\n",
      "Epoch 41/170\n",
      " - 0s - loss: 1.8001 - acc: 0.5500 - val_loss: 2.0537 - val_acc: 0.4667\n",
      "Epoch 42/170\n",
      " - 0s - loss: 1.7817 - acc: 0.5500 - val_loss: 2.0331 - val_acc: 0.4667\n",
      "Epoch 43/170\n",
      " - 0s - loss: 1.7635 - acc: 0.5500 - val_loss: 2.0119 - val_acc: 0.4667\n",
      "Epoch 44/170\n",
      " - 0s - loss: 1.7447 - acc: 0.5500 - val_loss: 1.9896 - val_acc: 0.4667\n",
      "Epoch 45/170\n",
      " - 0s - loss: 1.7255 - acc: 0.5500 - val_loss: 1.9674 - val_acc: 0.4667\n",
      "Epoch 46/170\n",
      " - 0s - loss: 1.7063 - acc: 0.5500 - val_loss: 1.9449 - val_acc: 0.4667\n",
      "Epoch 47/170\n",
      " - 0s - loss: 1.6875 - acc: 0.5500 - val_loss: 1.9237 - val_acc: 0.4667\n",
      "Epoch 48/170\n",
      " - 0s - loss: 1.6687 - acc: 0.5500 - val_loss: 1.9015 - val_acc: 0.4667\n",
      "Epoch 49/170\n",
      " - 0s - loss: 1.6494 - acc: 0.5500 - val_loss: 1.8787 - val_acc: 0.4667\n",
      "Epoch 50/170\n",
      " - 0s - loss: 1.6309 - acc: 0.5500 - val_loss: 1.8585 - val_acc: 0.4667\n",
      "Epoch 51/170\n",
      " - 0s - loss: 1.6126 - acc: 0.5500 - val_loss: 1.8366 - val_acc: 0.4667\n",
      "Epoch 52/170\n",
      " - 0s - loss: 1.5938 - acc: 0.5500 - val_loss: 1.8148 - val_acc: 0.4667\n",
      "Epoch 53/170\n",
      " - 0s - loss: 1.5760 - acc: 0.5500 - val_loss: 1.7949 - val_acc: 0.4667\n",
      "Epoch 54/170\n",
      " - 0s - loss: 1.5590 - acc: 0.5500 - val_loss: 1.7755 - val_acc: 0.4667\n",
      "Epoch 55/170\n",
      " - 0s - loss: 1.5417 - acc: 0.5500 - val_loss: 1.7551 - val_acc: 0.4667\n",
      "Epoch 56/170\n",
      " - 0s - loss: 1.5247 - acc: 0.5500 - val_loss: 1.7359 - val_acc: 0.4667\n",
      "Epoch 57/170\n",
      " - 0s - loss: 1.5089 - acc: 0.5500 - val_loss: 1.7181 - val_acc: 0.4667\n",
      "Epoch 58/170\n",
      " - 0s - loss: 1.4915 - acc: 0.5500 - val_loss: 1.6958 - val_acc: 0.4667\n",
      "Epoch 59/170\n",
      " - 0s - loss: 1.4733 - acc: 0.5500 - val_loss: 1.6758 - val_acc: 0.4667\n",
      "Epoch 60/170\n",
      " - 0s - loss: 1.4559 - acc: 0.5500 - val_loss: 1.6553 - val_acc: 0.4667\n",
      "Epoch 61/170\n",
      " - 0s - loss: 1.4393 - acc: 0.5500 - val_loss: 1.6368 - val_acc: 0.4667\n",
      "Epoch 62/170\n",
      " - 0s - loss: 1.4230 - acc: 0.5500 - val_loss: 1.6176 - val_acc: 0.4667\n",
      "Epoch 63/170\n",
      " - 0s - loss: 1.4066 - acc: 0.5500 - val_loss: 1.5986 - val_acc: 0.4667\n",
      "Epoch 64/170\n",
      " - 0s - loss: 1.3903 - acc: 0.5500 - val_loss: 1.5797 - val_acc: 0.4667\n",
      "Epoch 65/170\n",
      " - 0s - loss: 1.3737 - acc: 0.5500 - val_loss: 1.5599 - val_acc: 0.4667\n",
      "Epoch 66/170\n",
      " - 0s - loss: 1.3574 - acc: 0.5500 - val_loss: 1.5414 - val_acc: 0.4667\n",
      "Epoch 67/170\n",
      " - 0s - loss: 1.3408 - acc: 0.5500 - val_loss: 1.5211 - val_acc: 0.4667\n",
      "Epoch 68/170\n",
      " - 0s - loss: 1.3239 - acc: 0.5500 - val_loss: 1.5019 - val_acc: 0.4667\n",
      "Epoch 69/170\n",
      " - 0s - loss: 1.3075 - acc: 0.5500 - val_loss: 1.4826 - val_acc: 0.4667\n",
      "Epoch 70/170\n",
      " - 0s - loss: 1.2912 - acc: 0.5500 - val_loss: 1.4635 - val_acc: 0.4667\n",
      "Epoch 71/170\n",
      " - 0s - loss: 1.2751 - acc: 0.5500 - val_loss: 1.4449 - val_acc: 0.4667\n",
      "Epoch 72/170\n",
      " - 0s - loss: 1.2601 - acc: 0.5500 - val_loss: 1.4280 - val_acc: 0.4667\n",
      "Epoch 73/170\n",
      " - 0s - loss: 1.2444 - acc: 0.5500 - val_loss: 1.4081 - val_acc: 0.4667\n",
      "Epoch 74/170\n",
      " - 0s - loss: 1.2281 - acc: 0.5500 - val_loss: 1.3899 - val_acc: 0.4667\n",
      "Epoch 75/170\n",
      " - 0s - loss: 1.2129 - acc: 0.5500 - val_loss: 1.3722 - val_acc: 0.4667\n",
      "Epoch 76/170\n",
      " - 0s - loss: 1.1980 - acc: 0.5500 - val_loss: 1.3546 - val_acc: 0.4667\n",
      "Epoch 77/170\n",
      " - 0s - loss: 1.1828 - acc: 0.5500 - val_loss: 1.3364 - val_acc: 0.4667\n",
      "Epoch 78/170\n",
      " - 0s - loss: 1.1684 - acc: 0.5500 - val_loss: 1.3204 - val_acc: 0.4667\n",
      "Epoch 79/170\n",
      " - 0s - loss: 1.1550 - acc: 0.5500 - val_loss: 1.3046 - val_acc: 0.4667\n",
      "Epoch 80/170\n",
      " - 0s - loss: 1.1422 - acc: 0.5500 - val_loss: 1.2900 - val_acc: 0.4667\n",
      "Epoch 81/170\n",
      " - 0s - loss: 1.1292 - acc: 0.5500 - val_loss: 1.2742 - val_acc: 0.4667\n",
      "Epoch 82/170\n",
      " - 0s - loss: 1.1161 - acc: 0.5500 - val_loss: 1.2589 - val_acc: 0.4667\n",
      "Epoch 83/170\n",
      " - 0s - loss: 1.1033 - acc: 0.5500 - val_loss: 1.2436 - val_acc: 0.4667\n",
      "Epoch 84/170\n",
      " - 0s - loss: 1.0920 - acc: 0.5500 - val_loss: 1.2311 - val_acc: 0.4667\n",
      "Epoch 85/170\n",
      " - 0s - loss: 1.0811 - acc: 0.5500 - val_loss: 1.2184 - val_acc: 0.4667\n",
      "Epoch 86/170\n",
      " - 0s - loss: 1.0694 - acc: 0.5500 - val_loss: 1.2036 - val_acc: 0.4667\n",
      "Epoch 87/170\n",
      " - 0s - loss: 1.0588 - acc: 0.5500 - val_loss: 1.1922 - val_acc: 0.4667\n",
      "Epoch 88/170\n",
      " - 0s - loss: 1.0479 - acc: 0.5500 - val_loss: 1.1782 - val_acc: 0.4667\n",
      "Epoch 89/170\n",
      " - 0s - loss: 1.0363 - acc: 0.5500 - val_loss: 1.1640 - val_acc: 0.4667\n",
      "Epoch 90/170\n",
      " - 0s - loss: 1.0262 - acc: 0.5500 - val_loss: 1.1532 - val_acc: 0.4667\n",
      "Epoch 91/170\n",
      " - 0s - loss: 1.0164 - acc: 0.5500 - val_loss: 1.1413 - val_acc: 0.4667\n",
      "Epoch 92/170\n",
      " - 0s - loss: 1.0067 - acc: 0.5500 - val_loss: 1.1298 - val_acc: 0.4667\n",
      "Epoch 93/170\n",
      " - 0s - loss: 0.9968 - acc: 0.5500 - val_loss: 1.1173 - val_acc: 0.4667\n",
      "Epoch 94/170\n",
      " - 0s - loss: 0.9867 - acc: 0.5500 - val_loss: 1.1054 - val_acc: 0.4667\n",
      "Epoch 95/170\n",
      " - 0s - loss: 0.9767 - acc: 0.5500 - val_loss: 1.0928 - val_acc: 0.4667\n",
      "Epoch 96/170\n",
      " - 0s - loss: 0.9671 - acc: 0.5500 - val_loss: 1.0819 - val_acc: 0.4667\n",
      "Epoch 97/170\n",
      " - 0s - loss: 0.9583 - acc: 0.5500 - val_loss: 1.0714 - val_acc: 0.4667\n",
      "Epoch 98/170\n",
      " - 0s - loss: 0.9495 - acc: 0.5500 - val_loss: 1.0605 - val_acc: 0.4667\n",
      "Epoch 99/170\n",
      " - 0s - loss: 0.9403 - acc: 0.5500 - val_loss: 1.0486 - val_acc: 0.4667\n",
      "Epoch 100/170\n",
      " - 0s - loss: 0.9318 - acc: 0.5500 - val_loss: 1.0394 - val_acc: 0.4667\n",
      "Epoch 101/170\n",
      " - 0s - loss: 0.9240 - acc: 0.5500 - val_loss: 1.0299 - val_acc: 0.4667\n",
      "Epoch 102/170\n",
      " - 0s - loss: 0.9165 - acc: 0.5500 - val_loss: 1.0209 - val_acc: 0.4667\n",
      "Epoch 103/170\n",
      " - 0s - loss: 0.9088 - acc: 0.5500 - val_loss: 1.0113 - val_acc: 0.4667\n",
      "Epoch 104/170\n",
      " - 0s - loss: 0.9008 - acc: 0.5500 - val_loss: 1.0005 - val_acc: 0.4667\n",
      "Epoch 105/170\n",
      " - 0s - loss: 0.8926 - acc: 0.5500 - val_loss: 0.9912 - val_acc: 0.4667\n",
      "Epoch 106/170\n",
      " - 0s - loss: 0.8850 - acc: 0.5500 - val_loss: 0.9816 - val_acc: 0.4667\n",
      "Epoch 107/170\n",
      " - 0s - loss: 0.8782 - acc: 0.5500 - val_loss: 0.9739 - val_acc: 0.4667\n",
      "Epoch 108/170\n",
      " - 0s - loss: 0.8715 - acc: 0.5500 - val_loss: 0.9654 - val_acc: 0.4667\n",
      "Epoch 109/170\n",
      " - 0s - loss: 0.8648 - acc: 0.5500 - val_loss: 0.9572 - val_acc: 0.4667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/170\n",
      " - 0s - loss: 0.8588 - acc: 0.5500 - val_loss: 0.9503 - val_acc: 0.4667\n",
      "Epoch 111/170\n",
      " - 0s - loss: 0.8525 - acc: 0.5500 - val_loss: 0.9421 - val_acc: 0.4667\n",
      "Epoch 112/170\n",
      " - 0s - loss: 0.8462 - acc: 0.5500 - val_loss: 0.9346 - val_acc: 0.4667\n",
      "Epoch 113/170\n",
      " - 0s - loss: 0.8404 - acc: 0.5500 - val_loss: 0.9276 - val_acc: 0.4667\n",
      "Epoch 114/170\n",
      " - 0s - loss: 0.8355 - acc: 0.5500 - val_loss: 0.9221 - val_acc: 0.4667\n",
      "Epoch 115/170\n",
      " - 0s - loss: 0.8298 - acc: 0.5500 - val_loss: 0.9147 - val_acc: 0.4667\n",
      "Epoch 116/170\n",
      " - 0s - loss: 0.8243 - acc: 0.5500 - val_loss: 0.9084 - val_acc: 0.4667\n",
      "Epoch 117/170\n",
      " - 0s - loss: 0.8187 - acc: 0.5500 - val_loss: 0.9007 - val_acc: 0.4667\n",
      "Epoch 118/170\n",
      " - 0s - loss: 0.8129 - acc: 0.5500 - val_loss: 0.8940 - val_acc: 0.4667\n",
      "Epoch 119/170\n",
      " - 0s - loss: 0.8075 - acc: 0.5500 - val_loss: 0.8874 - val_acc: 0.4667\n",
      "Epoch 120/170\n",
      " - 0s - loss: 0.8021 - acc: 0.5500 - val_loss: 0.8804 - val_acc: 0.4667\n",
      "Epoch 121/170\n",
      " - 0s - loss: 0.7973 - acc: 0.5500 - val_loss: 0.8750 - val_acc: 0.4667\n",
      "Epoch 122/170\n",
      " - 0s - loss: 0.7922 - acc: 0.5500 - val_loss: 0.8684 - val_acc: 0.4667\n",
      "Epoch 123/170\n",
      " - 0s - loss: 0.7878 - acc: 0.5500 - val_loss: 0.8638 - val_acc: 0.4667\n",
      "Epoch 124/170\n",
      " - 0s - loss: 0.7830 - acc: 0.5500 - val_loss: 0.8567 - val_acc: 0.4667\n",
      "Epoch 125/170\n",
      " - 0s - loss: 0.7778 - acc: 0.5500 - val_loss: 0.8513 - val_acc: 0.4667\n",
      "Epoch 126/170\n",
      " - 0s - loss: 0.7733 - acc: 0.5500 - val_loss: 0.8462 - val_acc: 0.4667\n",
      "Epoch 127/170\n",
      " - 0s - loss: 0.7690 - acc: 0.5500 - val_loss: 0.8411 - val_acc: 0.4667\n",
      "Epoch 128/170\n",
      " - 0s - loss: 0.7647 - acc: 0.5500 - val_loss: 0.8358 - val_acc: 0.4667\n",
      "Epoch 129/170\n",
      " - 0s - loss: 0.7602 - acc: 0.5500 - val_loss: 0.8300 - val_acc: 0.4667\n",
      "Epoch 130/170\n",
      " - 0s - loss: 0.7557 - acc: 0.5500 - val_loss: 0.8247 - val_acc: 0.4667\n",
      "Epoch 131/170\n",
      " - 0s - loss: 0.7515 - acc: 0.5500 - val_loss: 0.8199 - val_acc: 0.4667\n",
      "Epoch 132/170\n",
      " - 0s - loss: 0.7472 - acc: 0.5500 - val_loss: 0.8147 - val_acc: 0.4667\n",
      "Epoch 133/170\n",
      " - 0s - loss: 0.7430 - acc: 0.5500 - val_loss: 0.8093 - val_acc: 0.4667\n",
      "Epoch 134/170\n",
      " - 0s - loss: 0.7387 - acc: 0.5500 - val_loss: 0.8043 - val_acc: 0.4667\n",
      "Epoch 135/170\n",
      " - 0s - loss: 0.7347 - acc: 0.5500 - val_loss: 0.7999 - val_acc: 0.4667\n",
      "Epoch 136/170\n",
      " - 0s - loss: 0.7307 - acc: 0.5500 - val_loss: 0.7951 - val_acc: 0.4667\n",
      "Epoch 137/170\n",
      " - 0s - loss: 0.7268 - acc: 0.5500 - val_loss: 0.7907 - val_acc: 0.4667\n",
      "Epoch 138/170\n",
      " - 0s - loss: 0.7232 - acc: 0.5500 - val_loss: 0.7868 - val_acc: 0.4667\n",
      "Epoch 139/170\n",
      " - 0s - loss: 0.7194 - acc: 0.5500 - val_loss: 0.7824 - val_acc: 0.4667\n",
      "Epoch 140/170\n",
      " - 0s - loss: 0.7157 - acc: 0.5500 - val_loss: 0.7769 - val_acc: 0.4667\n",
      "Epoch 141/170\n",
      " - 0s - loss: 0.7113 - acc: 0.5500 - val_loss: 0.7721 - val_acc: 0.4667\n",
      "Epoch 142/170\n",
      " - 0s - loss: 0.7073 - acc: 0.5500 - val_loss: 0.7675 - val_acc: 0.4667\n",
      "Epoch 143/170\n",
      " - 0s - loss: 0.7037 - acc: 0.5500 - val_loss: 0.7621 - val_acc: 0.4667\n",
      "Epoch 144/170\n",
      " - 0s - loss: 0.6997 - acc: 0.5500 - val_loss: 0.7583 - val_acc: 0.4667\n",
      "Epoch 145/170\n",
      " - 0s - loss: 0.6960 - acc: 0.5500 - val_loss: 0.7534 - val_acc: 0.4667\n",
      "Epoch 146/170\n",
      " - 0s - loss: 0.6921 - acc: 0.5500 - val_loss: 0.7488 - val_acc: 0.4667\n",
      "Epoch 147/170\n",
      " - 0s - loss: 0.6887 - acc: 0.5500 - val_loss: 0.7452 - val_acc: 0.4667\n",
      "Epoch 148/170\n",
      " - 0s - loss: 0.6853 - acc: 0.5500 - val_loss: 0.7417 - val_acc: 0.4667\n",
      "Epoch 149/170\n",
      " - 0s - loss: 0.6818 - acc: 0.5500 - val_loss: 0.7381 - val_acc: 0.4667\n",
      "Epoch 150/170\n",
      " - 0s - loss: 0.6788 - acc: 0.5500 - val_loss: 0.7351 - val_acc: 0.4667\n",
      "Epoch 151/170\n",
      " - 0s - loss: 0.6758 - acc: 0.5500 - val_loss: 0.7324 - val_acc: 0.4667\n",
      "Epoch 152/170\n",
      " - 0s - loss: 0.6725 - acc: 0.5500 - val_loss: 0.7289 - val_acc: 0.4667\n",
      "Epoch 153/170\n",
      " - 0s - loss: 0.6693 - acc: 0.5500 - val_loss: 0.7255 - val_acc: 0.4667\n",
      "Epoch 154/170\n",
      " - 0s - loss: 0.6661 - acc: 0.5500 - val_loss: 0.7222 - val_acc: 0.4667\n",
      "Epoch 155/170\n",
      " - 0s - loss: 0.6630 - acc: 0.5500 - val_loss: 0.7186 - val_acc: 0.4667\n",
      "Epoch 156/170\n",
      " - 0s - loss: 0.6597 - acc: 0.5500 - val_loss: 0.7145 - val_acc: 0.4667\n",
      "Epoch 157/170\n",
      " - 0s - loss: 0.6564 - acc: 0.5500 - val_loss: 0.7111 - val_acc: 0.4667\n",
      "Epoch 158/170\n",
      " - 0s - loss: 0.6532 - acc: 0.5500 - val_loss: 0.7077 - val_acc: 0.4667\n",
      "Epoch 159/170\n",
      " - 0s - loss: 0.6500 - acc: 0.5500 - val_loss: 0.7041 - val_acc: 0.4667\n",
      "Epoch 160/170\n",
      " - 0s - loss: 0.6468 - acc: 0.5500 - val_loss: 0.7004 - val_acc: 0.4667\n",
      "Epoch 161/170\n",
      " - 0s - loss: 0.6438 - acc: 0.5500 - val_loss: 0.6973 - val_acc: 0.4667\n",
      "Epoch 162/170\n",
      " - 0s - loss: 0.6407 - acc: 0.5500 - val_loss: 0.6940 - val_acc: 0.4667\n",
      "Epoch 163/170\n",
      " - 0s - loss: 0.6377 - acc: 0.5500 - val_loss: 0.6908 - val_acc: 0.4667\n",
      "Epoch 164/170\n",
      " - 0s - loss: 0.6346 - acc: 0.5500 - val_loss: 0.6873 - val_acc: 0.4667\n",
      "Epoch 165/170\n",
      " - 0s - loss: 0.6316 - acc: 0.5500 - val_loss: 0.6843 - val_acc: 0.4667\n",
      "Epoch 166/170\n",
      " - 0s - loss: 0.6286 - acc: 0.5500 - val_loss: 0.6808 - val_acc: 0.4667\n",
      "Epoch 167/170\n",
      " - 0s - loss: 0.6255 - acc: 0.5500 - val_loss: 0.6770 - val_acc: 0.4667\n",
      "Epoch 168/170\n",
      " - 0s - loss: 0.6225 - acc: 0.5500 - val_loss: 0.6740 - val_acc: 0.4667\n",
      "Epoch 169/170\n",
      " - 0s - loss: 0.6195 - acc: 0.5500 - val_loss: 0.6702 - val_acc: 0.4667\n",
      "Epoch 170/170\n",
      " - 0s - loss: 0.6163 - acc: 0.5500 - val_loss: 0.6668 - val_acc: 0.4667\n",
      "Test accuracy: 0.4666666626930237\n",
      "Train on 40 samples, validate on 60 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 2.2162 - acc: 0.5500 - val_loss: 2.5267 - val_acc: 0.4667\n",
      "Epoch 2/100\n",
      " - 0s - loss: 2.1981 - acc: 0.5500 - val_loss: 2.5105 - val_acc: 0.4667\n",
      "Epoch 3/100\n",
      " - 0s - loss: 2.1840 - acc: 0.5500 - val_loss: 2.4935 - val_acc: 0.4667\n",
      "Epoch 4/100\n",
      " - 0s - loss: 2.1692 - acc: 0.5500 - val_loss: 2.4754 - val_acc: 0.4667\n",
      "Epoch 5/100\n",
      " - 0s - loss: 2.1533 - acc: 0.5500 - val_loss: 2.4562 - val_acc: 0.4667\n",
      "Epoch 6/100\n",
      " - 0s - loss: 2.1367 - acc: 0.5500 - val_loss: 2.4365 - val_acc: 0.4667\n",
      "Epoch 7/100\n",
      " - 0s - loss: 2.1195 - acc: 0.5500 - val_loss: 2.4160 - val_acc: 0.4667\n",
      "Epoch 8/100\n",
      " - 0s - loss: 2.1021 - acc: 0.5500 - val_loss: 2.3960 - val_acc: 0.4667\n",
      "Epoch 9/100\n",
      " - 0s - loss: 2.0851 - acc: 0.5500 - val_loss: 2.3764 - val_acc: 0.4667\n",
      "Epoch 10/100\n",
      " - 0s - loss: 2.0669 - acc: 0.5500 - val_loss: 2.3537 - val_acc: 0.4667\n",
      "Epoch 11/100\n",
      " - 0s - loss: 2.0481 - acc: 0.5500 - val_loss: 2.3328 - val_acc: 0.4667\n",
      "Epoch 12/100\n",
      " - 0s - loss: 2.0288 - acc: 0.5500 - val_loss: 2.3090 - val_acc: 0.4667\n",
      "Epoch 13/100\n",
      " - 0s - loss: 2.0089 - acc: 0.5500 - val_loss: 2.2868 - val_acc: 0.4667\n",
      "Epoch 14/100\n",
      " - 0s - loss: 1.9901 - acc: 0.5500 - val_loss: 2.2655 - val_acc: 0.4667\n",
      "Epoch 15/100\n",
      " - 0s - loss: 1.9708 - acc: 0.5500 - val_loss: 2.2423 - val_acc: 0.4667\n",
      "Epoch 16/100\n",
      " - 0s - loss: 1.9515 - acc: 0.5500 - val_loss: 2.2207 - val_acc: 0.4667\n",
      "Epoch 17/100\n",
      " - 0s - loss: 1.9337 - acc: 0.5500 - val_loss: 2.2010 - val_acc: 0.4667\n",
      "Epoch 18/100\n",
      " - 0s - loss: 1.9163 - acc: 0.5500 - val_loss: 2.1807 - val_acc: 0.4667\n",
      "Epoch 19/100\n",
      " - 0s - loss: 1.8977 - acc: 0.5500 - val_loss: 2.1584 - val_acc: 0.4667\n",
      "Epoch 20/100\n",
      " - 0s - loss: 1.8785 - acc: 0.5500 - val_loss: 2.1362 - val_acc: 0.4667\n",
      "Epoch 21/100\n",
      " - 0s - loss: 1.8593 - acc: 0.5500 - val_loss: 2.1139 - val_acc: 0.4667\n",
      "Epoch 22/100\n",
      " - 0s - loss: 1.8399 - acc: 0.5500 - val_loss: 2.0915 - val_acc: 0.4667\n",
      "Epoch 23/100\n",
      " - 0s - loss: 1.8209 - acc: 0.5500 - val_loss: 2.0699 - val_acc: 0.4667\n",
      "Epoch 24/100\n",
      " - 0s - loss: 1.8023 - acc: 0.5500 - val_loss: 2.0484 - val_acc: 0.4667\n",
      "Epoch 25/100\n",
      " - 0s - loss: 1.7831 - acc: 0.5500 - val_loss: 2.0259 - val_acc: 0.4667\n",
      "Epoch 26/100\n",
      " - 0s - loss: 1.7637 - acc: 0.5500 - val_loss: 2.0035 - val_acc: 0.4667\n",
      "Epoch 27/100\n",
      " - 0s - loss: 1.7443 - acc: 0.5500 - val_loss: 1.9811 - val_acc: 0.4667\n",
      "Epoch 28/100\n",
      " - 0s - loss: 1.7249 - acc: 0.5500 - val_loss: 1.9586 - val_acc: 0.4667\n",
      "Epoch 29/100\n",
      " - 0s - loss: 1.7068 - acc: 0.5500 - val_loss: 1.9389 - val_acc: 0.4667\n",
      "Epoch 30/100\n",
      " - 0s - loss: 1.6903 - acc: 0.5500 - val_loss: 1.9205 - val_acc: 0.4667\n",
      "Epoch 31/100\n",
      " - 0s - loss: 1.6733 - acc: 0.5500 - val_loss: 1.9006 - val_acc: 0.4667\n",
      "Epoch 32/100\n",
      " - 0s - loss: 1.6545 - acc: 0.5500 - val_loss: 1.8768 - val_acc: 0.4667\n",
      "Epoch 33/100\n",
      " - 0s - loss: 1.6351 - acc: 0.5500 - val_loss: 1.8558 - val_acc: 0.4667\n",
      "Epoch 34/100\n",
      " - 0s - loss: 1.6160 - acc: 0.5500 - val_loss: 1.8329 - val_acc: 0.4667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100\n",
      " - 0s - loss: 1.5965 - acc: 0.5500 - val_loss: 1.8108 - val_acc: 0.4667\n",
      "Epoch 36/100\n",
      " - 0s - loss: 1.5771 - acc: 0.5500 - val_loss: 1.7880 - val_acc: 0.4667\n",
      "Epoch 37/100\n",
      " - 0s - loss: 1.5567 - acc: 0.5500 - val_loss: 1.7634 - val_acc: 0.4667\n",
      "Epoch 38/100\n",
      " - 0s - loss: 1.5362 - acc: 0.5500 - val_loss: 1.7409 - val_acc: 0.4667\n",
      "Epoch 39/100\n",
      " - 0s - loss: 1.5170 - acc: 0.5500 - val_loss: 1.7189 - val_acc: 0.4667\n",
      "Epoch 40/100\n",
      " - 0s - loss: 1.4987 - acc: 0.5500 - val_loss: 1.6985 - val_acc: 0.4667\n",
      "Epoch 41/100\n",
      " - 0s - loss: 1.4813 - acc: 0.5500 - val_loss: 1.6788 - val_acc: 0.4667\n",
      "Epoch 42/100\n",
      " - 0s - loss: 1.4636 - acc: 0.5500 - val_loss: 1.6582 - val_acc: 0.4667\n",
      "Epoch 43/100\n",
      " - 0s - loss: 1.4451 - acc: 0.5500 - val_loss: 1.6362 - val_acc: 0.4667\n",
      "Epoch 44/100\n",
      " - 0s - loss: 1.4260 - acc: 0.5500 - val_loss: 1.6140 - val_acc: 0.4667\n",
      "Epoch 45/100\n",
      " - 0s - loss: 1.4076 - acc: 0.5500 - val_loss: 1.5935 - val_acc: 0.4667\n",
      "Epoch 46/100\n",
      " - 0s - loss: 1.3899 - acc: 0.5500 - val_loss: 1.5732 - val_acc: 0.4667\n",
      "Epoch 47/100\n",
      " - 0s - loss: 1.3725 - acc: 0.5500 - val_loss: 1.5533 - val_acc: 0.4667\n",
      "Epoch 48/100\n",
      " - 0s - loss: 1.3552 - acc: 0.5500 - val_loss: 1.5334 - val_acc: 0.4667\n",
      "Epoch 49/100\n",
      " - 0s - loss: 1.3384 - acc: 0.5500 - val_loss: 1.5144 - val_acc: 0.4667\n",
      "Epoch 50/100\n",
      " - 0s - loss: 1.3213 - acc: 0.5500 - val_loss: 1.4941 - val_acc: 0.4667\n",
      "Epoch 51/100\n",
      " - 0s - loss: 1.3046 - acc: 0.5500 - val_loss: 1.4757 - val_acc: 0.4667\n",
      "Epoch 52/100\n",
      " - 0s - loss: 1.2885 - acc: 0.5500 - val_loss: 1.4571 - val_acc: 0.4667\n",
      "Epoch 53/100\n",
      " - 0s - loss: 1.2726 - acc: 0.5500 - val_loss: 1.4390 - val_acc: 0.4667\n",
      "Epoch 54/100\n",
      " - 0s - loss: 1.2558 - acc: 0.5500 - val_loss: 1.4185 - val_acc: 0.4667\n",
      "Epoch 55/100\n",
      " - 0s - loss: 1.2381 - acc: 0.5500 - val_loss: 1.3979 - val_acc: 0.4667\n",
      "Epoch 56/100\n",
      " - 0s - loss: 1.2211 - acc: 0.5500 - val_loss: 1.3790 - val_acc: 0.4667\n",
      "Epoch 57/100\n",
      " - 0s - loss: 1.2047 - acc: 0.5500 - val_loss: 1.3602 - val_acc: 0.4667\n",
      "Epoch 58/100\n",
      " - 0s - loss: 1.1877 - acc: 0.5500 - val_loss: 1.3397 - val_acc: 0.4667\n",
      "Epoch 59/100\n",
      " - 0s - loss: 1.1714 - acc: 0.5500 - val_loss: 1.3219 - val_acc: 0.4667\n",
      "Epoch 60/100\n",
      " - 0s - loss: 1.1556 - acc: 0.5500 - val_loss: 1.3036 - val_acc: 0.4667\n",
      "Epoch 61/100\n",
      " - 0s - loss: 1.1395 - acc: 0.5500 - val_loss: 1.2846 - val_acc: 0.4667\n",
      "Epoch 62/100\n",
      " - 0s - loss: 1.1240 - acc: 0.5500 - val_loss: 1.2674 - val_acc: 0.4667\n",
      "Epoch 63/100\n",
      " - 0s - loss: 1.1084 - acc: 0.5500 - val_loss: 1.2489 - val_acc: 0.4667\n",
      "Epoch 64/100\n",
      " - 0s - loss: 1.0927 - acc: 0.5500 - val_loss: 1.2310 - val_acc: 0.4667\n",
      "Epoch 65/100\n",
      " - 0s - loss: 1.0770 - acc: 0.5500 - val_loss: 1.2125 - val_acc: 0.4667\n",
      "Epoch 66/100\n",
      " - 0s - loss: 1.0612 - acc: 0.5500 - val_loss: 1.1942 - val_acc: 0.4667\n",
      "Epoch 67/100\n",
      " - 0s - loss: 1.0454 - acc: 0.5500 - val_loss: 1.1759 - val_acc: 0.4667\n",
      "Epoch 68/100\n",
      " - 0s - loss: 1.0292 - acc: 0.5500 - val_loss: 1.1565 - val_acc: 0.4667\n",
      "Epoch 69/100\n",
      " - 0s - loss: 1.0141 - acc: 0.5500 - val_loss: 1.1404 - val_acc: 0.4667\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.9991 - acc: 0.5500 - val_loss: 1.1220 - val_acc: 0.4667\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.9832 - acc: 0.5500 - val_loss: 1.1031 - val_acc: 0.4667\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.9679 - acc: 0.5500 - val_loss: 1.0864 - val_acc: 0.4667\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.9538 - acc: 0.5500 - val_loss: 1.0703 - val_acc: 0.4667\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.9396 - acc: 0.5500 - val_loss: 1.0535 - val_acc: 0.4667\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.9258 - acc: 0.5500 - val_loss: 1.0379 - val_acc: 0.4667\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.9126 - acc: 0.5500 - val_loss: 1.0226 - val_acc: 0.4667\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.8987 - acc: 0.5500 - val_loss: 1.0055 - val_acc: 0.4667\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.8844 - acc: 0.5500 - val_loss: 0.9892 - val_acc: 0.4667\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.8722 - acc: 0.5500 - val_loss: 0.9760 - val_acc: 0.4667\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.8593 - acc: 0.5500 - val_loss: 0.9597 - val_acc: 0.4667\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.8461 - acc: 0.5500 - val_loss: 0.9450 - val_acc: 0.4667\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.8337 - acc: 0.5500 - val_loss: 0.9305 - val_acc: 0.4667\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.8219 - acc: 0.5500 - val_loss: 0.9171 - val_acc: 0.4667\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.8100 - acc: 0.5500 - val_loss: 0.9027 - val_acc: 0.4667\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.7980 - acc: 0.5500 - val_loss: 0.8884 - val_acc: 0.4667\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.7867 - acc: 0.5500 - val_loss: 0.8758 - val_acc: 0.4667\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.7763 - acc: 0.5500 - val_loss: 0.8638 - val_acc: 0.4667\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.7669 - acc: 0.5500 - val_loss: 0.8532 - val_acc: 0.4667\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.7572 - acc: 0.5500 - val_loss: 0.8414 - val_acc: 0.4667\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.7476 - acc: 0.5500 - val_loss: 0.8303 - val_acc: 0.4667\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.7377 - acc: 0.5500 - val_loss: 0.8176 - val_acc: 0.4667\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.7280 - acc: 0.5500 - val_loss: 0.8069 - val_acc: 0.4667\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.7187 - acc: 0.5500 - val_loss: 0.7955 - val_acc: 0.4667\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.7093 - acc: 0.5500 - val_loss: 0.7842 - val_acc: 0.4667\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.7000 - acc: 0.5500 - val_loss: 0.7731 - val_acc: 0.4667\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.6911 - acc: 0.5500 - val_loss: 0.7625 - val_acc: 0.4667\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.6826 - acc: 0.5500 - val_loss: 0.7525 - val_acc: 0.4667\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.6747 - acc: 0.5500 - val_loss: 0.7431 - val_acc: 0.4667\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.6669 - acc: 0.5500 - val_loss: 0.7338 - val_acc: 0.4667\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.6597 - acc: 0.5500 - val_loss: 0.7254 - val_acc: 0.4667\n",
      "Test accuracy: 0.4666666626930237\n",
      "Evalutation of best performing model:\n",
      "60/60 [==============================] - 0s 17us/step\n",
      "[0.2943368494510651, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import print_function\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def data():\n",
    "    '''\n",
    "    Data providing function:\n",
    "\n",
    "    Make sure to have every relevant import statement included here and return data as\n",
    "    used in model function below. This function is separated from model() so that hyperopt\n",
    "    won't reload data for each evaluation run.\n",
    "    '''\n",
    "    url = \"../data/iris.csv\"\n",
    "    data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
    "    class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
    "    data.iloc[:,-1] = index\n",
    "    data = data.loc[data[4] != 2]\n",
    "    X = data.iloc[:,:-1]\n",
    "    Y = data.iloc[:,-1]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def model(x_train, y_train, x_test, y_test):\n",
    "    '''\n",
    "    Model providing function:\n",
    "\n",
    "    Create Keras model with double curly brackets dropped-in as needed.\n",
    "    Return value has to be a valid python dictionary with two customary keys:\n",
    "        - loss: Specify a numeric evaluation metric to be minimized\n",
    "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
    "    The last one is optional, though recommended, namely:\n",
    "        - model: specify the model just created so that we can later use it again.\n",
    "    '''\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim=4, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer={{choice(['adam', 'nadam'])}},\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size={{choice([10, 30])}},\n",
    "              epochs={{choice([100, 170])}},\n",
    "              verbose=2,\n",
    "              validation_data=(x_test, y_test))\n",
    "    score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=5,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='experiment')\n",
    "x_train, y_train, x_test, y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Hyperas(Linear regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from imly import dope\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import boto\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import sys\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from boto.s3.key import Key\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import LabelEncoder\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.utils.validation import column_or_1d\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import LabelEncoder\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import copy\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import datasets\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import experiment_automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LinearRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import mean_squared_error\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import experiment_automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import re\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from automation_script import get_dataset_info\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from imly import dope\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from utils.correlations import concordance_correlation_coefficient as ccc\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LinearRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from imly import dope\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from utils.correlations import concordance_correlation_coefficient as ccc\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import boto\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import sys\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from boto.s3.key import Key\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.datasets import make_regression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import theano.tensor as T\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import theano\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from theano.compile.ops import as_op\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adam\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l2\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'optimizer': hp.choice('optimizer', ['adam', 'nadam']),\n",
      "        'batch_size': hp.choice('batch_size', [10, 30]),\n",
      "        'epochs': hp.choice('epochs', [100, 170]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: '''\n",
      "  3: Data providing function:\n",
      "  4: \n",
      "  5: Make sure to have every relevant import statement included here and return data as\n",
      "  6: used in model function below. This function is separated from model() so that hyperopt\n",
      "  7: won't reload data for each evaluation run.\n",
      "  8: '''\n",
      "  9: from os import path\n",
      " 10: import pandas as pd\n",
      " 11: from sklearn import preprocessing\n",
      " 12: from sklearn.preprocessing import StandardScaler\n",
      " 13: \n",
      " 14: url = \"../data/diabetes.csv\"\n",
      " 15: data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
      " 16: sc = StandardScaler()\n",
      " 17: data = sc.fit_transform(data)\n",
      " 18: data = pd.DataFrame(data)\n",
      " 19: \n",
      " 20: \n",
      " 21: X = data.iloc[:,:-1]\n",
      " 22: Y = data.iloc[:,-1]\n",
      " 23: x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
      " 24: \n",
      " 25: \n",
      " 26: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     '''\n",
      "   4:     Model providing function:\n",
      "   5: \n",
      "   6:     Create Keras model with double curly brackets dropped-in as needed.\n",
      "   7:     Return value has to be a valid python dictionary with two customary keys:\n",
      "   8:         - loss: Specify a numeric evaluation metric to be minimized\n",
      "   9:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
      "  10:     The last one is optional, though recommended, namely:\n",
      "  11:         - model: specify the model just created so that we can later use it again.\n",
      "  12:     '''\n",
      "  13: \n",
      "  14:     model = Sequential()\n",
      "  15:     model.add(Dense(1, input_dim=10, activation='linear'))\n",
      "  16: \n",
      "  17:     model.compile(loss='mse', optimizer=space['optimizer'])\n",
      "  18: \n",
      "  19:     model.fit(x_train, y_train,\n",
      "  20:               batch_size=space['batch_size'],\n",
      "  21:               epochs=space['epochs'],\n",
      "  22:               verbose=2,\n",
      "  23:               validation_data=(x_test, y_test))\n",
      "  24:     score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
      "  25:     print('Test accuracy:', acc)\n",
      "  26:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  27: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 176 samples, validate on 266 samples\n",
      "Epoch 1/170\n",
      " - 0s - loss: 1.6659 - acc: 0.0000e+00 - val_loss: 1.5142 - val_acc: 0.0000e+00\n",
      "Epoch 2/170\n",
      " - 0s - loss: 1.6094 - acc: 0.0000e+00 - val_loss: 1.4674 - val_acc: 0.0000e+00\n",
      "Epoch 3/170\n",
      " - 0s - loss: 1.5604 - acc: 0.0000e+00 - val_loss: 1.4252 - val_acc: 0.0000e+00\n",
      "Epoch 4/170\n",
      " - 0s - loss: 1.5204 - acc: 0.0000e+00 - val_loss: 1.3813 - val_acc: 0.0000e+00\n",
      "Epoch 5/170\n",
      " - 0s - loss: 1.4753 - acc: 0.0000e+00 - val_loss: 1.3462 - val_acc: 0.0000e+00\n",
      "Epoch 6/170\n",
      " - 0s - loss: 1.4360 - acc: 0.0000e+00 - val_loss: 1.3165 - val_acc: 0.0000e+00\n",
      "Epoch 7/170\n",
      " - 0s - loss: 1.3996 - acc: 0.0000e+00 - val_loss: 1.2852 - val_acc: 0.0000e+00\n",
      "Epoch 8/170\n",
      " - 0s - loss: 1.3632 - acc: 0.0000e+00 - val_loss: 1.2518 - val_acc: 0.0000e+00\n",
      "Epoch 9/170\n",
      " - 0s - loss: 1.3302 - acc: 0.0000e+00 - val_loss: 1.2223 - val_acc: 0.0000e+00\n",
      "Epoch 10/170\n",
      " - 0s - loss: 1.2957 - acc: 0.0000e+00 - val_loss: 1.1963 - val_acc: 0.0000e+00\n",
      "Epoch 11/170\n",
      " - 0s - loss: 1.2650 - acc: 0.0000e+00 - val_loss: 1.1693 - val_acc: 0.0000e+00\n",
      "Epoch 12/170\n",
      " - 0s - loss: 1.2356 - acc: 0.0000e+00 - val_loss: 1.1414 - val_acc: 0.0000e+00\n",
      "Epoch 13/170\n",
      " - 0s - loss: 1.2079 - acc: 0.0000e+00 - val_loss: 1.1171 - val_acc: 0.0000e+00\n",
      "Epoch 14/170\n",
      " - 0s - loss: 1.1778 - acc: 0.0000e+00 - val_loss: 1.0964 - val_acc: 0.0000e+00\n",
      "Epoch 15/170\n",
      " - 0s - loss: 1.1536 - acc: 0.0000e+00 - val_loss: 1.0736 - val_acc: 0.0000e+00\n",
      "Epoch 16/170\n",
      " - 0s - loss: 1.1273 - acc: 0.0000e+00 - val_loss: 1.0521 - val_acc: 0.0000e+00\n",
      "Epoch 17/170\n",
      " - 0s - loss: 1.1030 - acc: 0.0000e+00 - val_loss: 1.0333 - val_acc: 0.0000e+00\n",
      "Epoch 18/170\n",
      " - 0s - loss: 1.0797 - acc: 0.0000e+00 - val_loss: 1.0138 - val_acc: 0.0000e+00\n",
      "Epoch 19/170\n",
      " - 0s - loss: 1.0573 - acc: 0.0000e+00 - val_loss: 0.9942 - val_acc: 0.0000e+00\n",
      "Epoch 20/170\n",
      " - 0s - loss: 1.0365 - acc: 0.0000e+00 - val_loss: 0.9776 - val_acc: 0.0000e+00\n",
      "Epoch 21/170\n",
      " - 0s - loss: 1.0154 - acc: 0.0000e+00 - val_loss: 0.9591 - val_acc: 0.0000e+00\n",
      "Epoch 22/170\n",
      " - 0s - loss: 0.9958 - acc: 0.0000e+00 - val_loss: 0.9389 - val_acc: 0.0000e+00\n",
      "Epoch 23/170\n",
      " - 0s - loss: 0.9760 - acc: 0.0000e+00 - val_loss: 0.9232 - val_acc: 0.0000e+00\n",
      "Epoch 24/170\n",
      " - 0s - loss: 0.9569 - acc: 0.0000e+00 - val_loss: 0.9071 - val_acc: 0.0000e+00\n",
      "Epoch 25/170\n",
      " - 0s - loss: 0.9396 - acc: 0.0000e+00 - val_loss: 0.8925 - val_acc: 0.0000e+00\n",
      "Epoch 26/170\n",
      " - 0s - loss: 0.9217 - acc: 0.0000e+00 - val_loss: 0.8776 - val_acc: 0.0000e+00\n",
      "Epoch 27/170\n",
      " - 0s - loss: 0.9045 - acc: 0.0000e+00 - val_loss: 0.8651 - val_acc: 0.0000e+00\n",
      "Epoch 28/170\n",
      " - 0s - loss: 0.8893 - acc: 0.0000e+00 - val_loss: 0.8498 - val_acc: 0.0000e+00\n",
      "Epoch 29/170\n",
      " - 0s - loss: 0.8741 - acc: 0.0000e+00 - val_loss: 0.8364 - val_acc: 0.0000e+00\n",
      "Epoch 30/170\n",
      " - 0s - loss: 0.8589 - acc: 0.0000e+00 - val_loss: 0.8254 - val_acc: 0.0000e+00\n",
      "Epoch 31/170\n",
      " - 0s - loss: 0.8440 - acc: 0.0000e+00 - val_loss: 0.8124 - val_acc: 0.0000e+00\n",
      "Epoch 32/170\n",
      " - 0s - loss: 0.8306 - acc: 0.0000e+00 - val_loss: 0.7989 - val_acc: 0.0000e+00\n",
      "Epoch 33/170\n",
      " - 0s - loss: 0.8164 - acc: 0.0000e+00 - val_loss: 0.7878 - val_acc: 0.0000e+00\n",
      "Epoch 34/170\n",
      " - 0s - loss: 0.8037 - acc: 0.0000e+00 - val_loss: 0.7759 - val_acc: 0.0000e+00\n",
      "Epoch 35/170\n",
      " - 0s - loss: 0.7916 - acc: 0.0000e+00 - val_loss: 0.7653 - val_acc: 0.0000e+00\n",
      "Epoch 36/170\n",
      " - 0s - loss: 0.7790 - acc: 0.0000e+00 - val_loss: 0.7573 - val_acc: 0.0000e+00\n",
      "Epoch 37/170\n",
      " - 0s - loss: 0.7675 - acc: 0.0000e+00 - val_loss: 0.7474 - val_acc: 0.0000e+00\n",
      "Epoch 38/170\n",
      " - 0s - loss: 0.7564 - acc: 0.0000e+00 - val_loss: 0.7361 - val_acc: 0.0000e+00\n",
      "Epoch 39/170\n",
      " - 0s - loss: 0.7450 - acc: 0.0000e+00 - val_loss: 0.7267 - val_acc: 0.0000e+00\n",
      "Epoch 40/170\n",
      " - 0s - loss: 0.7349 - acc: 0.0000e+00 - val_loss: 0.7191 - val_acc: 0.0000e+00\n",
      "Epoch 41/170\n",
      " - 0s - loss: 0.7244 - acc: 0.0000e+00 - val_loss: 0.7100 - val_acc: 0.0000e+00\n",
      "Epoch 42/170\n",
      " - 0s - loss: 0.7147 - acc: 0.0000e+00 - val_loss: 0.7014 - val_acc: 0.0000e+00\n",
      "Epoch 43/170\n",
      " - 0s - loss: 0.7063 - acc: 0.0000e+00 - val_loss: 0.6924 - val_acc: 0.0000e+00\n",
      "Epoch 44/170\n",
      " - 0s - loss: 0.6958 - acc: 0.0000e+00 - val_loss: 0.6877 - val_acc: 0.0000e+00\n",
      "Epoch 45/170\n",
      " - 0s - loss: 0.6875 - acc: 0.0000e+00 - val_loss: 0.6810 - val_acc: 0.0000e+00\n",
      "Epoch 46/170\n",
      " - 0s - loss: 0.6791 - acc: 0.0000e+00 - val_loss: 0.6740 - val_acc: 0.0000e+00\n",
      "Epoch 47/170\n",
      " - 0s - loss: 0.6702 - acc: 0.0000e+00 - val_loss: 0.6662 - val_acc: 0.0000e+00\n",
      "Epoch 48/170\n",
      " - 0s - loss: 0.6623 - acc: 0.0000e+00 - val_loss: 0.6592 - val_acc: 0.0000e+00\n",
      "Epoch 49/170\n",
      " - 0s - loss: 0.6549 - acc: 0.0000e+00 - val_loss: 0.6542 - val_acc: 0.0000e+00\n",
      "Epoch 50/170\n",
      " - 0s - loss: 0.6475 - acc: 0.0000e+00 - val_loss: 0.6479 - val_acc: 0.0000e+00\n",
      "Epoch 51/170\n",
      " - 0s - loss: 0.6406 - acc: 0.0000e+00 - val_loss: 0.6424 - val_acc: 0.0000e+00\n",
      "Epoch 52/170\n",
      " - 0s - loss: 0.6333 - acc: 0.0000e+00 - val_loss: 0.6384 - val_acc: 0.0000e+00\n",
      "Epoch 53/170\n",
      " - 0s - loss: 0.6274 - acc: 0.0000e+00 - val_loss: 0.6336 - val_acc: 0.0000e+00\n",
      "Epoch 54/170\n",
      " - 0s - loss: 0.6213 - acc: 0.0000e+00 - val_loss: 0.6271 - val_acc: 0.0000e+00\n",
      "Epoch 55/170\n",
      " - 0s - loss: 0.6146 - acc: 0.0000e+00 - val_loss: 0.6228 - val_acc: 0.0000e+00\n",
      "Epoch 56/170\n",
      " - 0s - loss: 0.6087 - acc: 0.0000e+00 - val_loss: 0.6168 - val_acc: 0.0000e+00\n",
      "Epoch 57/170\n",
      " - 0s - loss: 0.6027 - acc: 0.0000e+00 - val_loss: 0.6127 - val_acc: 0.0000e+00\n",
      "Epoch 58/170\n",
      " - 0s - loss: 0.5972 - acc: 0.0000e+00 - val_loss: 0.6092 - val_acc: 0.0000e+00\n",
      "Epoch 59/170\n",
      " - 0s - loss: 0.5920 - acc: 0.0000e+00 - val_loss: 0.6043 - val_acc: 0.0000e+00\n",
      "Epoch 60/170\n",
      " - 0s - loss: 0.5866 - acc: 0.0000e+00 - val_loss: 0.6005 - val_acc: 0.0000e+00\n",
      "Epoch 61/170\n",
      " - 0s - loss: 0.5813 - acc: 0.0000e+00 - val_loss: 0.5970 - val_acc: 0.0000e+00\n",
      "Epoch 62/170\n",
      " - 0s - loss: 0.5769 - acc: 0.0000e+00 - val_loss: 0.5940 - val_acc: 0.0000e+00\n",
      "Epoch 63/170\n",
      " - 0s - loss: 0.5719 - acc: 0.0000e+00 - val_loss: 0.5907 - val_acc: 0.0000e+00\n",
      "Epoch 64/170\n",
      " - 0s - loss: 0.5675 - acc: 0.0000e+00 - val_loss: 0.5877 - val_acc: 0.0000e+00\n",
      "Epoch 65/170\n",
      " - 0s - loss: 0.5627 - acc: 0.0000e+00 - val_loss: 0.5847 - val_acc: 0.0000e+00\n",
      "Epoch 66/170\n",
      " - 0s - loss: 0.5592 - acc: 0.0000e+00 - val_loss: 0.5815 - val_acc: 0.0000e+00\n",
      "Epoch 67/170\n",
      " - 0s - loss: 0.5546 - acc: 0.0000e+00 - val_loss: 0.5796 - val_acc: 0.0000e+00\n",
      "Epoch 68/170\n",
      " - 0s - loss: 0.5510 - acc: 0.0000e+00 - val_loss: 0.5770 - val_acc: 0.0000e+00\n",
      "Epoch 69/170\n",
      " - 0s - loss: 0.5473 - acc: 0.0000e+00 - val_loss: 0.5758 - val_acc: 0.0000e+00\n",
      "Epoch 70/170\n",
      " - 0s - loss: 0.5439 - acc: 0.0000e+00 - val_loss: 0.5723 - val_acc: 0.0000e+00\n",
      "Epoch 71/170\n",
      " - 0s - loss: 0.5394 - acc: 0.0000e+00 - val_loss: 0.5701 - val_acc: 0.0000e+00\n",
      "Epoch 72/170\n",
      " - 0s - loss: 0.5368 - acc: 0.0000e+00 - val_loss: 0.5685 - val_acc: 0.0000e+00\n",
      "Epoch 73/170\n",
      " - 0s - loss: 0.5329 - acc: 0.0000e+00 - val_loss: 0.5669 - val_acc: 0.0000e+00\n",
      "Epoch 74/170\n",
      " - 0s - loss: 0.5301 - acc: 0.0000e+00 - val_loss: 0.5640 - val_acc: 0.0000e+00\n",
      "Epoch 75/170\n",
      " - 0s - loss: 0.5271 - acc: 0.0000e+00 - val_loss: 0.5629 - val_acc: 0.0000e+00\n",
      "Epoch 76/170\n",
      " - 0s - loss: 0.5238 - acc: 0.0000e+00 - val_loss: 0.5614 - val_acc: 0.0000e+00\n",
      "Epoch 77/170\n",
      " - 0s - loss: 0.5214 - acc: 0.0000e+00 - val_loss: 0.5586 - val_acc: 0.0000e+00\n",
      "Epoch 78/170\n",
      " - 0s - loss: 0.5193 - acc: 0.0000e+00 - val_loss: 0.5578 - val_acc: 0.0000e+00\n",
      "Epoch 79/170\n",
      " - 0s - loss: 0.5163 - acc: 0.0000e+00 - val_loss: 0.5559 - val_acc: 0.0000e+00\n",
      "Epoch 80/170\n",
      " - 0s - loss: 0.5134 - acc: 0.0000e+00 - val_loss: 0.5553 - val_acc: 0.0000e+00\n",
      "Epoch 81/170\n",
      " - 0s - loss: 0.5112 - acc: 0.0000e+00 - val_loss: 0.5547 - val_acc: 0.0000e+00\n",
      "Epoch 82/170\n",
      " - 0s - loss: 0.5083 - acc: 0.0000e+00 - val_loss: 0.5533 - val_acc: 0.0000e+00\n",
      "Epoch 83/170\n",
      " - 0s - loss: 0.5062 - acc: 0.0000e+00 - val_loss: 0.5522 - val_acc: 0.0000e+00\n",
      "Epoch 84/170\n",
      " - 0s - loss: 0.5039 - acc: 0.0000e+00 - val_loss: 0.5512 - val_acc: 0.0000e+00\n",
      "Epoch 85/170\n",
      " - 0s - loss: 0.5019 - acc: 0.0000e+00 - val_loss: 0.5497 - val_acc: 0.0000e+00\n",
      "Epoch 86/170\n",
      " - 0s - loss: 0.5004 - acc: 0.0000e+00 - val_loss: 0.5487 - val_acc: 0.0000e+00\n",
      "Epoch 87/170\n",
      " - 0s - loss: 0.4983 - acc: 0.0000e+00 - val_loss: 0.5486 - val_acc: 0.0000e+00\n",
      "Epoch 88/170\n",
      " - 0s - loss: 0.4968 - acc: 0.0000e+00 - val_loss: 0.5480 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/170\n",
      " - 0s - loss: 0.4939 - acc: 0.0000e+00 - val_loss: 0.5473 - val_acc: 0.0000e+00\n",
      "Epoch 90/170\n",
      " - 0s - loss: 0.4932 - acc: 0.0000e+00 - val_loss: 0.5462 - val_acc: 0.0000e+00\n",
      "Epoch 91/170\n",
      " - 0s - loss: 0.4908 - acc: 0.0000e+00 - val_loss: 0.5469 - val_acc: 0.0000e+00\n",
      "Epoch 92/170\n",
      " - 0s - loss: 0.4895 - acc: 0.0000e+00 - val_loss: 0.5461 - val_acc: 0.0000e+00\n",
      "Epoch 93/170\n",
      " - 0s - loss: 0.4880 - acc: 0.0000e+00 - val_loss: 0.5459 - val_acc: 0.0000e+00\n",
      "Epoch 94/170\n",
      " - 0s - loss: 0.4863 - acc: 0.0000e+00 - val_loss: 0.5453 - val_acc: 0.0000e+00\n",
      "Epoch 95/170\n",
      " - 0s - loss: 0.4846 - acc: 0.0000e+00 - val_loss: 0.5446 - val_acc: 0.0000e+00\n",
      "Epoch 96/170\n",
      " - 0s - loss: 0.4838 - acc: 0.0000e+00 - val_loss: 0.5445 - val_acc: 0.0000e+00\n",
      "Epoch 97/170\n",
      " - 0s - loss: 0.4822 - acc: 0.0000e+00 - val_loss: 0.5445 - val_acc: 0.0000e+00\n",
      "Epoch 98/170\n",
      " - 0s - loss: 0.4813 - acc: 0.0000e+00 - val_loss: 0.5443 - val_acc: 0.0000e+00\n",
      "Epoch 99/170\n",
      " - 0s - loss: 0.4800 - acc: 0.0000e+00 - val_loss: 0.5433 - val_acc: 0.0000e+00\n",
      "Epoch 100/170\n",
      " - 0s - loss: 0.4791 - acc: 0.0000e+00 - val_loss: 0.5433 - val_acc: 0.0000e+00\n",
      "Epoch 101/170\n",
      " - 0s - loss: 0.4773 - acc: 0.0000e+00 - val_loss: 0.5428 - val_acc: 0.0000e+00\n",
      "Epoch 102/170\n",
      " - 0s - loss: 0.4764 - acc: 0.0000e+00 - val_loss: 0.5431 - val_acc: 0.0000e+00\n",
      "Epoch 103/170\n",
      " - 0s - loss: 0.4756 - acc: 0.0000e+00 - val_loss: 0.5430 - val_acc: 0.0000e+00\n",
      "Epoch 104/170\n",
      " - 0s - loss: 0.4742 - acc: 0.0000e+00 - val_loss: 0.5425 - val_acc: 0.0000e+00\n",
      "Epoch 105/170\n",
      " - 0s - loss: 0.4735 - acc: 0.0000e+00 - val_loss: 0.5430 - val_acc: 0.0000e+00\n",
      "Epoch 106/170\n",
      " - 0s - loss: 0.4726 - acc: 0.0000e+00 - val_loss: 0.5431 - val_acc: 0.0000e+00\n",
      "Epoch 107/170\n",
      " - 0s - loss: 0.4712 - acc: 0.0000e+00 - val_loss: 0.5431 - val_acc: 0.0000e+00\n",
      "Epoch 108/170\n",
      " - 0s - loss: 0.4709 - acc: 0.0000e+00 - val_loss: 0.5437 - val_acc: 0.0000e+00\n",
      "Epoch 109/170\n",
      " - 0s - loss: 0.4697 - acc: 0.0000e+00 - val_loss: 0.5435 - val_acc: 0.0000e+00\n",
      "Epoch 110/170\n",
      " - 0s - loss: 0.4694 - acc: 0.0000e+00 - val_loss: 0.5437 - val_acc: 0.0000e+00\n",
      "Epoch 111/170\n",
      " - 0s - loss: 0.4686 - acc: 0.0000e+00 - val_loss: 0.5434 - val_acc: 0.0000e+00\n",
      "Epoch 112/170\n",
      " - 0s - loss: 0.4674 - acc: 0.0000e+00 - val_loss: 0.5437 - val_acc: 0.0000e+00\n",
      "Epoch 113/170\n",
      " - 0s - loss: 0.4672 - acc: 0.0000e+00 - val_loss: 0.5439 - val_acc: 0.0000e+00\n",
      "Epoch 114/170\n",
      " - 0s - loss: 0.4662 - acc: 0.0000e+00 - val_loss: 0.5437 - val_acc: 0.0000e+00\n",
      "Epoch 115/170\n",
      " - 0s - loss: 0.4656 - acc: 0.0000e+00 - val_loss: 0.5443 - val_acc: 0.0000e+00\n",
      "Epoch 116/170\n",
      " - 0s - loss: 0.4650 - acc: 0.0000e+00 - val_loss: 0.5446 - val_acc: 0.0000e+00\n",
      "Epoch 117/170\n",
      " - 0s - loss: 0.4653 - acc: 0.0000e+00 - val_loss: 0.5448 - val_acc: 0.0000e+00\n",
      "Epoch 118/170\n",
      " - 0s - loss: 0.4638 - acc: 0.0000e+00 - val_loss: 0.5456 - val_acc: 0.0000e+00\n",
      "Epoch 119/170\n",
      " - 0s - loss: 0.4636 - acc: 0.0000e+00 - val_loss: 0.5446 - val_acc: 0.0000e+00\n",
      "Epoch 120/170\n",
      " - 0s - loss: 0.4627 - acc: 0.0000e+00 - val_loss: 0.5452 - val_acc: 0.0000e+00\n",
      "Epoch 121/170\n",
      " - 0s - loss: 0.4624 - acc: 0.0000e+00 - val_loss: 0.5458 - val_acc: 0.0000e+00\n",
      "Epoch 122/170\n",
      " - 0s - loss: 0.4623 - acc: 0.0000e+00 - val_loss: 0.5458 - val_acc: 0.0000e+00\n",
      "Epoch 123/170\n",
      " - 0s - loss: 0.4617 - acc: 0.0000e+00 - val_loss: 0.5464 - val_acc: 0.0000e+00\n",
      "Epoch 124/170\n",
      " - 0s - loss: 0.4611 - acc: 0.0000e+00 - val_loss: 0.5469 - val_acc: 0.0000e+00\n",
      "Epoch 125/170\n",
      " - 0s - loss: 0.4607 - acc: 0.0000e+00 - val_loss: 0.5468 - val_acc: 0.0000e+00\n",
      "Epoch 126/170\n",
      " - 0s - loss: 0.4604 - acc: 0.0000e+00 - val_loss: 0.5479 - val_acc: 0.0000e+00\n",
      "Epoch 127/170\n",
      " - 0s - loss: 0.4604 - acc: 0.0000e+00 - val_loss: 0.5478 - val_acc: 0.0000e+00\n",
      "Epoch 128/170\n",
      " - 0s - loss: 0.4600 - acc: 0.0000e+00 - val_loss: 0.5489 - val_acc: 0.0000e+00\n",
      "Epoch 129/170\n",
      " - 0s - loss: 0.4588 - acc: 0.0000e+00 - val_loss: 0.5486 - val_acc: 0.0000e+00\n",
      "Epoch 130/170\n",
      " - 0s - loss: 0.4594 - acc: 0.0000e+00 - val_loss: 0.5487 - val_acc: 0.0000e+00\n",
      "Epoch 131/170\n",
      " - 0s - loss: 0.4581 - acc: 0.0000e+00 - val_loss: 0.5495 - val_acc: 0.0000e+00\n",
      "Epoch 132/170\n",
      " - 0s - loss: 0.4577 - acc: 0.0000e+00 - val_loss: 0.5498 - val_acc: 0.0000e+00\n",
      "Epoch 133/170\n",
      " - 0s - loss: 0.4574 - acc: 0.0000e+00 - val_loss: 0.5498 - val_acc: 0.0000e+00\n",
      "Epoch 134/170\n",
      " - 0s - loss: 0.4573 - acc: 0.0000e+00 - val_loss: 0.5506 - val_acc: 0.0000e+00\n",
      "Epoch 135/170\n",
      " - 0s - loss: 0.4573 - acc: 0.0000e+00 - val_loss: 0.5506 - val_acc: 0.0000e+00\n",
      "Epoch 136/170\n",
      " - 0s - loss: 0.4568 - acc: 0.0000e+00 - val_loss: 0.5508 - val_acc: 0.0000e+00\n",
      "Epoch 137/170\n",
      " - 0s - loss: 0.4572 - acc: 0.0000e+00 - val_loss: 0.5513 - val_acc: 0.0000e+00\n",
      "Epoch 138/170\n",
      " - 0s - loss: 0.4562 - acc: 0.0000e+00 - val_loss: 0.5517 - val_acc: 0.0000e+00\n",
      "Epoch 139/170\n",
      " - 0s - loss: 0.4561 - acc: 0.0000e+00 - val_loss: 0.5520 - val_acc: 0.0000e+00\n",
      "Epoch 140/170\n",
      " - 0s - loss: 0.4556 - acc: 0.0000e+00 - val_loss: 0.5526 - val_acc: 0.0000e+00\n",
      "Epoch 141/170\n",
      " - 0s - loss: 0.4564 - acc: 0.0000e+00 - val_loss: 0.5520 - val_acc: 0.0000e+00\n",
      "Epoch 142/170\n",
      " - 0s - loss: 0.4551 - acc: 0.0000e+00 - val_loss: 0.5530 - val_acc: 0.0000e+00\n",
      "Epoch 143/170\n",
      " - 0s - loss: 0.4554 - acc: 0.0000e+00 - val_loss: 0.5533 - val_acc: 0.0000e+00\n",
      "Epoch 144/170\n",
      " - 0s - loss: 0.4547 - acc: 0.0000e+00 - val_loss: 0.5537 - val_acc: 0.0000e+00\n",
      "Epoch 145/170\n",
      " - 0s - loss: 0.4549 - acc: 0.0000e+00 - val_loss: 0.5536 - val_acc: 0.0000e+00\n",
      "Epoch 146/170\n",
      " - 0s - loss: 0.4546 - acc: 0.0000e+00 - val_loss: 0.5544 - val_acc: 0.0000e+00\n",
      "Epoch 147/170\n",
      " - 0s - loss: 0.4543 - acc: 0.0000e+00 - val_loss: 0.5543 - val_acc: 0.0000e+00\n",
      "Epoch 148/170\n",
      " - 0s - loss: 0.4540 - acc: 0.0000e+00 - val_loss: 0.5550 - val_acc: 0.0000e+00\n",
      "Epoch 149/170\n",
      " - 0s - loss: 0.4538 - acc: 0.0000e+00 - val_loss: 0.5551 - val_acc: 0.0000e+00\n",
      "Epoch 150/170\n",
      " - 0s - loss: 0.4534 - acc: 0.0000e+00 - val_loss: 0.5549 - val_acc: 0.0000e+00\n",
      "Epoch 151/170\n",
      " - 0s - loss: 0.4535 - acc: 0.0000e+00 - val_loss: 0.5556 - val_acc: 0.0000e+00\n",
      "Epoch 152/170\n",
      " - 0s - loss: 0.4537 - acc: 0.0000e+00 - val_loss: 0.5553 - val_acc: 0.0000e+00\n",
      "Epoch 153/170\n",
      " - 0s - loss: 0.4532 - acc: 0.0000e+00 - val_loss: 0.5564 - val_acc: 0.0000e+00\n",
      "Epoch 154/170\n",
      " - 0s - loss: 0.4530 - acc: 0.0000e+00 - val_loss: 0.5563 - val_acc: 0.0000e+00\n",
      "Epoch 155/170\n",
      " - 0s - loss: 0.4528 - acc: 0.0000e+00 - val_loss: 0.5568 - val_acc: 0.0000e+00\n",
      "Epoch 156/170\n",
      " - 0s - loss: 0.4527 - acc: 0.0000e+00 - val_loss: 0.5571 - val_acc: 0.0000e+00\n",
      "Epoch 157/170\n",
      " - 0s - loss: 0.4527 - acc: 0.0000e+00 - val_loss: 0.5576 - val_acc: 0.0000e+00\n",
      "Epoch 158/170\n",
      " - 0s - loss: 0.4531 - acc: 0.0000e+00 - val_loss: 0.5575 - val_acc: 0.0000e+00\n",
      "Epoch 159/170\n",
      " - 0s - loss: 0.4524 - acc: 0.0000e+00 - val_loss: 0.5576 - val_acc: 0.0000e+00\n",
      "Epoch 160/170\n",
      " - 0s - loss: 0.4525 - acc: 0.0000e+00 - val_loss: 0.5582 - val_acc: 0.0000e+00\n",
      "Epoch 161/170\n",
      " - 0s - loss: 0.4524 - acc: 0.0000e+00 - val_loss: 0.5587 - val_acc: 0.0000e+00\n",
      "Epoch 162/170\n",
      " - 0s - loss: 0.4521 - acc: 0.0000e+00 - val_loss: 0.5590 - val_acc: 0.0000e+00\n",
      "Epoch 163/170\n",
      " - 0s - loss: 0.4522 - acc: 0.0000e+00 - val_loss: 0.5585 - val_acc: 0.0000e+00\n",
      "Epoch 164/170\n",
      " - 0s - loss: 0.4525 - acc: 0.0000e+00 - val_loss: 0.5586 - val_acc: 0.0000e+00\n",
      "Epoch 165/170\n",
      " - 0s - loss: 0.4520 - acc: 0.0000e+00 - val_loss: 0.5595 - val_acc: 0.0000e+00\n",
      "Epoch 166/170\n",
      " - 0s - loss: 0.4516 - acc: 0.0000e+00 - val_loss: 0.5607 - val_acc: 0.0000e+00\n",
      "Epoch 167/170\n",
      " - 0s - loss: 0.4520 - acc: 0.0000e+00 - val_loss: 0.5597 - val_acc: 0.0000e+00\n",
      "Epoch 168/170\n",
      " - 0s - loss: 0.4521 - acc: 0.0000e+00 - val_loss: 0.5604 - val_acc: 0.0000e+00\n",
      "Epoch 169/170\n",
      " - 0s - loss: 0.4513 - acc: 0.0000e+00 - val_loss: 0.5608 - val_acc: 0.0000e+00\n",
      "Epoch 170/170\n",
      " - 0s - loss: 0.4514 - acc: 0.0000e+00 - val_loss: 0.5606 - val_acc: 0.0000e+00\n",
      "Test accuracy: 0.0\n",
      "Train on 176 samples, validate on 266 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 2.8030 - acc: 0.0000e+00 - val_loss: 2.1154 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      " - 0s - loss: 2.7350 - acc: 0.0000e+00 - val_loss: 2.0676 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      " - 0s - loss: 2.6646 - acc: 0.0000e+00 - val_loss: 2.0222 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      " - 0s - loss: 2.6021 - acc: 0.0000e+00 - val_loss: 1.9775 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      " - 0s - loss: 2.5360 - acc: 0.0000e+00 - val_loss: 1.9353 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      " - 0s - loss: 2.4758 - acc: 0.0000e+00 - val_loss: 1.8944 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      " - 0s - loss: 2.4168 - acc: 0.0000e+00 - val_loss: 1.8551 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      " - 0s - loss: 2.3611 - acc: 0.0000e+00 - val_loss: 1.8173 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      " - 0s - loss: 2.3044 - acc: 0.0000e+00 - val_loss: 1.7816 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      " - 0s - loss: 2.2538 - acc: 0.0000e+00 - val_loss: 1.7467 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      " - 0s - loss: 2.2037 - acc: 0.0000e+00 - val_loss: 1.7129 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      " - 0s - loss: 2.1562 - acc: 0.0000e+00 - val_loss: 1.6803 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      " - 0s - loss: 2.1104 - acc: 0.0000e+00 - val_loss: 1.6489 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      " - 0s - loss: 2.0637 - acc: 0.0000e+00 - val_loss: 1.6192 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      " - 0s - loss: 2.0216 - acc: 0.0000e+00 - val_loss: 1.5903 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      " - 0s - loss: 1.9791 - acc: 0.0000e+00 - val_loss: 1.5629 - val_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      " - 0s - loss: 1.9408 - acc: 0.0000e+00 - val_loss: 1.5359 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      " - 0s - loss: 1.9026 - acc: 0.0000e+00 - val_loss: 1.5097 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      " - 0s - loss: 1.8641 - acc: 0.0000e+00 - val_loss: 1.4847 - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      " - 0s - loss: 1.8284 - acc: 0.0000e+00 - val_loss: 1.4602 - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      " - 0s - loss: 1.7940 - acc: 0.0000e+00 - val_loss: 1.4367 - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      " - 0s - loss: 1.7600 - acc: 0.0000e+00 - val_loss: 1.4140 - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      " - 0s - loss: 1.7283 - acc: 0.0000e+00 - val_loss: 1.3919 - val_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      " - 0s - loss: 1.6965 - acc: 0.0000e+00 - val_loss: 1.3706 - val_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      " - 0s - loss: 1.6650 - acc: 0.0000e+00 - val_loss: 1.3502 - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      " - 0s - loss: 1.6369 - acc: 0.0000e+00 - val_loss: 1.3297 - val_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      " - 0s - loss: 1.6074 - acc: 0.0000e+00 - val_loss: 1.3102 - val_acc: 0.0000e+00\n",
      "Epoch 28/100\n",
      " - 0s - loss: 1.5803 - acc: 0.0000e+00 - val_loss: 1.2910 - val_acc: 0.0000e+00\n",
      "Epoch 29/100\n",
      " - 0s - loss: 1.5517 - acc: 0.0000e+00 - val_loss: 1.2730 - val_acc: 0.0000e+00\n",
      "Epoch 30/100\n",
      " - 0s - loss: 1.5260 - acc: 0.0000e+00 - val_loss: 1.2553 - val_acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      " - 0s - loss: 1.5021 - acc: 0.0000e+00 - val_loss: 1.2372 - val_acc: 0.0000e+00\n",
      "Epoch 32/100\n",
      " - 0s - loss: 1.4757 - acc: 0.0000e+00 - val_loss: 1.2205 - val_acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      " - 0s - loss: 1.4530 - acc: 0.0000e+00 - val_loss: 1.2035 - val_acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      " - 0s - loss: 1.4278 - acc: 0.0000e+00 - val_loss: 1.1875 - val_acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      " - 0s - loss: 1.4066 - acc: 0.0000e+00 - val_loss: 1.1712 - val_acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      " - 0s - loss: 1.3831 - acc: 0.0000e+00 - val_loss: 1.1558 - val_acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      " - 0s - loss: 1.3615 - acc: 0.0000e+00 - val_loss: 1.1407 - val_acc: 0.0000e+00\n",
      "Epoch 38/100\n",
      " - 0s - loss: 1.3401 - acc: 0.0000e+00 - val_loss: 1.1260 - val_acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      " - 0s - loss: 1.3203 - acc: 0.0000e+00 - val_loss: 1.1116 - val_acc: 0.0000e+00\n",
      "Epoch 40/100\n",
      " - 0s - loss: 1.2988 - acc: 0.0000e+00 - val_loss: 1.0979 - val_acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      " - 0s - loss: 1.2791 - acc: 0.0000e+00 - val_loss: 1.0845 - val_acc: 0.0000e+00\n",
      "Epoch 42/100\n",
      " - 0s - loss: 1.2606 - acc: 0.0000e+00 - val_loss: 1.0711 - val_acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      " - 0s - loss: 1.2420 - acc: 0.0000e+00 - val_loss: 1.0582 - val_acc: 0.0000e+00\n",
      "Epoch 44/100\n",
      " - 0s - loss: 1.2234 - acc: 0.0000e+00 - val_loss: 1.0457 - val_acc: 0.0000e+00\n",
      "Epoch 45/100\n",
      " - 0s - loss: 1.2054 - acc: 0.0000e+00 - val_loss: 1.0335 - val_acc: 0.0000e+00\n",
      "Epoch 46/100\n",
      " - 0s - loss: 1.1883 - acc: 0.0000e+00 - val_loss: 1.0212 - val_acc: 0.0000e+00\n",
      "Epoch 47/100\n",
      " - 0s - loss: 1.1713 - acc: 0.0000e+00 - val_loss: 1.0095 - val_acc: 0.0000e+00\n",
      "Epoch 48/100\n",
      " - 0s - loss: 1.1542 - acc: 0.0000e+00 - val_loss: 0.9979 - val_acc: 0.0000e+00\n",
      "Epoch 49/100\n",
      " - 0s - loss: 1.1377 - acc: 0.0000e+00 - val_loss: 0.9866 - val_acc: 0.0000e+00\n",
      "Epoch 50/100\n",
      " - 0s - loss: 1.1219 - acc: 0.0000e+00 - val_loss: 0.9756 - val_acc: 0.0000e+00\n",
      "Epoch 51/100\n",
      " - 0s - loss: 1.1059 - acc: 0.0000e+00 - val_loss: 0.9649 - val_acc: 0.0000e+00\n",
      "Epoch 52/100\n",
      " - 0s - loss: 1.0911 - acc: 0.0000e+00 - val_loss: 0.9542 - val_acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      " - 0s - loss: 1.0765 - acc: 0.0000e+00 - val_loss: 0.9437 - val_acc: 0.0000e+00\n",
      "Epoch 54/100\n",
      " - 0s - loss: 1.0609 - acc: 0.0000e+00 - val_loss: 0.9340 - val_acc: 0.0000e+00\n",
      "Epoch 55/100\n",
      " - 0s - loss: 1.0476 - acc: 0.0000e+00 - val_loss: 0.9241 - val_acc: 0.0000e+00\n",
      "Epoch 56/100\n",
      " - 0s - loss: 1.0332 - acc: 0.0000e+00 - val_loss: 0.9146 - val_acc: 0.0000e+00\n",
      "Epoch 57/100\n",
      " - 0s - loss: 1.0202 - acc: 0.0000e+00 - val_loss: 0.9051 - val_acc: 0.0000e+00\n",
      "Epoch 58/100\n",
      " - 0s - loss: 1.0069 - acc: 0.0000e+00 - val_loss: 0.8962 - val_acc: 0.0000e+00\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.9937 - acc: 0.0000e+00 - val_loss: 0.8874 - val_acc: 0.0000e+00\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.9803 - acc: 0.0000e+00 - val_loss: 0.8789 - val_acc: 0.0000e+00\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.9683 - acc: 0.0000e+00 - val_loss: 0.8706 - val_acc: 0.0000e+00\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.9562 - acc: 0.0000e+00 - val_loss: 0.8625 - val_acc: 0.0000e+00\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.9448 - acc: 0.0000e+00 - val_loss: 0.8543 - val_acc: 0.0000e+00\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.9329 - acc: 0.0000e+00 - val_loss: 0.8466 - val_acc: 0.0000e+00\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.9219 - acc: 0.0000e+00 - val_loss: 0.8388 - val_acc: 0.0000e+00\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.9097 - acc: 0.0000e+00 - val_loss: 0.8318 - val_acc: 0.0000e+00\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.8995 - acc: 0.0000e+00 - val_loss: 0.8248 - val_acc: 0.0000e+00\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.8899 - acc: 0.0000e+00 - val_loss: 0.8175 - val_acc: 0.0000e+00\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.8786 - acc: 0.0000e+00 - val_loss: 0.8106 - val_acc: 0.0000e+00\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.8684 - acc: 0.0000e+00 - val_loss: 0.8039 - val_acc: 0.0000e+00\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.8590 - acc: 0.0000e+00 - val_loss: 0.7974 - val_acc: 0.0000e+00\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.8493 - acc: 0.0000e+00 - val_loss: 0.7911 - val_acc: 0.0000e+00\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.8394 - acc: 0.0000e+00 - val_loss: 0.7850 - val_acc: 0.0000e+00\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.8304 - acc: 0.0000e+00 - val_loss: 0.7789 - val_acc: 0.0000e+00\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.8212 - acc: 0.0000e+00 - val_loss: 0.7732 - val_acc: 0.0000e+00\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.8126 - acc: 0.0000e+00 - val_loss: 0.7675 - val_acc: 0.0000e+00\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.8035 - acc: 0.0000e+00 - val_loss: 0.7620 - val_acc: 0.0000e+00\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.7949 - acc: 0.0000e+00 - val_loss: 0.7566 - val_acc: 0.0000e+00\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.7867 - acc: 0.0000e+00 - val_loss: 0.7515 - val_acc: 0.0000e+00\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.7795 - acc: 0.0000e+00 - val_loss: 0.7461 - val_acc: 0.0000e+00\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.7708 - acc: 0.0000e+00 - val_loss: 0.7411 - val_acc: 0.0000e+00\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.7635 - acc: 0.0000e+00 - val_loss: 0.7362 - val_acc: 0.0000e+00\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.7555 - acc: 0.0000e+00 - val_loss: 0.7315 - val_acc: 0.0000e+00\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.7487 - acc: 0.0000e+00 - val_loss: 0.7268 - val_acc: 0.0000e+00\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.7410 - acc: 0.0000e+00 - val_loss: 0.7224 - val_acc: 0.0000e+00\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.7338 - acc: 0.0000e+00 - val_loss: 0.7182 - val_acc: 0.0000e+00\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.7271 - acc: 0.0000e+00 - val_loss: 0.7139 - val_acc: 0.0000e+00\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.7208 - acc: 0.0000e+00 - val_loss: 0.7096 - val_acc: 0.0000e+00\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.7134 - acc: 0.0000e+00 - val_loss: 0.7057 - val_acc: 0.0000e+00\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.7073 - acc: 0.0000e+00 - val_loss: 0.7018 - val_acc: 0.0000e+00\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.7008 - acc: 0.0000e+00 - val_loss: 0.6980 - val_acc: 0.0000e+00\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.6951 - acc: 0.0000e+00 - val_loss: 0.6944 - val_acc: 0.0000e+00\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.6889 - acc: 0.0000e+00 - val_loss: 0.6907 - val_acc: 0.0000e+00\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.6830 - acc: 0.0000e+00 - val_loss: 0.6874 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100\n",
      " - 0s - loss: 0.6772 - acc: 0.0000e+00 - val_loss: 0.6841 - val_acc: 0.0000e+00\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.6718 - acc: 0.0000e+00 - val_loss: 0.6807 - val_acc: 0.0000e+00\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.6665 - acc: 0.0000e+00 - val_loss: 0.6776 - val_acc: 0.0000e+00\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.6608 - acc: 0.0000e+00 - val_loss: 0.6747 - val_acc: 0.0000e+00\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.6558 - acc: 0.0000e+00 - val_loss: 0.6716 - val_acc: 0.0000e+00\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.6508 - acc: 0.0000e+00 - val_loss: 0.6686 - val_acc: 0.0000e+00\n",
      "Test accuracy: 0.0\n",
      "Train on 176 samples, validate on 266 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 5.3148 - acc: 0.0000e+00 - val_loss: 3.6639 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      " - 0s - loss: 4.5647 - acc: 0.0000e+00 - val_loss: 3.1159 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      " - 0s - loss: 3.8448 - acc: 0.0000e+00 - val_loss: 2.6434 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      " - 0s - loss: 3.2392 - acc: 0.0000e+00 - val_loss: 2.2378 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      " - 0s - loss: 2.7215 - acc: 0.0000e+00 - val_loss: 1.9033 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      " - 0s - loss: 2.2965 - acc: 0.0000e+00 - val_loss: 1.6285 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      " - 0s - loss: 1.9373 - acc: 0.0000e+00 - val_loss: 1.4080 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      " - 0s - loss: 1.6528 - acc: 0.0000e+00 - val_loss: 1.2294 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      " - 0s - loss: 1.4198 - acc: 0.0000e+00 - val_loss: 1.0908 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      " - 0s - loss: 1.2332 - acc: 0.0000e+00 - val_loss: 0.9833 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      " - 0s - loss: 1.0887 - acc: 0.0000e+00 - val_loss: 0.8992 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.9715 - acc: 0.0000e+00 - val_loss: 0.8368 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.8836 - acc: 0.0000e+00 - val_loss: 0.7885 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.8119 - acc: 0.0000e+00 - val_loss: 0.7533 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.7584 - acc: 0.0000e+00 - val_loss: 0.7261 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.7163 - acc: 0.0000e+00 - val_loss: 0.7058 - val_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.6830 - acc: 0.0000e+00 - val_loss: 0.6909 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.6583 - acc: 0.0000e+00 - val_loss: 0.6789 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.6366 - acc: 0.0000e+00 - val_loss: 0.6702 - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.6204 - acc: 0.0000e+00 - val_loss: 0.6629 - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.6063 - acc: 0.0000e+00 - val_loss: 0.6562 - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.5941 - acc: 0.0000e+00 - val_loss: 0.6508 - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.5840 - acc: 0.0000e+00 - val_loss: 0.6459 - val_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.5754 - acc: 0.0000e+00 - val_loss: 0.6416 - val_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.5679 - acc: 0.0000e+00 - val_loss: 0.6373 - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.5601 - acc: 0.0000e+00 - val_loss: 0.6332 - val_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.5532 - acc: 0.0000e+00 - val_loss: 0.6294 - val_acc: 0.0000e+00\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.5469 - acc: 0.0000e+00 - val_loss: 0.6251 - val_acc: 0.0000e+00\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.5417 - acc: 0.0000e+00 - val_loss: 0.6222 - val_acc: 0.0000e+00\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.5355 - acc: 0.0000e+00 - val_loss: 0.6188 - val_acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.5306 - acc: 0.0000e+00 - val_loss: 0.6152 - val_acc: 0.0000e+00\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.5253 - acc: 0.0000e+00 - val_loss: 0.6121 - val_acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.5211 - acc: 0.0000e+00 - val_loss: 0.6086 - val_acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.5174 - acc: 0.0000e+00 - val_loss: 0.6057 - val_acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.5129 - acc: 0.0000e+00 - val_loss: 0.6024 - val_acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.5092 - acc: 0.0000e+00 - val_loss: 0.6004 - val_acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.5058 - acc: 0.0000e+00 - val_loss: 0.5978 - val_acc: 0.0000e+00\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.5030 - acc: 0.0000e+00 - val_loss: 0.5953 - val_acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.4991 - acc: 0.0000e+00 - val_loss: 0.5918 - val_acc: 0.0000e+00\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.4965 - acc: 0.0000e+00 - val_loss: 0.5892 - val_acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.4935 - acc: 0.0000e+00 - val_loss: 0.5872 - val_acc: 0.0000e+00\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.4911 - acc: 0.0000e+00 - val_loss: 0.5854 - val_acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.4884 - acc: 0.0000e+00 - val_loss: 0.5830 - val_acc: 0.0000e+00\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.4864 - acc: 0.0000e+00 - val_loss: 0.5816 - val_acc: 0.0000e+00\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.4844 - acc: 0.0000e+00 - val_loss: 0.5800 - val_acc: 0.0000e+00\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.4823 - acc: 0.0000e+00 - val_loss: 0.5779 - val_acc: 0.0000e+00\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.4801 - acc: 0.0000e+00 - val_loss: 0.5761 - val_acc: 0.0000e+00\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.4785 - acc: 0.0000e+00 - val_loss: 0.5745 - val_acc: 0.0000e+00\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.4763 - acc: 0.0000e+00 - val_loss: 0.5727 - val_acc: 0.0000e+00\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.4749 - acc: 0.0000e+00 - val_loss: 0.5707 - val_acc: 0.0000e+00\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.4737 - acc: 0.0000e+00 - val_loss: 0.5691 - val_acc: 0.0000e+00\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.4722 - acc: 0.0000e+00 - val_loss: 0.5679 - val_acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.4707 - acc: 0.0000e+00 - val_loss: 0.5670 - val_acc: 0.0000e+00\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.4696 - acc: 0.0000e+00 - val_loss: 0.5656 - val_acc: 0.0000e+00\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.4683 - acc: 0.0000e+00 - val_loss: 0.5644 - val_acc: 0.0000e+00\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.4676 - acc: 0.0000e+00 - val_loss: 0.5630 - val_acc: 0.0000e+00\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.4665 - acc: 0.0000e+00 - val_loss: 0.5622 - val_acc: 0.0000e+00\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.4656 - acc: 0.0000e+00 - val_loss: 0.5610 - val_acc: 0.0000e+00\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.4643 - acc: 0.0000e+00 - val_loss: 0.5593 - val_acc: 0.0000e+00\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.4637 - acc: 0.0000e+00 - val_loss: 0.5579 - val_acc: 0.0000e+00\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.4627 - acc: 0.0000e+00 - val_loss: 0.5568 - val_acc: 0.0000e+00\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.4620 - acc: 0.0000e+00 - val_loss: 0.5552 - val_acc: 0.0000e+00\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.4612 - acc: 0.0000e+00 - val_loss: 0.5550 - val_acc: 0.0000e+00\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.4605 - acc: 0.0000e+00 - val_loss: 0.5547 - val_acc: 0.0000e+00\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.4596 - acc: 0.0000e+00 - val_loss: 0.5538 - val_acc: 0.0000e+00\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.4602 - acc: 0.0000e+00 - val_loss: 0.5532 - val_acc: 0.0000e+00\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.4583 - acc: 0.0000e+00 - val_loss: 0.5528 - val_acc: 0.0000e+00\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.4586 - acc: 0.0000e+00 - val_loss: 0.5523 - val_acc: 0.0000e+00\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.4575 - acc: 0.0000e+00 - val_loss: 0.5518 - val_acc: 0.0000e+00\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.4570 - acc: 0.0000e+00 - val_loss: 0.5514 - val_acc: 0.0000e+00\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.4568 - acc: 0.0000e+00 - val_loss: 0.5518 - val_acc: 0.0000e+00\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.4561 - acc: 0.0000e+00 - val_loss: 0.5506 - val_acc: 0.0000e+00\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.4556 - acc: 0.0000e+00 - val_loss: 0.5495 - val_acc: 0.0000e+00\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.4553 - acc: 0.0000e+00 - val_loss: 0.5493 - val_acc: 0.0000e+00\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.4553 - acc: 0.0000e+00 - val_loss: 0.5489 - val_acc: 0.0000e+00\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.4536 - acc: 0.0000e+00 - val_loss: 0.5484 - val_acc: 0.0000e+00\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.4543 - acc: 0.0000e+00 - val_loss: 0.5478 - val_acc: 0.0000e+00\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.4531 - acc: 0.0000e+00 - val_loss: 0.5473 - val_acc: 0.0000e+00\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.4535 - acc: 0.0000e+00 - val_loss: 0.5472 - val_acc: 0.0000e+00\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.4532 - acc: 0.0000e+00 - val_loss: 0.5468 - val_acc: 0.0000e+00\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.4521 - acc: 0.0000e+00 - val_loss: 0.5461 - val_acc: 0.0000e+00\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.4518 - acc: 0.0000e+00 - val_loss: 0.5462 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100\n",
      " - 0s - loss: 0.4521 - acc: 0.0000e+00 - val_loss: 0.5458 - val_acc: 0.0000e+00\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.4518 - acc: 0.0000e+00 - val_loss: 0.5462 - val_acc: 0.0000e+00\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.4518 - acc: 0.0000e+00 - val_loss: 0.5463 - val_acc: 0.0000e+00\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.4508 - acc: 0.0000e+00 - val_loss: 0.5458 - val_acc: 0.0000e+00\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.4505 - acc: 0.0000e+00 - val_loss: 0.5452 - val_acc: 0.0000e+00\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.4501 - acc: 0.0000e+00 - val_loss: 0.5451 - val_acc: 0.0000e+00\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.4499 - acc: 0.0000e+00 - val_loss: 0.5444 - val_acc: 0.0000e+00\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.4502 - acc: 0.0000e+00 - val_loss: 0.5440 - val_acc: 0.0000e+00\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.4497 - acc: 0.0000e+00 - val_loss: 0.5447 - val_acc: 0.0000e+00\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.4493 - acc: 0.0000e+00 - val_loss: 0.5445 - val_acc: 0.0000e+00\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.4490 - acc: 0.0000e+00 - val_loss: 0.5443 - val_acc: 0.0000e+00\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.4489 - acc: 0.0000e+00 - val_loss: 0.5444 - val_acc: 0.0000e+00\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.4488 - acc: 0.0000e+00 - val_loss: 0.5436 - val_acc: 0.0000e+00\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.4485 - acc: 0.0000e+00 - val_loss: 0.5435 - val_acc: 0.0000e+00\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.4480 - acc: 0.0000e+00 - val_loss: 0.5431 - val_acc: 0.0000e+00\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.4476 - acc: 0.0000e+00 - val_loss: 0.5423 - val_acc: 0.0000e+00\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.4478 - acc: 0.0000e+00 - val_loss: 0.5421 - val_acc: 0.0000e+00\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.4477 - acc: 0.0000e+00 - val_loss: 0.5422 - val_acc: 0.0000e+00\n",
      "Test accuracy: 0.0\n",
      "Train on 176 samples, validate on 266 samples\n",
      "Epoch 1/170\n",
      " - 0s - loss: 1.6608 - acc: 0.0000e+00 - val_loss: 1.1954 - val_acc: 0.0000e+00\n",
      "Epoch 2/170\n",
      " - 0s - loss: 1.6198 - acc: 0.0000e+00 - val_loss: 1.1673 - val_acc: 0.0000e+00\n",
      "Epoch 3/170\n",
      " - 0s - loss: 1.5783 - acc: 0.0000e+00 - val_loss: 1.1391 - val_acc: 0.0000e+00\n",
      "Epoch 4/170\n",
      " - 0s - loss: 1.5354 - acc: 0.0000e+00 - val_loss: 1.1111 - val_acc: 0.0000e+00\n",
      "Epoch 5/170\n",
      " - 0s - loss: 1.4917 - acc: 0.0000e+00 - val_loss: 1.0835 - val_acc: 0.0000e+00\n",
      "Epoch 6/170\n",
      " - 0s - loss: 1.4479 - acc: 0.0000e+00 - val_loss: 1.0568 - val_acc: 0.0000e+00\n",
      "Epoch 7/170\n",
      " - 0s - loss: 1.4080 - acc: 0.0000e+00 - val_loss: 1.0314 - val_acc: 0.0000e+00\n",
      "Epoch 8/170\n",
      " - 0s - loss: 1.3663 - acc: 0.0000e+00 - val_loss: 1.0075 - val_acc: 0.0000e+00\n",
      "Epoch 9/170\n",
      " - 0s - loss: 1.3285 - acc: 0.0000e+00 - val_loss: 0.9844 - val_acc: 0.0000e+00\n",
      "Epoch 10/170\n",
      " - 0s - loss: 1.2912 - acc: 0.0000e+00 - val_loss: 0.9626 - val_acc: 0.0000e+00\n",
      "Epoch 11/170\n",
      " - 0s - loss: 1.2558 - acc: 0.0000e+00 - val_loss: 0.9423 - val_acc: 0.0000e+00\n",
      "Epoch 12/170\n",
      " - 0s - loss: 1.2206 - acc: 0.0000e+00 - val_loss: 0.9232 - val_acc: 0.0000e+00\n",
      "Epoch 13/170\n",
      " - 0s - loss: 1.1898 - acc: 0.0000e+00 - val_loss: 0.9049 - val_acc: 0.0000e+00\n",
      "Epoch 14/170\n",
      " - 0s - loss: 1.1581 - acc: 0.0000e+00 - val_loss: 0.8876 - val_acc: 0.0000e+00\n",
      "Epoch 15/170\n",
      " - 0s - loss: 1.1269 - acc: 0.0000e+00 - val_loss: 0.8714 - val_acc: 0.0000e+00\n",
      "Epoch 16/170\n",
      " - 0s - loss: 1.0976 - acc: 0.0000e+00 - val_loss: 0.8561 - val_acc: 0.0000e+00\n",
      "Epoch 17/170\n",
      " - 0s - loss: 1.0703 - acc: 0.0000e+00 - val_loss: 0.8415 - val_acc: 0.0000e+00\n",
      "Epoch 18/170\n",
      " - 0s - loss: 1.0441 - acc: 0.0000e+00 - val_loss: 0.8275 - val_acc: 0.0000e+00\n",
      "Epoch 19/170\n",
      " - 0s - loss: 1.0197 - acc: 0.0000e+00 - val_loss: 0.8143 - val_acc: 0.0000e+00\n",
      "Epoch 20/170\n",
      " - 0s - loss: 0.9943 - acc: 0.0000e+00 - val_loss: 0.8021 - val_acc: 0.0000e+00\n",
      "Epoch 21/170\n",
      " - 0s - loss: 0.9717 - acc: 0.0000e+00 - val_loss: 0.7905 - val_acc: 0.0000e+00\n",
      "Epoch 22/170\n",
      " - 0s - loss: 0.9492 - acc: 0.0000e+00 - val_loss: 0.7796 - val_acc: 0.0000e+00\n",
      "Epoch 23/170\n",
      " - 0s - loss: 0.9283 - acc: 0.0000e+00 - val_loss: 0.7694 - val_acc: 0.0000e+00\n",
      "Epoch 24/170\n",
      " - 0s - loss: 0.9075 - acc: 0.0000e+00 - val_loss: 0.7597 - val_acc: 0.0000e+00\n",
      "Epoch 25/170\n",
      " - 0s - loss: 0.8876 - acc: 0.0000e+00 - val_loss: 0.7506 - val_acc: 0.0000e+00\n",
      "Epoch 26/170\n",
      " - 0s - loss: 0.8697 - acc: 0.0000e+00 - val_loss: 0.7418 - val_acc: 0.0000e+00\n",
      "Epoch 27/170\n",
      " - 0s - loss: 0.8518 - acc: 0.0000e+00 - val_loss: 0.7335 - val_acc: 0.0000e+00\n",
      "Epoch 28/170\n",
      " - 0s - loss: 0.8337 - acc: 0.0000e+00 - val_loss: 0.7258 - val_acc: 0.0000e+00\n",
      "Epoch 29/170\n",
      " - 0s - loss: 0.8178 - acc: 0.0000e+00 - val_loss: 0.7185 - val_acc: 0.0000e+00\n",
      "Epoch 30/170\n",
      " - 0s - loss: 0.8028 - acc: 0.0000e+00 - val_loss: 0.7116 - val_acc: 0.0000e+00\n",
      "Epoch 31/170\n",
      " - 0s - loss: 0.7874 - acc: 0.0000e+00 - val_loss: 0.7050 - val_acc: 0.0000e+00\n",
      "Epoch 32/170\n",
      " - 0s - loss: 0.7728 - acc: 0.0000e+00 - val_loss: 0.6988 - val_acc: 0.0000e+00\n",
      "Epoch 33/170\n",
      " - 0s - loss: 0.7598 - acc: 0.0000e+00 - val_loss: 0.6930 - val_acc: 0.0000e+00\n",
      "Epoch 34/170\n",
      " - 0s - loss: 0.7465 - acc: 0.0000e+00 - val_loss: 0.6876 - val_acc: 0.0000e+00\n",
      "Epoch 35/170\n",
      " - 0s - loss: 0.7338 - acc: 0.0000e+00 - val_loss: 0.6823 - val_acc: 0.0000e+00\n",
      "Epoch 36/170\n",
      " - 0s - loss: 0.7220 - acc: 0.0000e+00 - val_loss: 0.6771 - val_acc: 0.0000e+00\n",
      "Epoch 37/170\n",
      " - 0s - loss: 0.7107 - acc: 0.0000e+00 - val_loss: 0.6723 - val_acc: 0.0000e+00\n",
      "Epoch 38/170\n",
      " - 0s - loss: 0.6994 - acc: 0.0000e+00 - val_loss: 0.6679 - val_acc: 0.0000e+00\n",
      "Epoch 39/170\n",
      " - 0s - loss: 0.6901 - acc: 0.0000e+00 - val_loss: 0.6636 - val_acc: 0.0000e+00\n",
      "Epoch 40/170\n",
      " - 0s - loss: 0.6789 - acc: 0.0000e+00 - val_loss: 0.6594 - val_acc: 0.0000e+00\n",
      "Epoch 41/170\n",
      " - 0s - loss: 0.6700 - acc: 0.0000e+00 - val_loss: 0.6553 - val_acc: 0.0000e+00\n",
      "Epoch 42/170\n",
      " - 0s - loss: 0.6604 - acc: 0.0000e+00 - val_loss: 0.6515 - val_acc: 0.0000e+00\n",
      "Epoch 43/170\n",
      " - 0s - loss: 0.6513 - acc: 0.0000e+00 - val_loss: 0.6480 - val_acc: 0.0000e+00\n",
      "Epoch 44/170\n",
      " - 0s - loss: 0.6429 - acc: 0.0000e+00 - val_loss: 0.6446 - val_acc: 0.0000e+00\n",
      "Epoch 45/170\n",
      " - 0s - loss: 0.6353 - acc: 0.0000e+00 - val_loss: 0.6414 - val_acc: 0.0000e+00\n",
      "Epoch 46/170\n",
      " - 0s - loss: 0.6268 - acc: 0.0000e+00 - val_loss: 0.6383 - val_acc: 0.0000e+00\n",
      "Epoch 47/170\n",
      " - 0s - loss: 0.6202 - acc: 0.0000e+00 - val_loss: 0.6354 - val_acc: 0.0000e+00\n",
      "Epoch 48/170\n",
      " - 0s - loss: 0.6131 - acc: 0.0000e+00 - val_loss: 0.6325 - val_acc: 0.0000e+00\n",
      "Epoch 49/170\n",
      " - 0s - loss: 0.6055 - acc: 0.0000e+00 - val_loss: 0.6298 - val_acc: 0.0000e+00\n",
      "Epoch 50/170\n",
      " - 0s - loss: 0.5997 - acc: 0.0000e+00 - val_loss: 0.6273 - val_acc: 0.0000e+00\n",
      "Epoch 51/170\n",
      " - 0s - loss: 0.5930 - acc: 0.0000e+00 - val_loss: 0.6248 - val_acc: 0.0000e+00\n",
      "Epoch 52/170\n",
      " - 0s - loss: 0.5877 - acc: 0.0000e+00 - val_loss: 0.6224 - val_acc: 0.0000e+00\n",
      "Epoch 53/170\n",
      " - 0s - loss: 0.5817 - acc: 0.0000e+00 - val_loss: 0.6202 - val_acc: 0.0000e+00\n",
      "Epoch 54/170\n",
      " - 0s - loss: 0.5770 - acc: 0.0000e+00 - val_loss: 0.6180 - val_acc: 0.0000e+00\n",
      "Epoch 55/170\n",
      " - 0s - loss: 0.5716 - acc: 0.0000e+00 - val_loss: 0.6156 - val_acc: 0.0000e+00\n",
      "Epoch 56/170\n",
      " - 0s - loss: 0.5661 - acc: 0.0000e+00 - val_loss: 0.6134 - val_acc: 0.0000e+00\n",
      "Epoch 57/170\n",
      " - 0s - loss: 0.5611 - acc: 0.0000e+00 - val_loss: 0.6114 - val_acc: 0.0000e+00\n",
      "Epoch 58/170\n",
      " - 0s - loss: 0.5568 - acc: 0.0000e+00 - val_loss: 0.6094 - val_acc: 0.0000e+00\n",
      "Epoch 59/170\n",
      " - 0s - loss: 0.5529 - acc: 0.0000e+00 - val_loss: 0.6074 - val_acc: 0.0000e+00\n",
      "Epoch 60/170\n",
      " - 0s - loss: 0.5486 - acc: 0.0000e+00 - val_loss: 0.6057 - val_acc: 0.0000e+00\n",
      "Epoch 61/170\n",
      " - 0s - loss: 0.5443 - acc: 0.0000e+00 - val_loss: 0.6041 - val_acc: 0.0000e+00\n",
      "Epoch 62/170\n",
      " - 0s - loss: 0.5407 - acc: 0.0000e+00 - val_loss: 0.6025 - val_acc: 0.0000e+00\n",
      "Epoch 63/170\n",
      " - 0s - loss: 0.5369 - acc: 0.0000e+00 - val_loss: 0.6009 - val_acc: 0.0000e+00\n",
      "Epoch 64/170\n",
      " - 0s - loss: 0.5335 - acc: 0.0000e+00 - val_loss: 0.5993 - val_acc: 0.0000e+00\n",
      "Epoch 65/170\n",
      " - 0s - loss: 0.5300 - acc: 0.0000e+00 - val_loss: 0.5978 - val_acc: 0.0000e+00\n",
      "Epoch 66/170\n",
      " - 0s - loss: 0.5263 - acc: 0.0000e+00 - val_loss: 0.5965 - val_acc: 0.0000e+00\n",
      "Epoch 67/170\n",
      " - 0s - loss: 0.5231 - acc: 0.0000e+00 - val_loss: 0.5950 - val_acc: 0.0000e+00\n",
      "Epoch 68/170\n",
      " - 0s - loss: 0.5209 - acc: 0.0000e+00 - val_loss: 0.5934 - val_acc: 0.0000e+00\n",
      "Epoch 69/170\n",
      " - 0s - loss: 0.5179 - acc: 0.0000e+00 - val_loss: 0.5922 - val_acc: 0.0000e+00\n",
      "Epoch 70/170\n",
      " - 0s - loss: 0.5144 - acc: 0.0000e+00 - val_loss: 0.5908 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/170\n",
      " - 0s - loss: 0.5124 - acc: 0.0000e+00 - val_loss: 0.5897 - val_acc: 0.0000e+00\n",
      "Epoch 72/170\n",
      " - 0s - loss: 0.5095 - acc: 0.0000e+00 - val_loss: 0.5884 - val_acc: 0.0000e+00\n",
      "Epoch 73/170\n",
      " - 0s - loss: 0.5076 - acc: 0.0000e+00 - val_loss: 0.5872 - val_acc: 0.0000e+00\n",
      "Epoch 74/170\n",
      " - 0s - loss: 0.5050 - acc: 0.0000e+00 - val_loss: 0.5862 - val_acc: 0.0000e+00\n",
      "Epoch 75/170\n",
      " - 0s - loss: 0.5029 - acc: 0.0000e+00 - val_loss: 0.5850 - val_acc: 0.0000e+00\n",
      "Epoch 76/170\n",
      " - 0s - loss: 0.5011 - acc: 0.0000e+00 - val_loss: 0.5838 - val_acc: 0.0000e+00\n",
      "Epoch 77/170\n",
      " - 0s - loss: 0.4985 - acc: 0.0000e+00 - val_loss: 0.5829 - val_acc: 0.0000e+00\n",
      "Epoch 78/170\n",
      " - 0s - loss: 0.4973 - acc: 0.0000e+00 - val_loss: 0.5820 - val_acc: 0.0000e+00\n",
      "Epoch 79/170\n",
      " - 0s - loss: 0.4950 - acc: 0.0000e+00 - val_loss: 0.5809 - val_acc: 0.0000e+00\n",
      "Epoch 80/170\n",
      " - 0s - loss: 0.4927 - acc: 0.0000e+00 - val_loss: 0.5800 - val_acc: 0.0000e+00\n",
      "Epoch 81/170\n",
      " - 0s - loss: 0.4916 - acc: 0.0000e+00 - val_loss: 0.5792 - val_acc: 0.0000e+00\n",
      "Epoch 82/170\n",
      " - 0s - loss: 0.4900 - acc: 0.0000e+00 - val_loss: 0.5783 - val_acc: 0.0000e+00\n",
      "Epoch 83/170\n",
      " - 0s - loss: 0.4878 - acc: 0.0000e+00 - val_loss: 0.5772 - val_acc: 0.0000e+00\n",
      "Epoch 84/170\n",
      " - 0s - loss: 0.4869 - acc: 0.0000e+00 - val_loss: 0.5763 - val_acc: 0.0000e+00\n",
      "Epoch 85/170\n",
      " - 0s - loss: 0.4847 - acc: 0.0000e+00 - val_loss: 0.5756 - val_acc: 0.0000e+00\n",
      "Epoch 86/170\n",
      " - 0s - loss: 0.4837 - acc: 0.0000e+00 - val_loss: 0.5749 - val_acc: 0.0000e+00\n",
      "Epoch 87/170\n",
      " - 0s - loss: 0.4820 - acc: 0.0000e+00 - val_loss: 0.5741 - val_acc: 0.0000e+00\n",
      "Epoch 88/170\n",
      " - 0s - loss: 0.4809 - acc: 0.0000e+00 - val_loss: 0.5735 - val_acc: 0.0000e+00\n",
      "Epoch 89/170\n",
      " - 0s - loss: 0.4797 - acc: 0.0000e+00 - val_loss: 0.5728 - val_acc: 0.0000e+00\n",
      "Epoch 90/170\n",
      " - 0s - loss: 0.4786 - acc: 0.0000e+00 - val_loss: 0.5722 - val_acc: 0.0000e+00\n",
      "Epoch 91/170\n",
      " - 0s - loss: 0.4777 - acc: 0.0000e+00 - val_loss: 0.5713 - val_acc: 0.0000e+00\n",
      "Epoch 92/170\n",
      " - 0s - loss: 0.4763 - acc: 0.0000e+00 - val_loss: 0.5707 - val_acc: 0.0000e+00\n",
      "Epoch 93/170\n",
      " - 0s - loss: 0.4749 - acc: 0.0000e+00 - val_loss: 0.5700 - val_acc: 0.0000e+00\n",
      "Epoch 94/170\n",
      " - 0s - loss: 0.4746 - acc: 0.0000e+00 - val_loss: 0.5694 - val_acc: 0.0000e+00\n",
      "Epoch 95/170\n",
      " - 0s - loss: 0.4732 - acc: 0.0000e+00 - val_loss: 0.5688 - val_acc: 0.0000e+00\n",
      "Epoch 96/170\n",
      " - 0s - loss: 0.4724 - acc: 0.0000e+00 - val_loss: 0.5682 - val_acc: 0.0000e+00\n",
      "Epoch 97/170\n",
      " - 0s - loss: 0.4717 - acc: 0.0000e+00 - val_loss: 0.5678 - val_acc: 0.0000e+00\n",
      "Epoch 98/170\n",
      " - 0s - loss: 0.4714 - acc: 0.0000e+00 - val_loss: 0.5672 - val_acc: 0.0000e+00\n",
      "Epoch 99/170\n",
      " - 0s - loss: 0.4697 - acc: 0.0000e+00 - val_loss: 0.5665 - val_acc: 0.0000e+00\n",
      "Epoch 100/170\n",
      " - 0s - loss: 0.4689 - acc: 0.0000e+00 - val_loss: 0.5660 - val_acc: 0.0000e+00\n",
      "Epoch 101/170\n",
      " - 0s - loss: 0.4683 - acc: 0.0000e+00 - val_loss: 0.5657 - val_acc: 0.0000e+00\n",
      "Epoch 102/170\n",
      " - 0s - loss: 0.4670 - acc: 0.0000e+00 - val_loss: 0.5652 - val_acc: 0.0000e+00\n",
      "Epoch 103/170\n",
      " - 0s - loss: 0.4667 - acc: 0.0000e+00 - val_loss: 0.5648 - val_acc: 0.0000e+00\n",
      "Epoch 104/170\n",
      " - 0s - loss: 0.4656 - acc: 0.0000e+00 - val_loss: 0.5644 - val_acc: 0.0000e+00\n",
      "Epoch 105/170\n",
      " - 0s - loss: 0.4658 - acc: 0.0000e+00 - val_loss: 0.5640 - val_acc: 0.0000e+00\n",
      "Epoch 106/170\n",
      " - 0s - loss: 0.4647 - acc: 0.0000e+00 - val_loss: 0.5635 - val_acc: 0.0000e+00\n",
      "Epoch 107/170\n",
      " - 0s - loss: 0.4637 - acc: 0.0000e+00 - val_loss: 0.5631 - val_acc: 0.0000e+00\n",
      "Epoch 108/170\n",
      " - 0s - loss: 0.4632 - acc: 0.0000e+00 - val_loss: 0.5627 - val_acc: 0.0000e+00\n",
      "Epoch 109/170\n",
      " - 0s - loss: 0.4625 - acc: 0.0000e+00 - val_loss: 0.5623 - val_acc: 0.0000e+00\n",
      "Epoch 110/170\n",
      " - 0s - loss: 0.4623 - acc: 0.0000e+00 - val_loss: 0.5620 - val_acc: 0.0000e+00\n",
      "Epoch 111/170\n",
      " - 0s - loss: 0.4619 - acc: 0.0000e+00 - val_loss: 0.5615 - val_acc: 0.0000e+00\n",
      "Epoch 112/170\n",
      " - 0s - loss: 0.4617 - acc: 0.0000e+00 - val_loss: 0.5612 - val_acc: 0.0000e+00\n",
      "Epoch 113/170\n",
      " - 0s - loss: 0.4606 - acc: 0.0000e+00 - val_loss: 0.5608 - val_acc: 0.0000e+00\n",
      "Epoch 114/170\n",
      " - 0s - loss: 0.4601 - acc: 0.0000e+00 - val_loss: 0.5604 - val_acc: 0.0000e+00\n",
      "Epoch 115/170\n",
      " - 0s - loss: 0.4597 - acc: 0.0000e+00 - val_loss: 0.5600 - val_acc: 0.0000e+00\n",
      "Epoch 116/170\n",
      " - 0s - loss: 0.4595 - acc: 0.0000e+00 - val_loss: 0.5599 - val_acc: 0.0000e+00\n",
      "Epoch 117/170\n",
      " - 0s - loss: 0.4588 - acc: 0.0000e+00 - val_loss: 0.5596 - val_acc: 0.0000e+00\n",
      "Epoch 118/170\n",
      " - 0s - loss: 0.4590 - acc: 0.0000e+00 - val_loss: 0.5594 - val_acc: 0.0000e+00\n",
      "Epoch 119/170\n",
      " - 0s - loss: 0.4580 - acc: 0.0000e+00 - val_loss: 0.5591 - val_acc: 0.0000e+00\n",
      "Epoch 120/170\n",
      " - 0s - loss: 0.4575 - acc: 0.0000e+00 - val_loss: 0.5587 - val_acc: 0.0000e+00\n",
      "Epoch 121/170\n",
      " - 0s - loss: 0.4583 - acc: 0.0000e+00 - val_loss: 0.5585 - val_acc: 0.0000e+00\n",
      "Epoch 122/170\n",
      " - 0s - loss: 0.4569 - acc: 0.0000e+00 - val_loss: 0.5583 - val_acc: 0.0000e+00\n",
      "Epoch 123/170\n",
      " - 0s - loss: 0.4563 - acc: 0.0000e+00 - val_loss: 0.5581 - val_acc: 0.0000e+00\n",
      "Epoch 124/170\n",
      " - 0s - loss: 0.4561 - acc: 0.0000e+00 - val_loss: 0.5578 - val_acc: 0.0000e+00\n",
      "Epoch 125/170\n",
      " - 0s - loss: 0.4555 - acc: 0.0000e+00 - val_loss: 0.5575 - val_acc: 0.0000e+00\n",
      "Epoch 126/170\n",
      " - 0s - loss: 0.4553 - acc: 0.0000e+00 - val_loss: 0.5571 - val_acc: 0.0000e+00\n",
      "Epoch 127/170\n",
      " - 0s - loss: 0.4552 - acc: 0.0000e+00 - val_loss: 0.5568 - val_acc: 0.0000e+00\n",
      "Epoch 128/170\n",
      " - 0s - loss: 0.4547 - acc: 0.0000e+00 - val_loss: 0.5567 - val_acc: 0.0000e+00\n",
      "Epoch 129/170\n",
      " - 0s - loss: 0.4545 - acc: 0.0000e+00 - val_loss: 0.5565 - val_acc: 0.0000e+00\n",
      "Epoch 130/170\n",
      " - 0s - loss: 0.4541 - acc: 0.0000e+00 - val_loss: 0.5564 - val_acc: 0.0000e+00\n",
      "Epoch 131/170\n",
      " - 0s - loss: 0.4539 - acc: 0.0000e+00 - val_loss: 0.5561 - val_acc: 0.0000e+00\n",
      "Epoch 132/170\n",
      " - 0s - loss: 0.4535 - acc: 0.0000e+00 - val_loss: 0.5560 - val_acc: 0.0000e+00\n",
      "Epoch 133/170\n",
      " - 0s - loss: 0.4535 - acc: 0.0000e+00 - val_loss: 0.5560 - val_acc: 0.0000e+00\n",
      "Epoch 134/170\n",
      " - 0s - loss: 0.4529 - acc: 0.0000e+00 - val_loss: 0.5559 - val_acc: 0.0000e+00\n",
      "Epoch 135/170\n",
      " - 0s - loss: 0.4528 - acc: 0.0000e+00 - val_loss: 0.5557 - val_acc: 0.0000e+00\n",
      "Epoch 136/170\n",
      " - 0s - loss: 0.4522 - acc: 0.0000e+00 - val_loss: 0.5556 - val_acc: 0.0000e+00\n",
      "Epoch 137/170\n",
      " - 0s - loss: 0.4526 - acc: 0.0000e+00 - val_loss: 0.5555 - val_acc: 0.0000e+00\n",
      "Epoch 138/170\n",
      " - 0s - loss: 0.4524 - acc: 0.0000e+00 - val_loss: 0.5554 - val_acc: 0.0000e+00\n",
      "Epoch 139/170\n",
      " - 0s - loss: 0.4521 - acc: 0.0000e+00 - val_loss: 0.5554 - val_acc: 0.0000e+00\n",
      "Epoch 140/170\n",
      " - 0s - loss: 0.4519 - acc: 0.0000e+00 - val_loss: 0.5554 - val_acc: 0.0000e+00\n",
      "Epoch 141/170\n",
      " - 0s - loss: 0.4515 - acc: 0.0000e+00 - val_loss: 0.5553 - val_acc: 0.0000e+00\n",
      "Epoch 142/170\n",
      " - 0s - loss: 0.4516 - acc: 0.0000e+00 - val_loss: 0.5551 - val_acc: 0.0000e+00\n",
      "Epoch 143/170\n",
      " - 0s - loss: 0.4515 - acc: 0.0000e+00 - val_loss: 0.5549 - val_acc: 0.0000e+00\n",
      "Epoch 144/170\n",
      " - 0s - loss: 0.4512 - acc: 0.0000e+00 - val_loss: 0.5548 - val_acc: 0.0000e+00\n",
      "Epoch 145/170\n",
      " - 0s - loss: 0.4509 - acc: 0.0000e+00 - val_loss: 0.5547 - val_acc: 0.0000e+00\n",
      "Epoch 146/170\n",
      " - 0s - loss: 0.4504 - acc: 0.0000e+00 - val_loss: 0.5544 - val_acc: 0.0000e+00\n",
      "Epoch 147/170\n",
      " - 0s - loss: 0.4505 - acc: 0.0000e+00 - val_loss: 0.5543 - val_acc: 0.0000e+00\n",
      "Epoch 148/170\n",
      " - 0s - loss: 0.4498 - acc: 0.0000e+00 - val_loss: 0.5542 - val_acc: 0.0000e+00\n",
      "Epoch 149/170\n",
      " - 0s - loss: 0.4496 - acc: 0.0000e+00 - val_loss: 0.5541 - val_acc: 0.0000e+00\n",
      "Epoch 150/170\n",
      " - 0s - loss: 0.4496 - acc: 0.0000e+00 - val_loss: 0.5542 - val_acc: 0.0000e+00\n",
      "Epoch 151/170\n",
      " - 0s - loss: 0.4498 - acc: 0.0000e+00 - val_loss: 0.5544 - val_acc: 0.0000e+00\n",
      "Epoch 152/170\n",
      " - 0s - loss: 0.4494 - acc: 0.0000e+00 - val_loss: 0.5542 - val_acc: 0.0000e+00\n",
      "Epoch 153/170\n",
      " - 0s - loss: 0.4491 - acc: 0.0000e+00 - val_loss: 0.5542 - val_acc: 0.0000e+00\n",
      "Epoch 154/170\n",
      " - 0s - loss: 0.4495 - acc: 0.0000e+00 - val_loss: 0.5541 - val_acc: 0.0000e+00\n",
      "Epoch 155/170\n",
      " - 0s - loss: 0.4489 - acc: 0.0000e+00 - val_loss: 0.5541 - val_acc: 0.0000e+00\n",
      "Epoch 156/170\n",
      " - 0s - loss: 0.4492 - acc: 0.0000e+00 - val_loss: 0.5542 - val_acc: 0.0000e+00\n",
      "Epoch 157/170\n",
      " - 0s - loss: 0.4486 - acc: 0.0000e+00 - val_loss: 0.5541 - val_acc: 0.0000e+00\n",
      "Epoch 158/170\n",
      " - 0s - loss: 0.4492 - acc: 0.0000e+00 - val_loss: 0.5542 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159/170\n",
      " - 0s - loss: 0.4482 - acc: 0.0000e+00 - val_loss: 0.5541 - val_acc: 0.0000e+00\n",
      "Epoch 160/170\n",
      " - 0s - loss: 0.4480 - acc: 0.0000e+00 - val_loss: 0.5538 - val_acc: 0.0000e+00\n",
      "Epoch 161/170\n",
      " - 0s - loss: 0.4490 - acc: 0.0000e+00 - val_loss: 0.5537 - val_acc: 0.0000e+00\n",
      "Epoch 162/170\n",
      " - 0s - loss: 0.4478 - acc: 0.0000e+00 - val_loss: 0.5535 - val_acc: 0.0000e+00\n",
      "Epoch 163/170\n",
      " - 0s - loss: 0.4477 - acc: 0.0000e+00 - val_loss: 0.5535 - val_acc: 0.0000e+00\n",
      "Epoch 164/170\n",
      " - 0s - loss: 0.4482 - acc: 0.0000e+00 - val_loss: 0.5533 - val_acc: 0.0000e+00\n",
      "Epoch 165/170\n",
      " - 0s - loss: 0.4470 - acc: 0.0000e+00 - val_loss: 0.5532 - val_acc: 0.0000e+00\n",
      "Epoch 166/170\n",
      " - 0s - loss: 0.4468 - acc: 0.0000e+00 - val_loss: 0.5532 - val_acc: 0.0000e+00\n",
      "Epoch 167/170\n",
      " - 0s - loss: 0.4473 - acc: 0.0000e+00 - val_loss: 0.5530 - val_acc: 0.0000e+00\n",
      "Epoch 168/170\n",
      " - 0s - loss: 0.4469 - acc: 0.0000e+00 - val_loss: 0.5531 - val_acc: 0.0000e+00\n",
      "Epoch 169/170\n",
      " - 0s - loss: 0.4471 - acc: 0.0000e+00 - val_loss: 0.5531 - val_acc: 0.0000e+00\n",
      "Epoch 170/170\n",
      " - 0s - loss: 0.4465 - acc: 0.0000e+00 - val_loss: 0.5529 - val_acc: 0.0000e+00\n",
      "Test accuracy: 0.0\n",
      "Train on 176 samples, validate on 266 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 0.9854 - acc: 0.0000e+00 - val_loss: 1.0209 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.9266 - acc: 0.0000e+00 - val_loss: 0.9717 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.8710 - acc: 0.0000e+00 - val_loss: 0.9245 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.8180 - acc: 0.0000e+00 - val_loss: 0.8819 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.7710 - acc: 0.0000e+00 - val_loss: 0.8439 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.7329 - acc: 0.0000e+00 - val_loss: 0.8104 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.6979 - acc: 0.0000e+00 - val_loss: 0.7813 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.6689 - acc: 0.0000e+00 - val_loss: 0.7563 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.6426 - acc: 0.0000e+00 - val_loss: 0.7350 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.6217 - acc: 0.0000e+00 - val_loss: 0.7159 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.6018 - acc: 0.0000e+00 - val_loss: 0.6993 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.5846 - acc: 0.0000e+00 - val_loss: 0.6846 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.5705 - acc: 0.0000e+00 - val_loss: 0.6716 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.5564 - acc: 0.0000e+00 - val_loss: 0.6601 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.5446 - acc: 0.0000e+00 - val_loss: 0.6497 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.5343 - acc: 0.0000e+00 - val_loss: 0.6404 - val_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.5245 - acc: 0.0000e+00 - val_loss: 0.6321 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.5153 - acc: 0.0000e+00 - val_loss: 0.6249 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.5079 - acc: 0.0000e+00 - val_loss: 0.6182 - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.5003 - acc: 0.0000e+00 - val_loss: 0.6125 - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.4939 - acc: 0.0000e+00 - val_loss: 0.6072 - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.4890 - acc: 0.0000e+00 - val_loss: 0.6026 - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.4835 - acc: 0.0000e+00 - val_loss: 0.5985 - val_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.4791 - acc: 0.0000e+00 - val_loss: 0.5949 - val_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.4749 - acc: 0.0000e+00 - val_loss: 0.5917 - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.4711 - acc: 0.0000e+00 - val_loss: 0.5889 - val_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.4675 - acc: 0.0000e+00 - val_loss: 0.5863 - val_acc: 0.0000e+00\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.4650 - acc: 0.0000e+00 - val_loss: 0.5840 - val_acc: 0.0000e+00\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.4624 - acc: 0.0000e+00 - val_loss: 0.5820 - val_acc: 0.0000e+00\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.4604 - acc: 0.0000e+00 - val_loss: 0.5802 - val_acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.4578 - acc: 0.0000e+00 - val_loss: 0.5787 - val_acc: 0.0000e+00\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.4561 - acc: 0.0000e+00 - val_loss: 0.5774 - val_acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.4540 - acc: 0.0000e+00 - val_loss: 0.5761 - val_acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.4531 - acc: 0.0000e+00 - val_loss: 0.5750 - val_acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.4513 - acc: 0.0000e+00 - val_loss: 0.5741 - val_acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.4501 - acc: 0.0000e+00 - val_loss: 0.5734 - val_acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.4492 - acc: 0.0000e+00 - val_loss: 0.5727 - val_acc: 0.0000e+00\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.4480 - acc: 0.0000e+00 - val_loss: 0.5721 - val_acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.4470 - acc: 0.0000e+00 - val_loss: 0.5716 - val_acc: 0.0000e+00\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.4469 - acc: 0.0000e+00 - val_loss: 0.5712 - val_acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.4456 - acc: 0.0000e+00 - val_loss: 0.5708 - val_acc: 0.0000e+00\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.4454 - acc: 0.0000e+00 - val_loss: 0.5703 - val_acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.4445 - acc: 0.0000e+00 - val_loss: 0.5700 - val_acc: 0.0000e+00\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.4448 - acc: 0.0000e+00 - val_loss: 0.5696 - val_acc: 0.0000e+00\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.4437 - acc: 0.0000e+00 - val_loss: 0.5693 - val_acc: 0.0000e+00\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.4434 - acc: 0.0000e+00 - val_loss: 0.5692 - val_acc: 0.0000e+00\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.4431 - acc: 0.0000e+00 - val_loss: 0.5691 - val_acc: 0.0000e+00\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.4427 - acc: 0.0000e+00 - val_loss: 0.5690 - val_acc: 0.0000e+00\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.4423 - acc: 0.0000e+00 - val_loss: 0.5689 - val_acc: 0.0000e+00\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.4421 - acc: 0.0000e+00 - val_loss: 0.5688 - val_acc: 0.0000e+00\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.4427 - acc: 0.0000e+00 - val_loss: 0.5685 - val_acc: 0.0000e+00\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.4414 - acc: 0.0000e+00 - val_loss: 0.5683 - val_acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.4413 - acc: 0.0000e+00 - val_loss: 0.5682 - val_acc: 0.0000e+00\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.4413 - acc: 0.0000e+00 - val_loss: 0.5680 - val_acc: 0.0000e+00\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.4408 - acc: 0.0000e+00 - val_loss: 0.5680 - val_acc: 0.0000e+00\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.4408 - acc: 0.0000e+00 - val_loss: 0.5679 - val_acc: 0.0000e+00\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.4407 - acc: 0.0000e+00 - val_loss: 0.5679 - val_acc: 0.0000e+00\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.4403 - acc: 0.0000e+00 - val_loss: 0.5680 - val_acc: 0.0000e+00\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.4411 - acc: 0.0000e+00 - val_loss: 0.5679 - val_acc: 0.0000e+00\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.4405 - acc: 0.0000e+00 - val_loss: 0.5680 - val_acc: 0.0000e+00\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.4401 - acc: 0.0000e+00 - val_loss: 0.5679 - val_acc: 0.0000e+00\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.4399 - acc: 0.0000e+00 - val_loss: 0.5680 - val_acc: 0.0000e+00\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.4398 - acc: 0.0000e+00 - val_loss: 0.5680 - val_acc: 0.0000e+00\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.4400 - acc: 0.0000e+00 - val_loss: 0.5680 - val_acc: 0.0000e+00\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.4396 - acc: 0.0000e+00 - val_loss: 0.5679 - val_acc: 0.0000e+00\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.4403 - acc: 0.0000e+00 - val_loss: 0.5678 - val_acc: 0.0000e+00\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.4400 - acc: 0.0000e+00 - val_loss: 0.5677 - val_acc: 0.0000e+00\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.4404 - acc: 0.0000e+00 - val_loss: 0.5678 - val_acc: 0.0000e+00\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.4397 - acc: 0.0000e+00 - val_loss: 0.5678 - val_acc: 0.0000e+00\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.4394 - acc: 0.0000e+00 - val_loss: 0.5677 - val_acc: 0.0000e+00\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.4394 - acc: 0.0000e+00 - val_loss: 0.5676 - val_acc: 0.0000e+00\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.4388 - acc: 0.0000e+00 - val_loss: 0.5675 - val_acc: 0.0000e+00\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.4392 - acc: 0.0000e+00 - val_loss: 0.5674 - val_acc: 0.0000e+00\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.4396 - acc: 0.0000e+00 - val_loss: 0.5672 - val_acc: 0.0000e+00\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.4389 - acc: 0.0000e+00 - val_loss: 0.5671 - val_acc: 0.0000e+00\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.4393 - acc: 0.0000e+00 - val_loss: 0.5671 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100\n",
      " - 0s - loss: 0.4392 - acc: 0.0000e+00 - val_loss: 0.5671 - val_acc: 0.0000e+00\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.4391 - acc: 0.0000e+00 - val_loss: 0.5669 - val_acc: 0.0000e+00\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.4399 - acc: 0.0000e+00 - val_loss: 0.5669 - val_acc: 0.0000e+00\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.4392 - acc: 0.0000e+00 - val_loss: 0.5668 - val_acc: 0.0000e+00\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.4390 - acc: 0.0000e+00 - val_loss: 0.5666 - val_acc: 0.0000e+00\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.4388 - acc: 0.0000e+00 - val_loss: 0.5664 - val_acc: 0.0000e+00\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.4388 - acc: 0.0000e+00 - val_loss: 0.5661 - val_acc: 0.0000e+00\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.4387 - acc: 0.0000e+00 - val_loss: 0.5663 - val_acc: 0.0000e+00\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.4386 - acc: 0.0000e+00 - val_loss: 0.5663 - val_acc: 0.0000e+00\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.4388 - acc: 0.0000e+00 - val_loss: 0.5662 - val_acc: 0.0000e+00\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.4384 - acc: 0.0000e+00 - val_loss: 0.5663 - val_acc: 0.0000e+00\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.4386 - acc: 0.0000e+00 - val_loss: 0.5661 - val_acc: 0.0000e+00\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.4395 - acc: 0.0000e+00 - val_loss: 0.5659 - val_acc: 0.0000e+00\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.4382 - acc: 0.0000e+00 - val_loss: 0.5661 - val_acc: 0.0000e+00\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.4390 - acc: 0.0000e+00 - val_loss: 0.5660 - val_acc: 0.0000e+00\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.4386 - acc: 0.0000e+00 - val_loss: 0.5660 - val_acc: 0.0000e+00\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.4388 - acc: 0.0000e+00 - val_loss: 0.5659 - val_acc: 0.0000e+00\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.4392 - acc: 0.0000e+00 - val_loss: 0.5659 - val_acc: 0.0000e+00\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.4387 - acc: 0.0000e+00 - val_loss: 0.5658 - val_acc: 0.0000e+00\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.4381 - acc: 0.0000e+00 - val_loss: 0.5657 - val_acc: 0.0000e+00\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.4388 - acc: 0.0000e+00 - val_loss: 0.5654 - val_acc: 0.0000e+00\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.4386 - acc: 0.0000e+00 - val_loss: 0.5654 - val_acc: 0.0000e+00\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.4387 - acc: 0.0000e+00 - val_loss: 0.5652 - val_acc: 0.0000e+00\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.4379 - acc: 0.0000e+00 - val_loss: 0.5652 - val_acc: 0.0000e+00\n",
      "Test accuracy: 0.0\n",
      "Evalutation of best performing model:\n",
      "266/266 [==============================] - 0s 4us/step\n",
      "[0.5605926444207815, 0.0]\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def data():\n",
    "    '''\n",
    "    Data providing function:\n",
    "\n",
    "    Make sure to have every relevant import statement included here and return data as\n",
    "    used in model function below. This function is separated from model() so that hyperopt\n",
    "    won't reload data for each evaluation run.\n",
    "    '''\n",
    "    from os import path\n",
    "    import pandas as pd\n",
    "    from sklearn import preprocessing\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    url = \"../data/diabetes.csv\"\n",
    "    data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "    sc = StandardScaler()\n",
    "    data = sc.fit_transform(data)\n",
    "    data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "    X = data.iloc[:,:-1]\n",
    "    Y = data.iloc[:,-1]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def model(x_train, y_train, x_test, y_test):\n",
    "    '''\n",
    "    Model providing function:\n",
    "\n",
    "    Create Keras model with double curly brackets dropped-in as needed.\n",
    "    Return value has to be a valid python dictionary with two customary keys:\n",
    "        - loss: Specify a numeric evaluation metric to be minimized\n",
    "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
    "    The last one is optional, though recommended, namely:\n",
    "        - model: specify the model just created so that we can later use it again.\n",
    "    '''\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim=10, activation='linear'))\n",
    "\n",
    "    model.compile(loss='mse', optimizer={{choice(['adam', 'nadam'])}})\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size={{choice([10, 30])}},\n",
    "              epochs={{choice([100, 170])}},\n",
    "              verbose=2,\n",
    "              validation_data=(x_test, y_test))\n",
    "    score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=5,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='experiment')\n",
    "x_train, y_train, x_test, y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Hyperas(LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from imly import dope\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import boto\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import sys\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from boto.s3.key import Key\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import LabelEncoder\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.utils.validation import column_or_1d\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import LabelEncoder\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from utils.losses import lda_loss\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import copy\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import datasets\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import experiment_automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LinearRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import mean_squared_error\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import experiment_automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import re\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from automation_script import get_dataset_info\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from imly import dope\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from utils.correlations import concordance_correlation_coefficient as ccc\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LinearRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from imly import dope\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from utils.correlations import concordance_correlation_coefficient as ccc\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import boto\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import sys\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from boto.s3.key import Key\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.datasets import make_regression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import theano.tensor as T\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import theano\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from theano.compile.ops import as_op\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adam\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l2\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'optimizer': hp.choice('optimizer', ['adam', 'nadam']),\n",
      "        'batch_size': hp.choice('batch_size', [10]),\n",
      "        'epochs': hp.choice('epochs', [100]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: '''\n",
      "  3: Data providing function:\n",
      "  4: \n",
      "  5: Make sure to have every relevant import statement included here and return data as\n",
      "  6: used in model function below. This function is separated from model() so that hyperopt\n",
      "  7: won't reload data for each evaluation run.\n",
      "  8: '''\n",
      "  9: url = \"../data/iris.csv\"\n",
      " 10: data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
      " 11: class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
      " 12: data.iloc[:,-1] = index\n",
      " 13: data = data.loc[data[4] != 2]\n",
      " 14: X = data.iloc[:,:-1]\n",
      " 15: Y = data.iloc[:,-1]\n",
      " 16: x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
      " 17: \n",
      " 18: \n",
      " 19: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     '''\n",
      "   4:     Model providing function:\n",
      "   5: \n",
      "   6:     Create Keras model with double curly brackets dropped-in as needed.\n",
      "   7:     Return value has to be a valid python dictionary with two customary keys:\n",
      "   8:         - loss: Specify a numeric evaluation metric to be minimized\n",
      "   9:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
      "  10:     The last one is optional, though recommended, namely:\n",
      "  11:         - model: specify the model just created so that we can later use it again.\n",
      "  12:     '''\n",
      "  13: \n",
      "  14:     model = Sequential()\n",
      "  15:     model.add(Dense(1, input_dim=4, activation='sigmoid',\n",
      "  16:                    kernel_regularizer=l2(1e-5)))\n",
      "  17: \n",
      "  18:     model.compile(loss=lda_loss(n_components=1, margin=1),\n",
      "  19:                  optimizer=space['optimizer'],\n",
      "  20:                  metrics=['accuracy'])\n",
      "  21: \n",
      "  22:     model.fit(x_train, y_train,\n",
      "  23:               batch_size=space['batch_size'],\n",
      "  24:               epochs=space['epochs'],\n",
      "  25:               verbose=2,\n",
      "  26:               validation_data=(x_test, y_test))\n",
      "  27:     score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
      "  28:     print('Test accuracy:', acc)\n",
      "  29:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  30: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40 samples, validate on 60 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\nanguardmode.py:150: RuntimeWarning: All-NaN slice encountered\n",
      "  return np.isinf(np.nanmax(arr)) or np.isinf(np.nanmin(arr))\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "NaN detected\nNanGuardMode found an error in the output of a node in this variable:\nElemwise{true_div,no_inplace} [id A] ''   \n |Dot22 [id B] ''   \n | |InplaceDimShuffle{1,0} [id C] ''   \n | | |Elemwise{Sub}[(0, 0)] [id D] ''   \n | |   |AdvancedSubtensor1 [id E] ''   \n | |   | |<TensorType(float32, matrix)> [id F]\n | |   | |Subtensor{int64} [id G] ''   \n | |   |   |Nonzero [id H] ''   \n | |   |   | |<TensorType(bool, vector)> [id I]\n | |   |   |Constant{0} [id J]\n | |   |Elemwise{TrueDiv}[(0, 0)] [id K] ''   \n | |     |InplaceDimShuffle{x,0} [id L] ''   \n | |     | |Sum{axis=[0], acc_dtype=float64} [id M] ''   \n | |     |   |AdvancedSubtensor1 [id E] ''   \n | |     |Elemwise{Cast{float32}} [id N] ''   \n | |       |InplaceDimShuffle{x,x} [id O] ''   \n | |         |Shape_i{1} [id P] ''   \n | |           |Nonzero [id H] ''   \n | |Elemwise{Sub}[(0, 0)] [id D] ''   \n |Elemwise{Add}[(0, 1)] [id Q] ''   \n   |TensorConstant{(1, 1) of -1.0} [id R]\n   |Elemwise{Cast{float32}} [id N] ''   \n\n\n\nApply node that caused the error: Elemwise{true_div,no_inplace}(Dot22.0, Elemwise{Add}[(0, 1)].0)\nToposort index: 13\nInputs types: [TensorType(float32, matrix), TensorType(float32, (True, True))]\nInputs shapes: [(1, 1), (1, 1)]\nInputs strides: [(4, 4), (4, 4)]\nInputs values: [array([[0.]], dtype=float32), array([[0.]], dtype=float32)]\nOutputs clients: [['output']]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\nApply node that caused the error: for{cpu,scan_fn}(Shape_i{0}.0, Elemwise{eq,no_inplace}.0, Shape_i{0}.0, Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)].0)\nToposort index: 70\nInputs types: [TensorType(int64, scalar), TensorType(bool, matrix), TensorType(int64, scalar), TensorType(float32, matrix)]\nInputs shapes: [(), (2, 10), (), (10, 1)]\nInputs strides: [(), (10, 1), (), (4, 4)]\nInputs values: [array(2, dtype=int64), 'not shown', array(2, dtype=int64), 'not shown']\nOutputs clients: [[Sum{axis=[0], acc_dtype=float64}(for{cpu,scan_fn}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\vm.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, output_subset)\u001b[0m\n\u001b[0;32m    489\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m                         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_thunk_of_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_apply\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    491\u001b[0m                         \u001b[1;32mdel\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\vm.py\u001b[0m in \u001b[0;36mrun_thunk_of_node\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    404\u001b[0m                 \u001b[0mstorage_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage_map\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_map\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m             )\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\nanguardmode.py\u001b[0m in \u001b[0;36mnan_check\u001b[1;34m(node, thunk, storage_map, compute_map)\u001b[0m\n\u001b[0;32m    272\u001b[0m                         getattr(var.tag, 'nan_guard_mode_check', True)):\n\u001b[1;32m--> 273\u001b[1;33m                     \u001b[0mdo_check_on\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\nanguardmode.py\u001b[0m in \u001b[0;36mdo_check_on\u001b[1;34m(value, nd, var)\u001b[0m\n\u001b[0;32m    260\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNanGuardMode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raise'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNanGuardMode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'pdb'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: NaN detected\nNanGuardMode found an error in the output of a node in this variable:\nElemwise{true_div,no_inplace} [id A] ''   \n |Dot22 [id B] ''   \n | |InplaceDimShuffle{1,0} [id C] ''   \n | | |Elemwise{Sub}[(0, 0)] [id D] ''   \n | |   |AdvancedSubtensor1 [id E] ''   \n | |   | |<TensorType(float32, matrix)> [id F]\n | |   | |Subtensor{int64} [id G] ''   \n | |   |   |Nonzero [id H] ''   \n | |   |   | |<TensorType(bool, vector)> [id I]\n | |   |   |Constant{0} [id J]\n | |   |Elemwise{TrueDiv}[(0, 0)] [id K] ''   \n | |     |InplaceDimShuffle{x,0} [id L] ''   \n | |     | |Sum{axis=[0], acc_dtype=float64} [id M] ''   \n | |     |   |AdvancedSubtensor1 [id E] ''   \n | |     |Elemwise{Cast{float32}} [id N] ''   \n | |       |InplaceDimShuffle{x,x} [id O] ''   \n | |         |Shape_i{1} [id P] ''   \n | |           |Nonzero [id H] ''   \n | |Elemwise{Sub}[(0, 0)] [id D] ''   \n |Elemwise{Add}[(0, 1)] [id Q] ''   \n   |TensorConstant{(1, 1) of -1.0} [id R]\n   |Elemwise{Cast{float32}} [id N] ''   \n\n\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 903\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\scan_module\\scan_op.py\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    962\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 963\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    964\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\scan_module\\scan_op.py\u001b[0m in \u001b[0;36mp\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    951\u001b[0m                                                 \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m                                                 self, node)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mscan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mscan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\vm.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, output_subset)\u001b[0m\n\u001b[0;32m    522\u001b[0m                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthunks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_apply\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 523\u001b[1;33m                             storage_map=storage_map)\n\u001b[0m\u001b[0;32m    524\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcurrent_apply\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\link.py\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    691\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 692\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    693\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\vm.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, output_subset)\u001b[0m\n\u001b[0;32m    489\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m                         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_thunk_of_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_apply\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    491\u001b[0m                         \u001b[1;32mdel\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\vm.py\u001b[0m in \u001b[0;36mrun_thunk_of_node\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    404\u001b[0m                 \u001b[0mstorage_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage_map\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_map\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m             )\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\nanguardmode.py\u001b[0m in \u001b[0;36mnan_check\u001b[1;34m(node, thunk, storage_map, compute_map)\u001b[0m\n\u001b[0;32m    272\u001b[0m                         getattr(var.tag, 'nan_guard_mode_check', True)):\n\u001b[1;32m--> 273\u001b[1;33m                     \u001b[0mdo_check_on\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\nanguardmode.py\u001b[0m in \u001b[0;36mdo_check_on\u001b[1;34m(value, nd, var)\u001b[0m\n\u001b[0;32m    260\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNanGuardMode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raise'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNanGuardMode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'pdb'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: NaN detected\nNanGuardMode found an error in the output of a node in this variable:\nElemwise{true_div,no_inplace} [id A] ''   \n |Dot22 [id B] ''   \n | |InplaceDimShuffle{1,0} [id C] ''   \n | | |Elemwise{Sub}[(0, 0)] [id D] ''   \n | |   |AdvancedSubtensor1 [id E] ''   \n | |   | |<TensorType(float32, matrix)> [id F]\n | |   | |Subtensor{int64} [id G] ''   \n | |   |   |Nonzero [id H] ''   \n | |   |   | |<TensorType(bool, vector)> [id I]\n | |   |   |Constant{0} [id J]\n | |   |Elemwise{TrueDiv}[(0, 0)] [id K] ''   \n | |     |InplaceDimShuffle{x,0} [id L] ''   \n | |     | |Sum{axis=[0], acc_dtype=float64} [id M] ''   \n | |     |   |AdvancedSubtensor1 [id E] ''   \n | |     |Elemwise{Cast{float32}} [id N] ''   \n | |       |InplaceDimShuffle{x,x} [id O] ''   \n | |         |Shape_i{1} [id P] ''   \n | |           |Nonzero [id H] ''   \n | |Elemwise{Sub}[(0, 0)] [id D] ''   \n |Elemwise{Add}[(0, 1)] [id Q] ''   \n   |TensorConstant{(1, 1) of -1.0} [id R]\n   |Elemwise{Cast{float32}} [id N] ''   \n\n\n\nApply node that caused the error: Elemwise{true_div,no_inplace}(Dot22.0, Elemwise{Add}[(0, 1)].0)\nToposort index: 13\nInputs types: [TensorType(float32, matrix), TensorType(float32, (True, True))]\nInputs shapes: [(1, 1), (1, 1)]\nInputs strides: [(4, 4), (4, 4)]\nInputs values: [array([[0.]], dtype=float32), array([[0.]], dtype=float32)]\nOutputs clients: [['output']]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bd9e63ea320b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m                                       \u001b[0mmax_evals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                                       \u001b[0mtrials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                                       notebook_name='experiment')\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Evalutation of best performing model:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperas\\optim.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(model, data, algo, max_evals, trials, functions, rseed, notebook_name, verbose, eval_space, return_space)\u001b[0m\n\u001b[0;32m     65\u001b[0m                                      \u001b[0mfull_model_string\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                                      \u001b[0mnotebook_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnotebook_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                                      verbose=verbose)\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperas\\optim.py\u001b[0m in \u001b[0;36mbase_minimizer\u001b[1;34m(model, data, functions, algo, max_evals, trials, rseed, full_model_string, notebook_name, verbose, stack)\u001b[0m\n\u001b[0;32m    131\u001b[0m              \u001b[0mtrials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m              \u001b[0mrstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m              return_argmin=True),\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[0mget_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     )\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len)\u001b[0m\n\u001b[0;32m    365\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m             \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m         )\n\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin)\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m             return_argmin=return_argmin)\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len)\u001b[0m\n\u001b[0;32m    383\u001b[0m                     max_queue_len=max_queue_len)\n\u001b[0;32m    384\u001b[0m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m                 \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstopped\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    135\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'job exception: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    838\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[1;32m--> 840\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLSquare\\cook-imly\\imly\\temp_model.py\u001b[0m in \u001b[0;36mkeras_fmin_fnct\u001b[1;34m(space)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\backend\\theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1386\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1387\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1388\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    915\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[0;32m    918\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m                 \u001b[1;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\link.py\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    690\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 692\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    693\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    901\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 903\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    905\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\scan_module\\scan_op.py\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    961\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0;32m    962\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 963\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    964\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\scan_module\\scan_op.py\u001b[0m in \u001b[0;36mp\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    950\u001b[0m                                                 \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m                                                 \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m                                                 self, node)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mscan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mscan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\vm.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, output_subset)\u001b[0m\n\u001b[0;32m    521\u001b[0m                             \u001b[0mcurrent_apply\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthunks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_apply\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 523\u001b[1;33m                             storage_map=storage_map)\n\u001b[0m\u001b[0;32m    524\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcurrent_apply\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m                         \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\link.py\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    690\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 692\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    693\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\vm.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, output_subset)\u001b[0m\n\u001b[0;32m    488\u001b[0m                     \u001b[1;31m# -- Non-lazy case: have inputs, time to compute outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m                         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_thunk_of_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_apply\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    491\u001b[0m                         \u001b[1;32mdel\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_global_stats\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\vm.py\u001b[0m in \u001b[0;36mrun_thunk_of_node\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    403\u001b[0m                 \u001b[0mthunk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthunks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m                 \u001b[0mstorage_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage_map\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_map\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m             )\n\u001b[0;32m    407\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\nanguardmode.py\u001b[0m in \u001b[0;36mnan_check\u001b[1;34m(node, thunk, storage_map, compute_map)\u001b[0m\n\u001b[0;32m    271\u001b[0m                 if (compute_map[var][0] and\n\u001b[0;32m    272\u001b[0m                         getattr(var.tag, 'nan_guard_mode_check', True)):\n\u001b[1;32m--> 273\u001b[1;33m                     \u001b[0mdo_check_on\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mnan_check_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\nanguardmode.py\u001b[0m in \u001b[0;36mdo_check_on\u001b[1;34m(value, nd, var)\u001b[0m\n\u001b[0;32m    259\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNanGuardMode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raise'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNanGuardMode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'pdb'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: NaN detected\nNanGuardMode found an error in the output of a node in this variable:\nElemwise{true_div,no_inplace} [id A] ''   \n |Dot22 [id B] ''   \n | |InplaceDimShuffle{1,0} [id C] ''   \n | | |Elemwise{Sub}[(0, 0)] [id D] ''   \n | |   |AdvancedSubtensor1 [id E] ''   \n | |   | |<TensorType(float32, matrix)> [id F]\n | |   | |Subtensor{int64} [id G] ''   \n | |   |   |Nonzero [id H] ''   \n | |   |   | |<TensorType(bool, vector)> [id I]\n | |   |   |Constant{0} [id J]\n | |   |Elemwise{TrueDiv}[(0, 0)] [id K] ''   \n | |     |InplaceDimShuffle{x,0} [id L] ''   \n | |     | |Sum{axis=[0], acc_dtype=float64} [id M] ''   \n | |     |   |AdvancedSubtensor1 [id E] ''   \n | |     |Elemwise{Cast{float32}} [id N] ''   \n | |       |InplaceDimShuffle{x,x} [id O] ''   \n | |         |Shape_i{1} [id P] ''   \n | |           |Nonzero [id H] ''   \n | |Elemwise{Sub}[(0, 0)] [id D] ''   \n |Elemwise{Add}[(0, 1)] [id Q] ''   \n   |TensorConstant{(1, 1) of -1.0} [id R]\n   |Elemwise{Cast{float32}} [id N] ''   \n\n\n\nApply node that caused the error: Elemwise{true_div,no_inplace}(Dot22.0, Elemwise{Add}[(0, 1)].0)\nToposort index: 13\nInputs types: [TensorType(float32, matrix), TensorType(float32, (True, True))]\nInputs shapes: [(1, 1), (1, 1)]\nInputs strides: [(4, 4), (4, 4)]\nInputs values: [array([[0.]], dtype=float32), array([[0.]], dtype=float32)]\nOutputs clients: [['output']]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\nApply node that caused the error: for{cpu,scan_fn}(Shape_i{0}.0, Elemwise{eq,no_inplace}.0, Shape_i{0}.0, Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)].0)\nToposort index: 70\nInputs types: [TensorType(int64, scalar), TensorType(bool, matrix), TensorType(int64, scalar), TensorType(float32, matrix)]\nInputs shapes: [(), (2, 10), (), (10, 1)]\nInputs strides: [(), (10, 1), (), (4, 4)]\nInputs values: [array(2, dtype=int64), 'not shown', array(2, dtype=int64), 'not shown']\nOutputs clients: [[Sum{axis=[0], acc_dtype=float64}(for{cpu,scan_fn}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "# from __future__ import print_function\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from utils.losses import lda_loss\n",
    "\n",
    "def data():\n",
    "    '''\n",
    "    Data providing function:\n",
    "\n",
    "    Make sure to have every relevant import statement included here and return data as\n",
    "    used in model function below. This function is separated from model() so that hyperopt\n",
    "    won't reload data for each evaluation run.\n",
    "    '''\n",
    "    url = \"../data/iris.csv\"\n",
    "    data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
    "    class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
    "    data.iloc[:,-1] = index\n",
    "    data = data.loc[data[4] != 2]\n",
    "    X = data.iloc[:,:-1]\n",
    "    Y = data.iloc[:,-1]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def model(x_train, y_train, x_test, y_test):\n",
    "    '''\n",
    "    Model providing function:\n",
    "\n",
    "    Create Keras model with double curly brackets dropped-in as needed.\n",
    "    Return value has to be a valid python dictionary with two customary keys:\n",
    "        - loss: Specify a numeric evaluation metric to be minimized\n",
    "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
    "    The last one is optional, though recommended, namely:\n",
    "        - model: specify the model just created so that we can later use it again.\n",
    "    '''\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim=4, activation='sigmoid',\n",
    "                   kernel_regularizer=l2(1e-5)))\n",
    "\n",
    "    model.compile(loss=lda_loss(n_components=1, margin=1),\n",
    "                 optimizer={{choice(['adam', 'nadam'])}},\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size={{choice([10])}},\n",
    "              epochs={{choice([100])}},\n",
    "              verbose=2,\n",
    "              validation_data=(x_test, y_test))\n",
    "    score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=5,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='experiment')\n",
    "x_train, y_train, x_test, y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "url = \"../data/iris.csv\"\n",
    "data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
    "class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
    "data.iloc[:,-1] = index\n",
    "data = data.loc[data[4] != 2]\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.60, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def make_model(config):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=1, input_dim=4, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-25 11:23:48,162\tINFO node.py:278 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-02-25_11-23-46_9918/logs.\n",
      "2019-02-25 11:23:48,275\tINFO services.py:396 -- Waiting for redis server at 127.0.0.1:23634 to respond...\n",
      "2019-02-25 11:23:48,394\tINFO services.py:396 -- Waiting for redis server at 127.0.0.1:51440 to respond...\n",
      "2019-02-25 11:23:48,396\tINFO services.py:798 -- Starting Redis shard with 10.0 GB max memory.\n",
      "2019-02-25 11:23:48,431\tINFO services.py:1360 -- Starting the Plasma object store with 3.2851689470000003 GB memory using /dev/shm.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8889/notebooks/ray_ui.ipynb?token=7e1c769c9dbab3c86415c6ebfb51e2d1bc8bf762fe807762\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': None,\n",
       " 'redis_address': '192.168.1.4:23634',\n",
       " 'object_store_address': '/tmp/ray/session_2019-02-25_11-23-46_9918/sockets/plasma_store',\n",
       " 'webui_url': 'http://localhost:8889/notebooks/ray_ui.ipynb?token=7e1c769c9dbab3c86415c6ebfb51e2d1bc8bf762fe807762',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2019-02-25_11-23-46_9918/sockets/raylet'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize tune\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit the signature of your function to match that of Tune #\n",
    "def train_iris_tune(config, reporter):\n",
    "    model = make_model(config)\n",
    "    last_checkpoint = \"weights_tune_{}.h5\".format(config)\n",
    "    model.save_weights(last_checkpoint)\n",
    "    accuracy = model.evaluate(x_train, y_train)[1]\n",
    "    reporter(mean_accuracy=accuracy, checkpoint=last_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define experiment config\n",
    "configuration = tune.Experiment(\n",
    "    \"experiment_name\",\n",
    "    run=train_iris_tune,\n",
    "    resources_per_trial={\"cpu\": 4},\n",
    "    stop={\"mean_accuracy\": 95},  # TODO: Part 1\n",
    "    config={\n",
    "        \"optimizer\": tune.grid_search(['adam', 'nadam']),\n",
    "        \"activation\": {\n",
    "            \"grid_search\": ['relu', 'sigmoid']\n",
    "        }\n",
    "    }  # TODO: Part 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-25 11:25:06,155\tINFO tune.py:135 -- Tip: to resume incomplete experiments, pass resume='prompt' or resume=True to run_experiments()\n",
      "2019-02-25 11:25:06,156\tINFO tune.py:145 -- Starting a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/8.2 GB\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/8 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "PENDING trials:\n",
      " - train_iris_tune_1_activation=sigmoid,optimizer=adam:\tPENDING\n",
      " - train_iris_tune_2_activation=relu,optimizer=nadam:\tPENDING\n",
      " - train_iris_tune_3_activation=sigmoid,optimizer=nadam:\tPENDING\n",
      "RUNNING trials:\n",
      " - train_iris_tune_0_activation=relu,optimizer=adam:\tRUNNING\n",
      "\n",
      "Result for train_iris_tune_1_activation=sigmoid,optimizer=adam:\n",
      "  checkpoint: 'weights_tune_{''optimizer'': ''adam'', ''activation'': ''sigmoid''}.h5'\n",
      "  date: 2019-02-25_11-25-09\n",
      "  done: false\n",
      "  experiment_id: 43f472c81d014bc59faf5e66095953b5\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.45\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 9955\n",
      "  time_since_restore: 2.002084493637085\n",
      "  time_this_iter_s: 2.002084493637085\n",
      "  time_total_s: 2.002084493637085\n",
      "  timestamp: 1551074109\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "Result for train_iris_tune_0_activation=relu,optimizer=adam:\n",
      "  checkpoint: 'weights_tune_{''optimizer'': ''adam'', ''activation'': ''relu''}.h5'\n",
      "  date: 2019-02-25_11-25-10\n",
      "  done: false\n",
      "  experiment_id: 3aae8497d0a04586b604e74d24398ce3\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.45\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 9958\n",
      "  time_since_restore: 3.0030441284179688\n",
      "  time_this_iter_s: 3.0030441284179688\n",
      "  time_total_s: 3.0030441284179688\n",
      "  timestamp: 1551074110\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "Result for train_iris_tune_1_activation=sigmoid,optimizer=adam:\n",
      "  checkpoint: 'weights_tune_{''optimizer'': ''adam'', ''activation'': ''sigmoid''}.h5'\n",
      "  date: 2019-02-25_11-25-10\n",
      "  done: true\n",
      "  experiment_id: 43f472c81d014bc59faf5e66095953b5\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 2\n",
      "  mean_accuracy: 0.45\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 9955\n",
      "  time_since_restore: 3.0026144981384277\n",
      "  time_this_iter_s: 1.0005300045013428\n",
      "  time_total_s: 3.0026144981384277\n",
      "  timestamp: 1551074110\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  \n",
      "Result for train_iris_tune_0_activation=relu,optimizer=adam:\n",
      "  checkpoint: 'weights_tune_{''optimizer'': ''adam'', ''activation'': ''relu''}.h5'\n",
      "  date: 2019-02-25_11-25-11\n",
      "  done: true\n",
      "  experiment_id: 3aae8497d0a04586b604e74d24398ce3\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 2\n",
      "  mean_accuracy: 0.45\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 9958\n",
      "  time_since_restore: 4.0045082569122314\n",
      "  time_this_iter_s: 1.0014641284942627\n",
      "  time_total_s: 4.0045082569122314\n",
      "  timestamp: 1551074111\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/8 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "PENDING trials:\n",
      " - train_iris_tune_3_activation=sigmoid,optimizer=nadam:\tPENDING\n",
      "RUNNING trials:\n",
      " - train_iris_tune_2_activation=relu,optimizer=nadam:\tRUNNING\n",
      "TERMINATED trials:\n",
      " - train_iris_tune_0_activation=relu,optimizer=adam:\tTERMINATED [pid=9958], 4 s, 2 iter, 0.45 acc\n",
      " - train_iris_tune_1_activation=sigmoid,optimizer=adam:\tTERMINATED [pid=9955], 3 s, 2 iter, 0.45 acc\n",
      "\n",
      "Result for train_iris_tune_2_activation=relu,optimizer=nadam:\n",
      "  checkpoint: 'weights_tune_{''optimizer'': ''nadam'', ''activation'': ''relu''}.h5'\n",
      "  date: 2019-02-25_11-25-12\n",
      "  done: false\n",
      "  experiment_id: 9b12c49f3c94474c88d57818fe10026a\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.55\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 9960\n",
      "  time_since_restore: 1.0011799335479736\n",
      "  time_this_iter_s: 1.0011799335479736\n",
      "  time_total_s: 1.0011799335479736\n",
      "  timestamp: 1551074112\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "Result for train_iris_tune_2_activation=relu,optimizer=nadam:\n",
      "  checkpoint: 'weights_tune_{''optimizer'': ''nadam'', ''activation'': ''relu''}.h5'\n",
      "  date: 2019-02-25_11-25-13\n",
      "  done: true\n",
      "  experiment_id: 9b12c49f3c94474c88d57818fe10026a\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 2\n",
      "  mean_accuracy: 0.55\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 9960\n",
      "  time_since_restore: 2.0022482872009277\n",
      "  time_this_iter_s: 1.001068353652954\n",
      "  time_total_s: 2.0022482872009277\n",
      "  timestamp: 1551074113\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  \n",
      "Result for train_iris_tune_3_activation=sigmoid,optimizer=nadam:\n",
      "  checkpoint: 'weights_tune_{''optimizer'': ''nadam'', ''activation'': ''sigmoid''}.h5'\n",
      "  date: 2019-02-25_11-25-13\n",
      "  done: false\n",
      "  experiment_id: e38abc52f3fc4c1dbd9cdfbd4c6ff6b2\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.45\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 9954\n",
      "  time_since_restore: 1.0006685256958008\n",
      "  time_this_iter_s: 1.0006685256958008\n",
      "  time_total_s: 1.0006685256958008\n",
      "  timestamp: 1551074113\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "Result for train_iris_tune_3_activation=sigmoid,optimizer=nadam:\n",
      "  checkpoint: 'weights_tune_{''optimizer'': ''nadam'', ''activation'': ''sigmoid''}.h5'\n",
      "  date: 2019-02-25_11-25-14\n",
      "  done: true\n",
      "  experiment_id: e38abc52f3fc4c1dbd9cdfbd4c6ff6b2\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 2\n",
      "  mean_accuracy: 0.45\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 9954\n",
      "  time_since_restore: 2.0018787384033203\n",
      "  time_this_iter_s: 1.0012102127075195\n",
      "  time_total_s: 2.0018787384033203\n",
      "  timestamp: 1551074114\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "TERMINATED trials:\n",
      " - train_iris_tune_0_activation=relu,optimizer=adam:\tTERMINATED [pid=9958], 4 s, 2 iter, 0.45 acc\n",
      " - train_iris_tune_1_activation=sigmoid,optimizer=adam:\tTERMINATED [pid=9955], 3 s, 2 iter, 0.45 acc\n",
      " - train_iris_tune_2_activation=relu,optimizer=nadam:\tTERMINATED [pid=9960], 2 s, 2 iter, 0.55 acc\n",
      " - train_iris_tune_3_activation=sigmoid,optimizer=nadam:\tTERMINATED [pid=9954], 2 s, 2 iter, 0.45 acc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trials = tune.run_experiments(configuration, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'optimizer': 'adam', 'activation': 'relu'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials[0].config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils from Tune #\n",
    "import os\n",
    "\n",
    "def get_best_trial(trial_list, metric):\n",
    "    \"\"\"Retrieve the best trial.\"\"\"\n",
    "    return max(trial_list, key=lambda trial: trial.last_result.get(metric, 0))\n",
    "\n",
    "def get_sorted_trials(trial_list, metric):\n",
    "    return sorted(trial_list, key=lambda trial: trial.last_result.get(metric, 0), reverse=True)\n",
    "\n",
    "\n",
    "def get_best_result(trial_list, metric):\n",
    "    \"\"\"Retrieve the last result from the best trial.\"\"\"\n",
    "    return {metric: get_best_trial(trial_list, metric).last_result[metric]}\n",
    "\n",
    "\n",
    "def get_best_model(model_creator, trial_list, metric):\n",
    "    \"\"\"Restore a model from the best trial.\"\"\"\n",
    "    sorted_trials = get_sorted_trials(trial_list, metric)\n",
    "    for best_trial in sorted_trials:\n",
    "        try:\n",
    "            print(\"Creating model...\")\n",
    "            model = model_creator(best_trial.config)\n",
    "            weights = os.path.join(best_trial.logdir, best_trial.last_result[\"checkpoint\"])\n",
    "            print(\"Loading from\", weights)\n",
    "            model.load_weights(weights)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Loading failed. Trying next model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "Loading from /home/shakkeel/ray_results/experiment_name/train_iris_tune_2_activation=relu,optimizer=nadam_2019-02-25_11-25-10h_29nt8m/weights_tune_{'optimizer': 'nadam', 'activation': 'relu'}.h5\n"
     ]
    }
   ],
   "source": [
    "# Get best model #\n",
    "final_model = get_best_model(make_model, trials, metric=\"mean_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_accuracy': 0.5333333373069763}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result = get_best_result(trials, metric=\"mean_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'events.out.tfevents.1551022839.shakkeel-TUF-GAMING-FX504GD-FX80GD.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-9d0194744034>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'events.out.tfevents.1551022839.shakkeel-TUF-GAMING-FX504GD-FX80GD.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`load_weights` requires h5py.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1157\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'events.out.tfevents.1551022839.shakkeel-TUF-GAMING-FX504GD-FX80GD.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "final_model.save_weights('test_weight_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_accuracy': 0.5333333373069763,\n",
       " 'done': True,\n",
       " 'timesteps_total': None,\n",
       " 'episodes_total': None,\n",
       " 'experiment_id': '582783da69934b25855ace6145ef9937',\n",
       " 'date': '2019-02-24_21-10-36',\n",
       " 'timestamp': 1551022836,\n",
       " 'training_iteration': 2,\n",
       " 'time_this_iter_s': 1.0006427764892578,\n",
       " 'time_total_s': 2.001708745956421,\n",
       " 'pid': 5566,\n",
       " 'hostname': 'shakkeel-TUF-GAMING-FX504GD-FX80GD',\n",
       " 'node_ip': '192.168.1.4',\n",
       " 'config': {'optimizer': 'adam', 'activation': 'relu'},\n",
       " 'time_since_restore': 2.001708745956421,\n",
       " 'timesteps_since_restore': 0,\n",
       " 'iterations_since_restore': 2}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials[0].last_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_trials = get_sorted_trials(trials, metric='mean_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/shakkeel/ray_results/experiment_name/train_iris_tune_0_activation=relu,optimizer=adam_2019-02-24_21-10-32txqiiqe4'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-24 21:50:05,379\tERROR worker.py:1632 -- The node with client ID fcae37c0a6177ae8267a4555f61db6161214821e has been marked dead because the monitor has missed too many heartbeats from it.\n"
     ]
    }
   ],
   "source": [
    "sorted_trials[0].logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=os.path.join('/home/shakkeel/ray_results/experiment_name/train_iris_tune_3_activation=sigmoid,optimizer=nadam_2019-02-24_17-42-01rtdbsg5p', 'events.out.tfevents.1551010324.shakkeel-TUF-GAMING-FX504GD-FX80GD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "Loading from /home/shakkeel/ray_results/experiment_name/train_iris_tune_3_activation=sigmoid,optimizer=nadam_2019-02-24_17-42-01rtdbsg5p/events.out.tfevents.1551010324.shakkeel-TUF-GAMING-FX504GD-FX80GD\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to open file (file signature not found)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-902c45df4eba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading from\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`load_weights` requires h5py.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1157\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (file signature not found)"
     ]
    }
   ],
   "source": [
    "print(\"Creating model...\")\n",
    "model = make_model(None)\n",
    "weights = path\n",
    "print(\"Loading from\", weights)\n",
    "model.load_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.6546545 ],\n",
       "        [0.47185266],\n",
       "        [0.24574947],\n",
       "        [0.19877207]], dtype=float32), array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_weight_model = make_model(None)\n",
    "test_weight_model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Tune integration with IMLY(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4413 - acc: 0.8500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from imly import dope\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ray import tune\n",
    "from hyperopt import hp\n",
    "import numpy as np\n",
    "\n",
    "url = \"../data/iris.csv\"\n",
    "data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
    "class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
    "data.iloc[:,-1] = index\n",
    "data = data.loc[data[4] != 2]\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.60, random_state=0)\n",
    "glm_1 = {\n",
    "    \"epochs\": tune.grid_search([100, 200]),\n",
    "    \"optimizer\": tune.grid_search([\"adam\", \"nadam\"])\n",
    "}\n",
    "\n",
    "space = {\n",
    "    \"lr\": hp.uniform(\"lr\", 0.001, 0.1),\n",
    "    'activation': hp.choice(\"activation\", [\"relu\", \"tanh\"])\n",
    "}\n",
    "\n",
    "m = dope(LogisticRegression())\n",
    "m.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wrappers.sklearn.keras_classifier.SklearnKerasClassifier at 0x7f51b06dde80>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 0s 653us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8833333373069763"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Keras MNIST Example')\n",
    "parser.add_argument('--lr', type=float, default=0.1, help='learning rate')\n",
    "parser.add_argument('--momentum', type=float, default=0.0, help='SGD momentum')\n",
    "parser.add_argument('--kernel1', type=int, default=3, help='Size of first kernel')\n",
    "parser.add_argument('--kernel2', type=int, default=3, help='Size of second kernel')\n",
    "parser.add_argument('--poolsize', type=int, default=2, help='Size of Poolin')\n",
    "parser.add_argument('--dropout1', type=float, default=0.25, help='Size of first kernel')\n",
    "parser.add_argument('--hidden', type=int, default=4, help='Size of Hidden Layer')\n",
    "parser.add_argument('--dropout2', type=float, default=0.5, help='Size of first kernel')\n",
    "\n",
    "DEFAULT_ARGS = vars(parser.parse_known_args()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.1,\n",
       " 'momentum': 0.0,\n",
       " 'kernel1': 3,\n",
       " 'kernel2': 3,\n",
       " 'poolsize': 2,\n",
       " 'dropout1': 0.25,\n",
       " 'hidden': 4,\n",
       " 'dropout2': 0.5}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-19 18:08:25,085\tERROR worker.py:1632 -- The node with client ID 5626a904d1847aa6a026fc204de5c26fb852e18b has been marked dead because the monitor has missed too many heartbeats from it.\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SklearnKerasClassifier' object has no attribute 'get_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4a6344f61237>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'SklearnKerasClassifier' object has no attribute 'get_config'"
     ]
    }
   ],
   "source": [
    "m.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Tune's compatibility with IMLY's Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "176/176 [==============================] - 0s 483us/step - loss: 1.0402 - acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from imly import dope\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ray import tune\n",
    "from hyperopt import hp\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "url = \"../data/diabetes.csv\"\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "sc = StandardScaler()\n",
    "data = sc.fit_transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.60, random_state=0)\n",
    "glm_1 = {\n",
    "    \"epochs\": tune.grid_search([100, 200]),\n",
    "    \"optimizer\": tune.grid_search([\"adam\", \"nadam\"])\n",
    "}\n",
    "\n",
    "space = {\n",
    "    \"lr\": hp.uniform(\"lr\", 0.001, 0.1),\n",
    "    'activation': hp.choice(\"activation\", [\"relu\", \"tanh\"])\n",
    "}\n",
    "\n",
    "m = dope(LinearRegression())\n",
    "m.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266/266 [==============================] - 0s 91us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3672893154890018"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing onnx deployability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl'\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_name = \"uci_iris\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "url = \"../data/iris.csv\" if path.exists(\"../data/iris.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
    "class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
    "data.iloc[:,-1] = index\n",
    "data = data.loc[data[4] != 2]\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "params = {\n",
    "    'epochs': 170\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n",
      "WARNING:tensorflow:From /home/shakkeel/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakkeel/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shakkeel/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1\n",
      "40/40 [==============================] - 1s 22ms/step - loss: 1.1980 - acc: 0.5500\n"
     ]
    }
   ],
   "source": [
    "from imly import dope\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "m = dope(model)\n",
    "m.fit(x_train, y_train)\n",
    "# m.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_pred = m.model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The maximum opset needed by this model is only 7.\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = m.save()\n",
    "\n",
    "onnx.save(onnx_model, 'deploy_test_onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_float32 = np.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.to_csv(path_or_buf='./x_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as rt\n",
    "\n",
    "sess = rt.InferenceSession('deploy_test_onnx')\n",
    "\n",
    "input_name = sess.get_inputs()[0].name\n",
    "res = sess.run(None, {input_name: x_test.values.astype(np.float32)})\n",
    "prob = res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Keras predictions')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot concordance btw onnx and keras\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "plt.scatter(prob, keras_pred)\n",
    "plt.title('Correlation plot')\n",
    "plt.xlabel('Onnx predictions')\n",
    "plt.ylabel('Keras predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006518782"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss calculation btw onnx and keras\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "mse = mean_squared_error(prob, keras_pred)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear_model to test onnx.js deployability\n",
    "\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "url = \"../data/diabetes.csv\"\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "sc = StandardScaler()\n",
    "data = sc.fit_transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.60, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(176, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shakkeel/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/shakkeel/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1\n",
      "176/176 [==============================] - 9s 53ms/step - loss: 3.7089 - acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2e42f4dd30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1,\n",
    "                input_dim=10,\n",
    "                activation='linear'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/shakkeel/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/keras2onnx/main.py:92: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.convert_variables_to_constants\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shakkeel/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/keras2onnx/main.py:92: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.convert_variables_to_constants\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shakkeel/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.extract_sub_graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shakkeel/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.extract_sub_graph\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 2 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 2 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converted 2 variables to const ops.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converted 2 variables to const ops.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shakkeel/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/tensorflow/python/util/decorator_utils.py:145: GraphKeys.VARIABLES (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.GraphKeys.GLOBAL_VARIABLES` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shakkeel/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/tensorflow/python/util/decorator_utils.py:145: GraphKeys.VARIABLES (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.GraphKeys.GLOBAL_VARIABLES` instead.\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxmltools\n",
    "onnx_model = onnxmltools.convert_keras(model)\n",
    "\n",
    "onnx.save(onnx_model, 'test_onnx_linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ir_version: 4\n",
       "producer_name: \"OnnxMLTools\"\n",
       "producer_version: \"1.3.1\"\n",
       "domain: \"onnxml\"\n",
       "model_version: 0\n",
       "doc_string: \"\"\n",
       "graph {\n",
       "  node {\n",
       "    input: \"dense_1_input_0\"\n",
       "    input: \"W\"\n",
       "    output: \"transformed_tensor\"\n",
       "    name: \"_class__keras_layers_core_Dense__\"\n",
       "    op_type: \"MatMul\"\n",
       "    domain: \"\"\n",
       "  }\n",
       "  node {\n",
       "    input: \"transformed_tensor\"\n",
       "    input: \"B\"\n",
       "    output: \"biased_tensor_name\"\n",
       "    name: \"Add\"\n",
       "    op_type: \"Add\"\n",
       "    domain: \"\"\n",
       "  }\n",
       "  node {\n",
       "    input: \"biased_tensor_name\"\n",
       "    output: \"dense_1_Sigmoid_01\"\n",
       "    name: \"Sigmoid\"\n",
       "    op_type: \"Sigmoid\"\n",
       "    domain: \"\"\n",
       "  }\n",
       "  name: \"bc3456cf278a46f88f390df4056e634c\"\n",
       "  initializer {\n",
       "    dims: 4\n",
       "    dims: 1\n",
       "    data_type: 1\n",
       "    float_data: 0.7228454947471619\n",
       "    float_data: -0.13993597030639648\n",
       "    float_data: 1.0774915218353271\n",
       "    float_data: 0.9269605278968811\n",
       "    name: \"W\"\n",
       "  }\n",
       "  initializer {\n",
       "    dims: 1\n",
       "    data_type: 1\n",
       "    float_data: -0.001932300627231598\n",
       "    name: \"B\"\n",
       "  }\n",
       "  input {\n",
       "    name: \"dense_1_input_0\"\n",
       "    type {\n",
       "      tensor_type {\n",
       "        elem_type: 1\n",
       "        shape {\n",
       "          dim {\n",
       "            dim_value: 1\n",
       "          }\n",
       "          dim {\n",
       "            dim_value: 4\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  input {\n",
       "    name: \"W\"\n",
       "    type {\n",
       "      tensor_type {\n",
       "        elem_type: 1\n",
       "        shape {\n",
       "          dim {\n",
       "            dim_value: 4\n",
       "          }\n",
       "          dim {\n",
       "            dim_value: 1\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  input {\n",
       "    name: \"B\"\n",
       "    type {\n",
       "      tensor_type {\n",
       "        elem_type: 1\n",
       "        shape {\n",
       "          dim {\n",
       "            dim_value: 1\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  output {\n",
       "    name: \"dense_1_Sigmoid_01\"\n",
       "    type {\n",
       "      tensor_type {\n",
       "        elem_type: 1\n",
       "        shape {\n",
       "          dim {\n",
       "            dim_value: 1\n",
       "          }\n",
       "          dim {\n",
       "            dim_value: 1\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "opset_import {\n",
       "  domain: \"\"\n",
       "  version: 7\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnx\n",
    "onnx_model = onnx.load('/home/shakkeel/Desktop/docker/onnx_web/onnx_model')\n",
    "# onnx_model = onnx.load('deploy_test_onnx')\n",
    "onnx_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx_tf.backend import prepare\n",
    "\n",
    "# onnx_model = onnx.load(\"input_path\")  # load onnx model\n",
    "tf_rep = prepare(onnx_model)  # prepare tf representation\n",
    "tf_rep.export_graph(\"tf_model\") # export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<onnx_tf.backend_rep.TensorflowRep at 0x7f219291be10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tf vs onnx vs keras(plot graph)\n",
    "\n",
    "# tf predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "/onnx/onnx/version_converter/BaseConverter.h:64: adapter_lookup: Assertion `false` failed: No Adapter For MatMul",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-da867a3836a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0monnx_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./test_onnx_linear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mconverted_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mversion_converter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monnx_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The model is:\\n{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/onnx/version_converter.py\u001b[0m in \u001b[0;36mconvert_version\u001b[0;34m(model, target_version)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'VersionConverter only accepts int as target_version, incorrect type: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mmodel_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mconverted_model_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted_model_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: /onnx/onnx/version_converter/BaseConverter.h:64: adapter_lookup: Assertion `false` failed: No Adapter For MatMul"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx import version_converter\n",
    "\n",
    "onnx_model = onnx.load('./test_onnx_linear')\n",
    "\n",
    "converted_model = version_converter.convert_version(onnx_model, 3)\n",
    "print('The model is:\\n{}'.format(converted_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Hyperopt functionality #\n",
    "# WILL IT WORK WITHOUT THE HYPERBAND SCHEDULER? #\n",
    "from hyperopt import hp\n",
    "from ray.tune.suggest import HyperOptSearch\n",
    "\n",
    "space = {\n",
    "    \"lr\": hp.uniform(\"lr\", 0.001, 0.1),\n",
    "    \"momentum\": hp.uniform(\"momentum\", 0.1, 0.9),\n",
    "    \"hidden\": hp.choice(\"hidden\", np.arange(16, 256, dtype=int)),\n",
    "}\n",
    "\n",
    "## TODO: CREATE A HyperOptObject\n",
    "hyperopt_search = HyperOptSearch(space, reward_attr=\"mean_accuracy\")\n",
    "\n",
    "## TODO: Pass in the object to Tune.\n",
    "good_results = tune.run_experiments(\n",
    "    configuration2, search_alg=hyperopt_search, scheduler=hyperband, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LinearRegression'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "\n",
    "model_name = 'linear_regression'\n",
    "model_mappings = {\n",
    "    'linear_regression': 'LinearRegression',\n",
    "    'logistic_regression': 'LogisticRegression'\n",
    "}\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "for key, value in model_mappings.items():\n",
    "    if key == model_name:\n",
    "        name = value\n",
    "\n",
    "module = __import__('sklearn.linear_model', fromlist=[name])\n",
    "imported_module = getattr(module, name)\n",
    "model = imported_module\n",
    "\n",
    "primal_model = model()\n",
    "\n",
    "# Primal\n",
    "primal_model.fit(x_train, y_train)\n",
    "primal_model.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "import experiment_automation_script\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "dataset_info = experiment_automation_script.get_dataset_info(\"diabetes\")\n",
    "url = \"../data/diabetes.csv\" if path.exists(\"../data/diabetes.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "sc = StandardScaler()\n",
    "data = sc.fit_transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "# diabetes = datasets.load_diabetes()\n",
    "# sc = StandardScaler()\n",
    "# diabetes = sc.fit_transform(diabetes)\n",
    "#####\n",
    "# # Use only one feature\n",
    "# diabetes_X = diabetes.data\n",
    "# # sc = StandardScaler()\n",
    "# # diabetes.data = sc.fit_transform(diabetes.data)\n",
    "\n",
    "# X = diabetes.data\n",
    "# Y = diabetes.target\n",
    "#####\n",
    "\n",
    "# X = preprocessing.scale(X)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "# # Split the data into training/testing sets\n",
    "# x_train = diabetes_X[:-20]\n",
    "# x_test = diabetes_X[-20:]\n",
    "\n",
    "# # Split the targets into training/testing sets\n",
    "# y_train = diabetes.target[:-20]\n",
    "# y_test = diabetes.target[-20:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
       "         0.01990842, -0.01764613],\n",
       "       [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
       "        -0.06832974, -0.09220405],\n",
       "       [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
       "         0.00286377, -0.02593034],\n",
       "       ...,\n",
       "       [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
       "        -0.04687948,  0.01549073],\n",
       "       [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
       "         0.04452837, -0.02593034],\n",
       "       [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
       "        -0.00421986,  0.00306441]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5481227216244245"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "score = mean_squared_error(y_test, y_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x17f81a0aeb8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm.__call__(param_name=\"log_reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import experiment_automation_script\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# dataset_info = experiment_automation_script.get_dataset_info(\"diabetes\")\n",
    "url = \"../data/diabetes.csv\" if path.exists(\"../data/diabetes.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wrappers.sklearn.keras_classifier.SklearnKerasClassifier"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "wrapper_class = 'SklearnKerasClassifier'\n",
    "\n",
    "path = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', wrapper_class)\n",
    "module_path = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', path).lower()\n",
    "package_name = module_path.split('_')[0]\n",
    "wrapper_name = '_'.join(module_path.split('_')[1:3])\n",
    "\n",
    "module_path = 'wrappers.' + package_name + '.' + wrapper_name\n",
    "module_path\n",
    "wrapper_module = __import__(module_path, fromlist=[wrapper_class])\n",
    "function = getattr(wrapper_module, wrapper_class)\n",
    "function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = 'sklearn_keras_classifier'\n",
    "'_'.join(module_path.split('_')[1:3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"../data/uci_carbon_nanotubes.csv\"\n",
    "data = pd.read_csv(url, delimiter=\";\")\n",
    "data\n",
    "# frames = [X, Y]\n",
    "# data = pd.concat(frames, axis=1)\n",
    "# data.to_csv('../data/uci_auto_mpg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot perform reduce with flexible type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-463eefa48d20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# score = m.score(x_test, y_test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLSquare\\cook-imly\\imly\\wrappers\\sklearn\\keras_classifier.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x_train, y_train, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m                                                                        \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                                                                        \u001b[0mval_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_metric\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                                                                        metric=self.metric) \n\u001b[0m\u001b[0;32m     40\u001b[0m             self.model.fit(x_train, y_train, epochs=final_epoch,\n\u001b[0;32m     41\u001b[0m                            batch_size=final_batch_size, verbose=0)\n",
      "\u001b[1;32m~\\Desktop\\MLSquare\\cook-imly\\imly\\optimizers\\talos\\talos.py\u001b[0m in \u001b[0;36mget_best_model\u001b[1;34m(x_train, y_train, **kwargs)\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0mexperiment_no\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexperiment_no\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtalos_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                 grid_downsample=0.5)\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\talos\\scan\\Scan.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, params, model, dataset_name, experiment_no, x_val, y_val, val_split, shuffle, round_limit, grid_downsample, random_method, seed, search_method, reduction_method, reduction_interval, reduction_window, reduction_threshold, reduction_metric, reduce_loss, last_epoch_value, clear_tf_session, disable_progress_bar, print_params, debug)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;31m# input parameters section ends\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_null\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mruntime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mruntime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\talos\\scan\\Scan.py\u001b[0m in \u001b[0;36mruntime\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mruntime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[0mself\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscan_prepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscan_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\talos\\scan\\scan_prepare.py\u001b[0m in \u001b[0;36mscan_prepare\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m# create the data asset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[0mself\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[1;34m(a, axis, out, keepdims, initial)\u001b[0m\n\u001b[0;32m     26\u001b[0m def _amax(a, axis=None, out=None, keepdims=False,\n\u001b[0;32m     27\u001b[0m           initial=_NoValue):\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m def _amin(a, axis=None, out=None, keepdims=False,\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot perform reduce with flexible type"
     ]
    }
   ],
   "source": [
    "### Testing concordance ###\n",
    "\n",
    "from automation_script import get_dataset_info\n",
    "from imly import dope\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_info = get_dataset_info(\"uci_iris_lda\")\n",
    "url = dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "# sc = StandardScaler()\n",
    "# data = sc.fit_transform(data)\n",
    "# data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "model = LinearDiscriminantAnalysis()\n",
    "\n",
    "m = dope(model)\n",
    "\n",
    "x_train = x_train.values\n",
    "y_train = y_train.values\n",
    "\n",
    "m.fit(x_train, y_train)\n",
    "\n",
    "# score = m.score(x_test, y_test)\n",
    "\n",
    "\n",
    "### Automation script ###\n",
    "\n",
    "# params = {\n",
    "#     'epochs': 200\n",
    "# }\n",
    "\n",
    "# experiment_automation_script.dopify(dataset_info, 'logistic_regression', X, Y, 0.60, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train)\n",
    "sklearn_pred = model.predict(x_test)\n",
    "keras_pred = m.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9989899125789394"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.correlations import concordance_correlation_coefficient as ccc\n",
    "\n",
    "ccc(sklearn_pred, keras_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x22704f6fb38>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAHJlJREFUeJzt3X+U3HV97/Hne4cJTKhlg4mSLKwBy8GSRhLcg6G55x60yo+oyRLBYLFijzbH3nJu7aE5N1w5ECg28aZVrtXWE5VTqRxEIayhxKZa4rGlDWXjJqwhpAYKZGdzZCUsFpkLm+R9/5iZZDL5fmdm5/udn9/X45yczI/PzvfDZPm8v9/39/35fMzdERGR5OlpdQdERKQ1FABERBJKAUBEJKEUAEREEkoBQEQkoRQAREQSSgFARCShFABERBJKAUBEJKFOaXUHKpk9e7bPnz+/1d0QEekYO3fu/IW7z6mlbVsHgPnz5zM8PNzqboiIdAwze77WtkoBiYgklAKAiEhCKQCIiCSUAoCISEIpAIiIJJQCgIhIQrV1GaiISFIMjWTZuG0f45M55vVmWHPFBQwu7mvoMRUARERabGgky82bR8lNHQEgO5nj5s2jAA0NAkoBiYi02MZt+44N/kW5qSNs3LavocfVFYCISBNUSvGMT+YCfybs9bjoCkBEpMGKKZ7sZA7neIpnaCQLwLzeTODPhb0eFwUAEZEGq5biWXPFBWTSqRPez6RTrLnigob2SykgEZGYhKV5qqV4iqkgVQGJiHSgSpU883ozZAOCQGmKZ3BxX8MH/HKRU0Bmdo6ZbTezvWa2x8z+OKCNmdmXzGy/mT1pZhdHPa6ISDuplOZpVYqnmjiuAA4DN7n7T8zsTcBOM/uBuz9V0uYq4PzCn3cDf1P4W0SkK1RK87QqxVNN5ADg7geBg4XH/2Vme4E+oDQArADucXcHdphZr5nNLfysiEjHq5bmaUWKp5pYq4DMbD6wGHi87K0+4EDJ87HCa0GfsdrMhs1seGJiIs7uiYg0TLumeSqJLQCY2a8BDwKfcfdflr8d8CMe9DnuvsndB9x9YM6cmra1FBFpucHFfaxfuZC+3gwG9PVmWL9yYdud9ZeKpQrIzNLkB/973X1zQJMx4JyS52cD43EcW0SkXbRjmqeSOKqADPgGsNfdvxDSbAvw8UI10BLgFeX/RURaK44rgKXA7wGjZrar8Nr/BvoB3P2rwFZgGbAfeA34/RiOKyIiEcRRBfQvBOf4S9s48EdRjyUiIvHRWkAiIgmlACAiklAKACIiCaUAICKSUAoAIiIJpQAgIpJQCgAiIgmlACAiklAKACIiCaUtIUWkq5Xv0/ued8xh+9MTbbUxS6soAIhIVykd8M/IpPnVG4eZOpJffT47meNbO1441rZ0394kBgGlgESkaxQ3Zs9O5nBgMjd1bPAPU9y3N4kUAESkawRtzF6LsP18u50CgIh0jXoH8uK+vUmjewAi0vGKef/KyZ5g7b5vbyMpAIhIRyoO+tkazvrNAEdVQGUUAESk4xRv9tac73f4zw0faGynOpDuAYhIx5nuzd6k5virieUKwMzuBj4IvOjuvxXw/mXA94D/LLy02d3viOPYItL9yidz1ZL2KUpyjr+auFJAfwt8GbinQpt/dvcPxnQ8EUmI8nRPdjJHIaVfVcqM9SsXJjbHX00sKSB3/zFwKI7PEhEpFZTuccBq+Nmj7hr8K2jmPYBLzWy3mX3fzBaENTKz1WY2bGbDExMTTeyeiLSboZFsaLrHgb4quX3l/itrVhXQT4C3ufurZrYMGALOD2ro7puATQADAwP1lPWKSBcopn7C9PVmeGzte09oW3qloNx/dU25AnD3X7r7q4XHW4G0mc1uxrFFpDNVqvQpH9wHF/exfuVC+nozGPngoNx/dU25AjCzs4Cfu7ub2SXkA89LzTi2iLSn8sqe4oSsWiZ4BQ3ug4v7NOBPU1xloPcBlwGzzWwMuA1IA7j7V4FrgD80s8NADrjO3ZXeEUmooMqemzePMvz8IR7cma1Y49/Xm9FAH5NYAoC7f7TK+18mXyYqIhKY3slNHeG+xw9wpMK5ofL68dJSECLSdGGrdlYa/PsSvm5PIygAiEjDlef7e2emefm1qZPapcwCg0BpxY/ER2sBiUhDle/SlZ3M8Uru5MEfYMl5s8ikUye8prRP4ygAiEjDDI1kuek7u0/K9x8NyfQ891JO5ZxNpBSQiDTELUOj3LvjhWlt0jI+mVM5ZxMpAIhILErr93ss/Cy/Ei3d0FwKACISWXldf7XBP50ycJgqaahcf/MpAIhIJMU8f6USznIbr7ko/3fATGBpHgUAEalbPXl+4NhArwG/tVQFJCJ1GRrJ1jX4V1vCWZpHAUBE6rJx275pD/4GyvO3EaWARKRmpTN66xn8r1/Sr7RPG1EAEJGqhkay3P7wnsDlG8L0GPz6aWleyU3pJm+bUgAQkYqCdtuqJpNOaQZvB1AAEJGKKu3MVaq4kJtW7ewcCgAiUlHY0s1FWqmzcykAiEjo9oyQX54hbHtGVfV0NpWBiiRc0HLNN28eZWgkC1Qe4B1N5upksQQAM7vbzF40s5+GvG9m9iUz229mT5rZxXEcV0SiC9ue8TP372LphkcB6M2kA39Wk7o6W1xXAH8LXFnh/auA8wt/VgN/E9NxRaRGQyNZlm54lHPXPsLSDY8eO8OvlOMvXg188KK52qilC8USANz9x8ChCk1WAPd43g6g18zmxnFsEakuKM2z5ru7WXDrP1Sd0JWbOsL2pye0UUsXatZN4D7gQMnzscJrB5t0fJFEC0rzTB11pt6orbZfG7V0p2bdBLaA1wJPPMxstZkNm9nwxMREg7slkgzVSjmr0UYt3alZAWAMOKfk+dnAeFBDd9/k7gPuPjBnzpymdE6kWxXz/nVsznWMcv3dq1kBYAvw8UI10BLgFXdX+kekgUrz/vWaNTOtXH8Xi+UegJndB1wGzDazMeA2IA3g7l8FtgLLgP3Aa8Dvx3FcEQlWzy5d5T62pJ87BxfG2CtpN7EEAHf/aJX3HfijOI4lknSVZu0W379582jdg/+smWlu+9ACnfUngJaCEOkg5StzFuv04fiM3FoXbwOt2pl0WgpCpIOEzdrduG3fsee1VvyYocE/4RQARDpI2OBe+nqtJZtf/MgiDf4JpwAg0kHCBvfS19dccUHgxJtyGvxFAUCkg6y54oKT1uQx4D3vOD5nZnBxH9cv6a/4OVrETUABQKSjDC7u4+L+M054zYH7nzhwbHE3gDsHF3LXqkVk0if/L66JXVKkACDSQYZGsjz2zMnrLk4dcW5/eM8Jrw0u7mPvn13FXasWaRE3CaQyUJE2FFTrD3DTd3aH/szLr00Fvq5F3CSMAoBImwmq9f/M/bta3CvpRkoBibSZ6UzkKhW2a5dIGAUAkTYyNJKte/G2dcsXxNwb6XZKAYm0iVuGRvnWjhfq+tneTFp5fpk2BQCRFim90ds7Mx16E7eaTDqls3+piwKASBMVB/3sZA7j+LZ49Q7+oPV8pH4KACJNUl7dE2WXrqJZM5X6kfopAIg0WOlZf5zSKeO2Dyn1I/VTABBpoPKz/ihO6THe+uunhW4EIzJdCgAiDVRvTX+5HoO/uPYiDfgSq1jmAZjZlWa2z8z2m9nagPc/YWYTZrar8OdTcRxXpN1FSfuUrt/zBa3dLw0Q+QrAzFLAV4D3A2PAE2a2xd2fKmt6v7vfGPV4Ip0kZVbX3ry9mTSPrX1vA3okclwcKaBLgP3u/iyAmX0bWAGUBwCRrhW2UXs9g38PmtUrzRFHCqgPOFDyfKzwWrkPm9mTZvaAmZ0Tw3FF2kLxRm92ModzfPG2+WsfmfZn9WbSfGGV0j3SHHFcAQTtPld+2vMwcJ+7v25mnwa+CQRe35rZamA1QH9/5V2NRFop7vLO5zZ8IJbPEalVHFcAY0DpGf3ZwHhpA3d/yd1fLzz9GvCusA9z903uPuDuA3PmzAlrJtJSpWf9cdAWjdIKcQSAJ4DzzexcM5sBXAdsKW1gZnNLni4H9sZwXJGWiau8E7RFo7RO5BSQux82sxuBbUAKuNvd95jZHcCwu28B/qeZLQcOA4eAT0Q9rkgzld/kjevMvzeTZt3yBcr5S0uY11Gl0CwDAwM+PDzc6m5IwsU5m9cM3PMpH83klUYws53uPlBLW80EFqni9of3RB78M+ke1q98pwZ8aSsKACIVDI1kIy3VDPkyub1/dlU8HRKJkQKASJnSfH+PBVU5T888VfhIm1IAEClRnu+vZyZvKVX4SDtTABApEUd5Z0/hRq+WbJZ2pwAgQnyzetMpY+M1WrZZOoMCgCRe1DLPlBlH3XXGLx1HAUASL2ra5y8/ojN+6UyxbAgj0smipH1On5HS4C8dS1cAkji3DI1y3+MHIlf4pFPG565eGFOvRJpPAUAS5ZahUb6144XInzNrZprbPqQ1fKSzKQBIotz7eLTBX4u3STfRPQBJjFuGRom69uHpp56iwV+6hq4ApKvlSzyfJDd1NJbPG49pGWiRdqAAIF0rrnx/Ka3rI91EAUC6RulsXuPkjamj0ro+0m0UAKQr3DI0yr07Xjg26E938C9W9QDHVgI9I5PGDCZfm9IsX+lKCgDS8YZGsicM/tN116pFJwzsGuQlKWIJAGZ2JfB/ye8J/HV331D2/qnAPcC7gJeAVe7+XBzHluSKYwG3vt6MBnxJrMhloGaWAr4CXAVcCHzUzC4sa/ZJ4GV3/w3gi8Dnox5Xkq24gFuUwV85fUm6OK4ALgH2u/uzAGb2bWAF8FRJmxXAusLjB4Avm5l5O+9IL22hdHeu0jx81AXctCm7SDwBoA84UPJ8DHh3WBt3P2xmrwBvBn4Rw/GlS5Uv05ydzHHz5lGg/nr8jy3p585Brd8jAvHMBA7aNLX8zL6WNvmGZqvNbNjMhicmJiJ3TjpX0Fl+buoI67bsoZ6tejX4i5wojgAwBpxT8vxsYDysjZmdApwBHAr6MHff5O4D7j4wZ86cGLonnSrsLH8yN8XRaSQPezNp7lq1SIO/SJk4UkBPAOeb2blAFrgO+N2yNluAG4B/A64BHlX+X4KU5vx7zOpesjmTTrF+5ULl+EUqiBwACjn9G4Ft5MtA73b3PWZ2BzDs7luAbwB/Z2b7yZ/5Xxf1uNJ9ynP+9Q7+p89I8bmrNfiLVBPLPAB33wpsLXvt1pLH/w+4No5jSfeqVtnTW5iZ+/JrU6Ftlr79TO79g0sb0T2RrqPloKVtVKvs+dUbh/nAO+eSSadOeq+Y59fgL1I7BQBpG9VW2pw64mx/eoL1KxfS15vByNfz37VqEbtuu1wpH5Fp0lpA0jLlk7ze8445VZdvHp/MMbi4T4O9SAwUAKQlgiZ5PbgzS8rgSIV7v1qPXyQ+CgDSVJUWcKtlaQet3SMSHwUAaajSNE/vzHTFCp5qPrakX6kfkRgpAEjDlKd5og7+mskrEi9VAUnDRF2xs6g3k9bgL9IACgDSMPWs2Fm+xlsmnWLd8gXxdEhETqAAIA1TT8WOwwk1/lrPR6RxdA9AYle+Qft09PVmeGzte2Pvk4icTAFAYhHH/rzpHlOZp0gTKQBIRWFbMpa3WfPAbqYqzeCqojeTZt3yBUr3iDSRAoCEqrQl4+DiPoZGstz+8J66yju1J69I6ykASKiwLRk3btvH8POHqq7bE8SAL65apIFfpA2oCkhChZVxZidzdQ3+kK/y0eAv0h4UACRUIxZe69NibiJtQwFAQq254oLAzVfqlUmnVOUj0kZ0D0ACDY1kWbdlTyxLOYBu+oq0o0gBwMzOBO4H5gPPAR9x95cD2h0BRgtPX3D35VGOK401NJJlzXd3M3V0+mWdS99+Js+9lKtYNioi7SHqFcBa4J/cfYOZrS08/18B7XLuvijisaQJhkay3PSd3Rzx+mr6tSevSOeIGgBWAJcVHn8T+BHBAUDaWJR6/lK6wSvSWaIGgLe6+0EAdz9oZm8JaXeamQ0Dh4EN7j4U9oFmthpYDdDf3x+xe1IUNqN3aCTLTd/dzZE60j2ldINXpPNUDQBm9kPgrIC3PjuN4/S7+7iZnQc8amaj7v5MUEN33wRsAhgYGIg2KglQeUbv7Q/viTz46wavSGeqGgDc/X1h75nZz81sbuHsfy7wYshnjBf+ftbMfgQsBgIDgMSv0ozeetM+WrtHpPNFnQewBbih8PgG4HvlDcxslpmdWng8G1gKPBXxuDINlWb01mvXbZdr8BfpcFEDwAbg/Wb2M+D9heeY2YCZfb3Q5jeBYTPbDWwnfw9AAaAJhkayLN3waF3r8lcyM635gyLdINJNYHd/CfidgNeHgU8VHv8roA1dm6w87x+XHoM/X/nOWD9TRFpDM4G7VFwbshcZaGKXSJdRAOhS9WzIHkbbNIp0JyVzu9DQSJYes1g+S/X9It1LVwBd5vqv/RuPPXMo0mekzDjqrpSPSJdTAOgitwyNTnvwz6RTJ9wryKRTrF+5UIO+SAIoBdRF6tmla/3KhfT1ZjDyuX4N/iLJoSuALjE0kp32z/Rm0gwu7tOAL5JQCgAdJmxRt43b9k3rc3qAdcsXNKaTItIRzOtc970ZBgYGfHh4uNXdaBtxTe6ame7hz1e+U2f+Il3IzHa6+0AtbXUF0OZKz/h7zOreqEUTuUSknAJAGys/469n8E/1GH957UUa9EXkJKoCamNRl3OYNTOtwV9EQukKoI1FWc7BgJFbL4+vMyLSdXQF0MZmzkjV/bPztD+viFShK4AWCivpLL73qzfqS/9o/R4RqYXKQFskqKQznTJOn3EKk7npb9NogKP9eUWSTmWgHSDoBu/UEZ/W4J8qlIVq0BeReigAtEiUG7ynz0ix544rY+yNiCRRpJvAZnatme0xs6NmFnrJYWZXmtk+M9tvZmujHLNb1HuTNtVjfO5q7bApItFFrQL6KbAS+HFYAzNLAV8BrgIuBD5qZhdGPG7HW3PFBWTS06vyUV2/iMQp6qbwewGs8u5TlwD73f3ZQttvAyuAp6IcuxOVV/18+F19bH96gmwN6aClbz+Te//g0ib0UkSSohn3APqAAyXPx4B3hzU2s9XAaoD+/v7G9qwJioN+djJ3rFIHIDuZ48GdWdavXMif3L+LSrVYGvxFpBGqBgAz+yFwVsBbn3X379VwjKDLg9Dxzt03AZsgXwZaw+e3rfJSz/L/mNzUETZu28e83kzgVYA2YxeRRqoaANz9fRGPMQacU/L8bGA84me2pfIUz69eP1x1LZ/xyRxfXLXopDkBmswlIo3WjBTQE8D5ZnYukAWuA363CcdtqvKz/Vry+pCvBire1A2bFSwi0giRAoCZXQ38FTAHeMTMdrn7FWY2D/i6uy9z98NmdiOwDUgBd7v7nsg9bzP1rNxZepavrRlFpNmiVgE9BDwU8Po4sKzk+VZga5Rjtbtaz/i1ZIOItAvNBI5Jqobdugy4fkk/dw5qIpeItJ6Wg45JLbt1ObD96YnGd0ZEpAa6AoigtOqnlisAiLYGkIhInBQA6lTvfr3aqEVE2oVSQHWKWvUjItJqugIIUGmnrqJaUzmq+hGRdqUAUCZoQtfNm0cZfv4Q25+eOBYUzsikAzdv6c2kOf3UUzShS0TangJAmaDUTm7qCPfueOGEhdwAegyOlqT+M+kU65Yv0IAvIh1B9wDKhKV2gm7xHvX8Gv1GPsWzfuVCDf4i0jF0BVAmbGXOMDNnnMLIrZc3sEciIo2hK4AyQTt1VdruRnX9ItKpFADKDC7uY/3KhfT1Zo6ldq5f0h8aBFTXLyKdSimgAGErc5beCAbV9YtIZ+vKAFBLHf903Tm4kIG3nak1+0Wka3RdAAir4weODdb1Bgit2S8i3aTr7gGE1fFv3LYPOB4gspM5nOMBYmgk24Leioi0TtcFgLCqnOLr67bsqRggRESSousCQFhVzrzeDEMj2cDlG0DlnCKSPJECgJlda2Z7zOyomQ1UaPecmY2a2S4zG45yzGqC6viL1TqVzvJVzikiSRP1CuCnwErgxzW0fY+7L3L30EARh6A6/uISDZXO8lXOKSJJE3VT+L0AZpXmyjZfWLVO2DIPs2amVd0jIonTrHsADvyjme00s9VNOuZJwtJDt31oQYt6JCLSOlWvAMzsh8BZAW991t2/V+Nxlrr7uJm9BfiBmT3t7oFpo0KAWA3Q399f48fXpniWr8lcIiJgXuNethU/xOxHwJ+6e9UbvGa2DnjV3f+iWtuBgQEfHm7oPWMRka5iZjtrvdfa8BSQmZ1uZm8qPgYuJ3/zWEREWihqGejVZjYGXAo8YmbbCq/PM7OthWZvBf7FzHYD/w484u7/EOW4IiISXdQqoIeAhwJeHweWFR4/C1wU5TgiIhK/rpsJLCIitVEAEBFJKAUAEZGEiqUMtFHMbAJ4PsJHzAZ+EVN3GkV9jIf6GA/1MR6t7OPb3H1OLQ3bOgBEZWbDjV57KCr1MR7qYzzUx3h0Qh9BKSARkcRSABARSahuDwCbWt2BGqiP8VAf46E+xqMT+tjd9wBERCRct18BiIhIiK4KAO24RWWEPl5pZvvMbL+ZrW1yH880sx+Y2c8Kf88KaXek8B3uMrMtTehXxe/EzE41s/sL7z9uZvMb3ac6+vgJM5so+d4+1YI+3m1mL5pZ4KKMlvelwn/Dk2Z2cRv28TIze6Xke7y1yf07x8y2m9newv/PfxzQpuXfY1Xu3jV/gN8ELgB+BAxUaPccMLtd+wikgGeA84AZwG7gwib28f8AawuP1wKfD2n3ahP7VPU7Af4H8NXC4+uA+5v8b1tLHz8BfLkVv3slffjvwMXAT0PeXwZ8HzBgCfB4G/bxMuDvW/gdzgUuLjx+E/AfAf/WLf8eq/3pqisAd9/r7uE7v7eBGvt4CbDf3Z919zeAbwMrGt+7Y1YA3yw8/iYw2MRjh6nlOynt9wPA71hz9ytt9b9bTTy/GdOhCk1WAPd43g6g18zmNqd3eTX0saXc/aC7/6Tw+L+AvUD5zlIt/x6r6aoAMA1tsUVlBX3AgZLnY5z8y9VIb3X3g5D/RQfeEtLuNDMbNrMdZtboIFHLd3KsjbsfBl4B3tzgfgUevyDs3+3DhZTAA2Z2TnO6Ni2t/v2r1aVmttvMvm9mLdvXtZBqXAw8XvZW23+PkZaDboVmb1HZoj4GnbXGWq5VqY/T+Jj+wvd4HvComY26+zPx9PAktXwnDf/eqqjl+A8D97n762b2afJXLO9teM+mp9XfYy1+Qn7Jg1fNbBkwBJzf7E6Y2a8BDwKfcfdflr8d8CNt9T12XABw9/fF8Bnjhb9fNLOHyF+6xxYAYujjGFB6Zng2MB7xM09QqY9m9nMzm+vuBwuXrC+GfEbxe3y2sC3oYvI58Eao5Tspthkzs1OAM2huGqFqH939pZKnXwM+34R+TVfDf/+iKh1s3X2rmf21mc1296atv2NmafKD/73uvjmgSdt/j4lLAXXIFpVPAOeb2blmNoP8Dc2GV9mU2ALcUHh8A3DSVYuZzTKzUwuPZwNLgaca2KdavpPSfl8DPOqFu3FNUrWPZTng5eRzx+1mC/DxQhXLEuCVYkqwXZjZWcX7O2Z2Cfmx7KXKPxXr8Q34BrDX3b8Q0qztv8eW34WO8w9wNfmo+zrwc2Bb4fV5wNbC4/PIV2fsBvaQT8u0VR/9eAXBf5A/o252H98M/BPws8LfZxZeHwC+Xnj828Bo4XscBT7ZhH6d9J0AdwDLC49PA74L7Ce//eh5LfgdrNbH9YXfu93AduAdLejjfcBBYKrwu/hJ4NPApwvvG/CVwn/DKBUq6lrYxxtLvscdwG83uX//jXw650lgV+HPsnb7Hqv90UxgEZGESlwKSERE8hQAREQSSgFARCShFABERBJKAUBEJKEUAEREEkoBQEQkoRQAREQS6v8D0cSJ5UPc614AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "plt.scatter(sklearn_pred, keras_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:15<00:00, 15.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x261561d9358>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Testing CCC ###\n",
    "\n",
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from imly import dope\n",
    "\n",
    "dataset_name = \"uci_auto_mpg\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/uci_auto_mpg.csv\", delimiter=\",\", header=0, index_col='car name')\n",
    "data = data[data.horsepower != '?']\n",
    "sc = StandardScaler()\n",
    "data = sc.fit_transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "Y = data.iloc[:,1]\n",
    "X = data.iloc[:,2:]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "m = dope(model)\n",
    "\n",
    "x_train = x_train.values\n",
    "y_train = y_train.values\n",
    "\n",
    "m.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train)\n",
    "sklearn_pred = model.predict(x_test)\n",
    "keras_pred = m.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9894374129958334"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.correlations import concordance_correlation_coefficient as ccc\n",
    "\n",
    "ccc(sklearn_pred, keras_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x26156618e10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(sklearn_pred, keras_pred)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../data/test2_pdf.pdf\", bbox_inches='tight') # write pdf to local disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ../data/test_pdf.pdf to Amazon S3 bucket mlsquare-datasets\n",
      "..."
     ]
    }
   ],
   "source": [
    "### Test upload to AWS S3 ###\n",
    "import boto\n",
    "import sys\n",
    "from boto.s3.key import Key\n",
    "# from boto.s3.key import Key\n",
    "bucket_name = 'mlsquare-datasets'\n",
    "AWS_ACCESS_KEY_ID = 'AKIAJXRNK62PGFLPIJTA'\n",
    "AWS_SECRET_ACCESS_KEY = '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'\n",
    "REGION_HOST = 's3.ap-south-1.amazonaws.com'\n",
    "\n",
    "# bucket_name = AWS_ACCESS_KEY_ID.lower() + '-dump'\n",
    "conn = boto.connect_s3(AWS_ACCESS_KEY_ID,\n",
    "        AWS_SECRET_ACCESS_KEY, host=REGION_HOST)\n",
    "bucket = conn.get_bucket('mlsquare-pdf', validate=False)\n",
    "\n",
    "# bucket = conn.create_bucket(bucket_name,\n",
    "#     location=boto.s3.connection.Location.DEFAULT)\n",
    "\n",
    "testfile = \"../data/test_pdf.pdf\"\n",
    "print ('Uploading %s to Amazon S3 bucket %s' % (testfile, bucket_name))\n",
    "\n",
    "def percent_cb(complete, total):\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "k = Key(bucket)\n",
    "k.key = 'my test file'\n",
    "k.set_contents_from_filename(testfile,\n",
    "    cb=percent_cb, num_cb=10) # upload file\n",
    "url = k.generate_url(expires_in=0, query_auth=False) # get url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/uci_abalone_logistic.pdf'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'uci_abalone'\n",
    "algo = 'logistic'\n",
    "'../data/' + ('_').join([name,algo]) + '.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate datasets #\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(1000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)\n",
    "df = pd.concat([X, y], axis=1)\n",
    "# testData3 = np.concatenate((X,y), axis=1)\n",
    "# testData3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1001)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/testData4.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y type --  <class 'theano.tensor.var.TensorVariable'>\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 5\n",
      "Trainable params: 5\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "40/40 [==============================] - 0s 449us/step - loss: -6.6398\n",
      "60/60 [==============================] - 0s 50us/step\n",
      "-5.562779839833578\n"
     ]
    }
   ],
   "source": [
    "### Testing Deep LDA implementation with Theano loss ###\n",
    "\n",
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_name = \"uci_iris\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "url = \"../data/iris.csv\" if path.exists(\"../data/iris.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
    "class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
    "data.iloc[:,-1] = index\n",
    "data = data.loc[data[4] != 2]\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "# params = {\n",
    "#     'epochs': 170\n",
    "# }\n",
    "\n",
    "# automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60, params=params)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "\n",
    "## Create model ##\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "def create_model(input_dim, reg_par):\n",
    "    \"\"\"\n",
    "    Builds the model\n",
    "    The structure of the model can get easily substituted with a more efficient and powerful network like CNN\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(1, input_shape=(input_dim,), activation='sigmoid', kernel_regularizer=l2(reg_par)))\n",
    "#     model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(reg_par)))\n",
    "#     model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(reg_par)))\n",
    "#     model.add(Dense(2, activation='linear', kernel_regularizer=l2(reg_par))) \n",
    "#     outdim_size is passed via arguments\n",
    "\n",
    "    return model\n",
    "\n",
    "## Define loss function ##\n",
    "\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import numpy as np\n",
    "from theano.compile.ops import as_op\n",
    "\n",
    "\n",
    "@as_op(itypes=[theano.tensor.ivector],  # Why? What is the need for such an op?\n",
    "       otypes=[theano.tensor.ivector])\n",
    "def numpy_unique(a):\n",
    "    return np.unique(a)\n",
    "\n",
    "\n",
    "def lda_loss(n_components, margin):\n",
    "    \"\"\"\n",
    "    The main loss function (inner_lda_objective) is wrapped in this function due to\n",
    "    the constraints imposed by Keras on objective functions\n",
    "    \"\"\"\n",
    "    def inner_lda_objective(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        It is the loss function of LDA as introduced in the original paper. \n",
    "        It is adopted from the the original implementation in the following link:\n",
    "        https://github.com/CPJKU/deep_lda\n",
    "        Note: it is implemented by Theano tensor operations, and does not work on Tensorflow backend\n",
    "        \"\"\"\n",
    "        r = 1e-4\n",
    "\n",
    "        # init groups\n",
    "        print('y type -- ',type(y_true))\n",
    "        yt = T.cast(y_true.flatten(), \"int32\")\n",
    "        groups = numpy_unique(yt)\n",
    "\n",
    "        def compute_cov(group, Xt, yt):\n",
    "            Xgt = Xt[T.eq(yt, group).nonzero()[0], :]\n",
    "            Xgt_bar = Xgt - T.mean(Xgt, axis=0)\n",
    "            m = T.cast(Xgt_bar.shape[0], 'float32')\n",
    "            return (1.0 / (m - 1)) * T.dot(Xgt_bar.T, Xgt_bar)\n",
    "\n",
    "        # scan over groups\n",
    "        covs_t, updates = theano.scan(fn=compute_cov, outputs_info=None,\n",
    "                                      sequences=[groups], non_sequences=[y_pred, yt])\n",
    "\n",
    "        # compute average covariance matrix (within scatter)\n",
    "        Sw_t = T.mean(covs_t, axis=0)\n",
    "\n",
    "        # compute total scatter\n",
    "        Xt_bar = y_pred - T.mean(y_pred, axis=0)\n",
    "        m = T.cast(Xt_bar.shape[0], 'float32')\n",
    "        St_t = (1.0 / (m - 1)) * T.dot(Xt_bar.T, Xt_bar)\n",
    "\n",
    "        # compute between scatter\n",
    "        Sb_t = St_t - Sw_t\n",
    "\n",
    "        # cope for numerical instability (regularize)\n",
    "        Sw_t += T.identity_like(Sw_t) * r\n",
    "\n",
    "        # return T.cast(T.neq(yt[0], -1), 'float32')*T.nlinalg.trace(T.dot(T.nlinalg.matrix_inverse(St_t), Sb_t))\n",
    "\n",
    "        # compute eigenvalues\n",
    "        evals_t = T.slinalg.eigvalsh(Sb_t, Sw_t)\n",
    "\n",
    "        # get eigenvalues\n",
    "        top_k_evals = evals_t[-n_components:]\n",
    "\n",
    "        # maximize variance between classes\n",
    "        # (k smallest eigenvalues below threshold)\n",
    "        thresh = T.min(top_k_evals) + margin\n",
    "        top_k_evals = top_k_evals[(top_k_evals <= thresh).nonzero()]\n",
    "        costs = T.mean(top_k_evals)\n",
    "\n",
    "        return -costs\n",
    "\n",
    "    return inner_lda_objective\n",
    "\n",
    "## Fit the model ##\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = create_model(x_train.shape[-1], reg_par=1e-5)\n",
    "\n",
    "model_optimizer = Adam()\n",
    "model.compile(loss=lda_loss(n_components=1, margin=1), optimizer='adam')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "score = model.evaluate(x_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lda_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c4fa72ce1ce2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlda_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmargin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lda_loss' is not defined"
     ]
    }
   ],
   "source": [
    "### Testing Keras with Theano backend ###\n",
    "\n",
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_name = \"uci_iris\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "url = \"../data/iris.csv\" if path.exists(\"../data/iris.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
    "class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
    "data.iloc[:,-1] = index\n",
    "data = data.loc[data[4] != 2]\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "# params = {\n",
    "#     'epochs': 170\n",
    "# }\n",
    "\n",
    "# automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60, params=params)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "\n",
    "## Create model ##\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    \"\"\"\n",
    "    Builds the model\n",
    "    The structure of the model can get easily substituted with a more efficient and powerful network like CNN\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(1, input_shape=(4,), activation='sigmoid', kernel_regularizer=l2(1e-5))) \n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.compile(loss=lda_loss(n_components=1, margin=1), optimizer='adam')\n",
    "model.fit(x_train, y_train)\n",
    "score = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3719358722368876"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "247.067px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
